{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPKWVIVeEtb8/A7NP4NfWMT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjleece/rag-experimentation-framework/blob/main/rag_experimentation_framework_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you use this code, please cite:\n",
        "\n",
        "{\n",
        "  title = {RAG Experimentation Framework},\n",
        "\n",
        "  author = {Bill Leece},\n",
        "\n",
        "  year = {2024}\n",
        "}"
      ],
      "metadata": {
        "id": "wZ0kV_UtQn5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "_lHNBLR-92Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers --quiet\n",
        "#!pip install -U optimum --quiet\n",
        "!pip install -U accelerate  --quiet\n",
        "!pip install -U bitsandbytes  --quiet\n",
        "!pip install -U torch --quiet\n",
        "!pip install -U sentencepiece --quiet\n",
        "!pip install -U llama-index --quiet\n",
        "!pip install -U llama-index-llms-mistralai --quiet\n",
        "!pip install -U llama-index-embeddings-mistralai --quiet\n",
        "!pip install -U llama-index-llms-langchain --quiet\n",
        "!pip install -U langchain --quiet\n",
        "!pip install -U langchain-community --quiet\n",
        "!pip install -U langchain-mistralai --quiet\n",
        "!pip install -U langchain_huggingface --quiet\n",
        "!pip install -U faiss-gpu --quiet"
      ],
      "metadata": {
        "id": "4g_Vs7wgZW-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "import transformers\n",
        "import torch\n",
        "import gc\n",
        "import openai\n",
        "import json\n",
        "import tiktoken\n",
        "import textwrap\n",
        "import time\n",
        "from google.colab import drive, userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "from llama_index.core import SimpleDirectoryReader, Settings\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "import time\n",
        "from typing import List, Dict, Tuple\n",
        "from contextlib import contextmanager\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any"
      ],
      "metadata": {
        "id": "Ao7eaSfq-TKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get('MISTRAL_API_KEY')\n",
        "api_key = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "YvGHY024-OXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' #Use GPUs when possible"
      ],
      "metadata": {
        "id": "mxAHV7T_-Xlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment Configurations"
      ],
      "metadata": {
        "id": "jkqEV8M_HUKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup configurations\n",
        "MODEL_CONFIGS = {\n",
        "    \"models\": [\n",
        "    #    {\n",
        "    #        \"name\": \"open-mixtral-8x7b\",\n",
        "    #        \"type\": \"mistral_api\",\n",
        "    #        \"tokenizer\": None,  # Not needed for API models\n",
        "    #    },\n",
        "\n",
        "          {\n",
        "            \"name\": \"mistral-large-latest\",\n",
        "            \"type\": \"mistral_api\",\n",
        "            \"tokenizer\": None,  # Not needed for API models\n",
        "         },\n",
        "\n",
        "         {\n",
        "            \"name\": \"open-mistral-nemo\",\n",
        "            \"type\": \"mistral_api\",\n",
        "            \"tokenizer\": None,  # Not needed for API models\n",
        "        },\n",
        "#        {\n",
        "#            \"name\": \"ministral-8b-latest\",\n",
        "#            \"type\": \"mistral_api\",\n",
        "#            \"tokenizer\": None,  # Not needed for API models\n",
        "#        },\n",
        " #       {\n",
        " #             \"name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        " #             \"type\": \"huggingface\",\n",
        " #             \"tokenizer\": \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        " #       },\n",
        "\n",
        "   #   {\n",
        "   #         \"name\": \"wjleece/quantized-mistral-7b\",\n",
        "   #         \"type\": \"huggingface\",\n",
        "   #         \"tokenizer\": \"mistralai/Mixtral-8x7B-v0.1\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "   #          \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "   #             \"load_in_4bit\": True,\n",
        "   #             \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "   #             \"bnb_4bit_quant_type\": \"nf4\",\n",
        "   #             \"bnb_4bit_use_double_quant\": False\n",
        "   #         }\n",
        "   #     },\n",
        "      {\n",
        "              \"name\": \"wjleece/quantized-mistral-nemo-12b\",\n",
        "              \"type\": \"huggingface\",\n",
        "              \"tokenizer\": \"mistralai/Mistral-Nemo-Instruct-2407\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "              \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "                  \"load_in_4bit\": True,\n",
        "                  \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "                  \"bnb_4bit_quant_type\": \"nf4\",\n",
        "                  \"bnb_4bit_use_double_quant\": False\n",
        "             }\n",
        "          },\n",
        "   #    {\n",
        "   #           \"name\": \"wjleece/quantized-mistral-8b\",\n",
        "   #           \"type\": \"huggingface\",\n",
        "   #           \"tokenizer\": \"mistralai/Ministral-8B-Instruct-2410\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "   #           \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "   #               \"load_in_4bit\": True,\n",
        "   #               \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "   #               \"bnb_4bit_quant_type\": \"nf4\",\n",
        "   #              \"bnb_4bit_use_double_quant\": False\n",
        "   #           }\n",
        "   #       },\n",
        "   #     {\n",
        "   #           \"name\": \"wjleece/quantized-llama-3.1-8b\",\n",
        "   #           \"type\": \"huggingface\",\n",
        "   #           \"tokenizer\": \"meta-llama/Llama-3.1-8B-Instruct\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "   #           \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "   #               \"load_in_4bit\": True,\n",
        "   #               \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "   #               \"bnb_4bit_quant_type\": \"nf4\",\n",
        "   #               \"bnb_4bit_use_double_quant\": False\n",
        "   #           }\n",
        "   #       }\n",
        "       ]\n",
        "}\n",
        "\n",
        "\n",
        "CHUNKING_CONFIGS = {\n",
        "    \"strategies\": [\"paragraph\", \"header\"],\n",
        "    \"semantic_config\": {\n",
        "        \"enabled\": True,\n",
        "        \"thresholds\": [85, 95] if True else []\n",
        "    },\n",
        "    \"max_chunk_size\": 2048,\n",
        "    \"chunk_overlap\": 100,\n",
        "    \"min_chunk_size\": 35 #we'll ignore any chunk ~5 words or less\n",
        "}\n",
        "\n",
        "QUESTION_CONFIGS = {\n",
        "    \"questions\": [\n",
        "        \"What were cloud revenues in the most recent quarter?\",\n",
        "        \"What were the main drivers of revenue growth in the most recent quarter?\",\n",
        "        \"How much did YouTube ad revenues grow in the most recent quarter in APAC?\",\n",
        "        \"Can you summarize recent key antitrust matters?\",\n",
        "        \"Compare the revenue growth across all geographic regions and explain the main factors for each region.\",\n",
        "        \"Summarize all mentioned risk factors related to international operations.\",\n",
        "        \"What were the major changes in operating expenses across all categories and their stated reasons?\",\n",
        "    ] #These quetsions should relate to the RAG document --> these are your 'business use cases'\n",
        "}\n",
        "\n",
        "FILE_CONFIGS = {\n",
        "    \"save_directory\": '/content/drive/My Drive/AI/Model_Analysis'\n",
        "}"
      ],
      "metadata": {
        "id": "YDjgk_JhHWkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load RAG Document"
      ],
      "metadata": {
        "id": "e_wxgOGc95sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "documents = SimpleDirectoryReader(input_files=[\"/content/drive/My Drive/AI/Datasets/Google-10-q/goog-10-q-q3-2024.pdf\"]).load_data()"
      ],
      "metadata": {
        "id": "gS4Lemk09v9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RAG Pipeline Class"
      ],
      "metadata": {
        "id": "MgLxma5M-bZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global singleton instance\n",
        "_GLOBAL_RAG_PIPELINE = None\n",
        "\n",
        "class RAGPipeline:\n",
        "    def __init__(self):\n",
        "        self.chunk_cache = {}\n",
        "        self.embedding_cache = {}\n",
        "        self.embedding_model = None\n",
        "\n",
        "    @classmethod\n",
        "    def get_instance(cls):\n",
        "        \"\"\"Get or create singleton instance\"\"\"\n",
        "        global _GLOBAL_RAG_PIPELINE\n",
        "        if _GLOBAL_RAG_PIPELINE is None:\n",
        "            _GLOBAL_RAG_PIPELINE = cls()\n",
        "        return _GLOBAL_RAG_PIPELINE\n",
        "\n",
        "\n",
        "    def initialize_embedding_model(self):\n",
        "        \"\"\"Initialize the embedding model if not already initialized\"\"\"\n",
        "        if self.embedding_model is None:\n",
        "            mistral_api_key = userdata.get('MISTRAL_API_KEY')\n",
        "            self.embedding_model = MistralAIEmbedding(\n",
        "                model_name=\"mistral-embed\",\n",
        "                api_key=mistral_api_key\n",
        "            )\n",
        "        return self.embedding_model\n",
        "\n",
        "    def convert_to_markdown_headers(self, text):\n",
        "        \"\"\"Convert document section titles to markdown headers\"\"\"\n",
        "        import re\n",
        "\n",
        "        patterns = [\n",
        "            (r'^(?:ITEM|Section)\\s+\\d+[.:]\\s*(.+)$', '# '),\n",
        "            (r'^\\d+\\.\\d+\\s+(.+)$', '## '),\n",
        "            (r'^\\([a-z]\\)\\s+(.+)$', '### ')\n",
        "        ]\n",
        "\n",
        "        lines = text.split('\\n')\n",
        "        markdown_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            converted = False\n",
        "\n",
        "            for pattern, header_mark in patterns:\n",
        "                if re.match(pattern, line, re.IGNORECASE):\n",
        "                    markdown_lines.append(f\"{header_mark}{line}\")\n",
        "                    converted = True\n",
        "                    break\n",
        "\n",
        "            if not converted:\n",
        "                markdown_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(markdown_lines)\n",
        "\n",
        "\n",
        "    def create_chunks(self, documents: List, threshold: int, chunk_strategy: str = \"semantic\") -> Dict:\n",
        "        \"\"\"Create or retrieve chunks based on specified strategy\"\"\"\n",
        "\n",
        "        MAX_CHUNK_SIZE = CHUNKING_CONFIGS['max_chunk_size']\n",
        "        CHUNK_OVERLAP = CHUNKING_CONFIGS['chunk_overlap']\n",
        "        MIN_CHUNK_SIZE = CHUNKING_CONFIGS['min_chunk_size']\n",
        "\n",
        "\n",
        "        if chunk_strategy == \"semantic\":\n",
        "            cache_key = f\"{chunk_strategy}_{threshold}\"\n",
        "            print(f\"Using semantic cache key: {cache_key} with threshold: {threshold}\")\n",
        "        else:\n",
        "            cache_key = f\"{chunk_strategy}_{MAX_CHUNK_SIZE}\"\n",
        "            print(f\"Using non-semantic cache key: {cache_key}\")\n",
        "\n",
        "\n",
        "        if cache_key not in self.chunk_cache:\n",
        "            print(\"\\nStarting new chunk creation:\")\n",
        "            texts = []\n",
        "\n",
        "            try:\n",
        "                if chunk_strategy == \"semantic\":\n",
        "                    print(\"Processing semantic chunking...\")\n",
        "                    if self.embedding_model is None:\n",
        "                        print(\"Initializing embedding model\")\n",
        "                        self.initialize_embedding_model()\n",
        "\n",
        "                    splitter = SemanticSplitterNodeParser(\n",
        "                        buffer_size=1,\n",
        "                        breakpoint_percentile_threshold=threshold,\n",
        "                        embed_model=self.embedding_model\n",
        "                    )\n",
        "                    nodes = splitter.get_nodes_from_documents(documents)\n",
        "                    texts = [node.text for node in nodes]\n",
        "                    print(f\"Generated {len(texts)} semantic chunks\")\n",
        "\n",
        "                elif chunk_strategy == \"paragraph\":\n",
        "                    print(\"Processing paragraph chunking...\")\n",
        "                    text_splitter = RecursiveCharacterTextSplitter(\n",
        "                        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "                        chunk_size=MAX_CHUNK_SIZE,\n",
        "                        chunk_overlap=CHUNK_OVERLAP,\n",
        "                        length_function=len\n",
        "                    )\n",
        "\n",
        "                    for idx, doc in enumerate(documents):\n",
        "                        print(f\"\\nProcessing document {idx + 1}/{len(documents)}\")\n",
        "                        print(f\"Document length: {len(doc.text)} characters\")\n",
        "                        doc_chunks = text_splitter.split_text(doc.text)\n",
        "                        print(f\"Initial chunks from document: {len(doc_chunks)}\")\n",
        "                        if doc_chunks:\n",
        "                            print(f\"Sample chunk lengths: {[len(c) for c in doc_chunks[:3]]}\")\n",
        "                        texts.extend(doc_chunks)\n",
        "\n",
        "                elif chunk_strategy == \"header\":\n",
        "                    print(\"Processing header chunking...\")\n",
        "                    text_splitter = RecursiveCharacterTextSplitter(\n",
        "                        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "                        chunk_size=MAX_CHUNK_SIZE,\n",
        "                        chunk_overlap=CHUNK_OVERLAP,\n",
        "                        length_function=len\n",
        "                    )\n",
        "\n",
        "                    for idx, doc in enumerate(documents):\n",
        "                        print(f\"\\nProcessing document {idx + 1}/{len(documents)}\")\n",
        "                        md_text = self.convert_to_markdown_headers(doc.text)\n",
        "                        print(\"Headers identified. First 100 chars of markdown text:\")\n",
        "                        print(md_text[:100] + \"...\")\n",
        "\n",
        "                        headers_to_split_on = [\n",
        "                            (\"#\", \"Header 1\"),\n",
        "                            (\"##\", \"Header 2\"),\n",
        "                            (\"###\", \"Header 3\"),\n",
        "                        ]\n",
        "\n",
        "                        header_splitter = MarkdownHeaderTextSplitter(\n",
        "                            headers_to_split_on=headers_to_split_on\n",
        "                        )\n",
        "\n",
        "                        splits = header_splitter.split_text(md_text)\n",
        "                        print(f\"Generated {len(splits)} header sections\")\n",
        "                        if splits:\n",
        "                            print(\"Sample section lengths:\", [len(s.page_content) for s in splits[:3]])\n",
        "\n",
        "                        for split in splits:\n",
        "                            if len(split.page_content) > MAX_CHUNK_SIZE:\n",
        "                                print(f\"Splitting large section: {len(split.page_content)} chars\")\n",
        "                                subsections = text_splitter.split_text(split.page_content)\n",
        "                                print(f\"Created {len(subsections)} subsections\")\n",
        "                                texts.extend(subsections)\n",
        "                            else:\n",
        "                                texts.append(split.page_content)\n",
        "\n",
        "                print(\"\\nCleaning and filtering chunks...\")\n",
        "                initial_count = len(texts)\n",
        "                cleaned_texts = []\n",
        "                for idx, text in enumerate(texts):\n",
        "                    if not isinstance(text, str):\n",
        "                        print(f\"Warning: Non-string chunk found at index {idx}\")\n",
        "                        continue\n",
        "\n",
        "                    cleaned_text = text.strip()\n",
        "                    if len(cleaned_text) >= MIN_CHUNK_SIZE:\n",
        "                        cleaned_texts.append(cleaned_text)\n",
        "                    else:\n",
        "                        print(f\"Filtered out small chunk: {len(cleaned_text)} chars\")\n",
        "\n",
        "                texts = cleaned_texts\n",
        "                print(f\"Chunks after cleaning: {len(texts)} (removed {initial_count - len(texts)})\")\n",
        "\n",
        "                if not texts:\n",
        "                    print(\"WARNING: No valid chunks generated!\")\n",
        "                    return {\n",
        "                        'texts': [],\n",
        "                        'strategy': chunk_strategy,\n",
        "                        'chunk_stats': {\n",
        "                            'num_chunks': 0,\n",
        "                            'avg_chunk_size': 0,\n",
        "                            'min_chunk_size': 0,\n",
        "                            'max_chunk_size': 0\n",
        "                        }\n",
        "                    }\n",
        "\n",
        "                # Calculate chunk statistics\n",
        "                chunk_lengths = [len(t) for t in texts]\n",
        "                chunk_stats = {\n",
        "                    'num_chunks': len(texts),\n",
        "                    'avg_chunk_size': sum(chunk_lengths)/len(texts),\n",
        "                    'min_chunk_size': min(chunk_lengths),\n",
        "                    'max_chunk_size': max(chunk_lengths)\n",
        "                }\n",
        "\n",
        "                print(\"\\nFinal Chunk Statistics:\")\n",
        "                print(f\"Total chunks: {chunk_stats['num_chunks']}\")\n",
        "                print(f\"Average chunk size: {chunk_stats['avg_chunk_size']:.2f} chars\")\n",
        "                print(f\"Minimum chunk size: {chunk_stats['min_chunk_size']} chars\")\n",
        "                print(f\"Maximum chunk size: {chunk_stats['max_chunk_size']} chars\")\n",
        "\n",
        "                print(\"\\nSample of first chunk:\")\n",
        "                if texts:\n",
        "                    print(texts[0][:200] + \"...\")\n",
        "\n",
        "                # Store in cache\n",
        "                self.chunk_cache[cache_key] = {\n",
        "                    'texts': texts,\n",
        "                    'strategy': chunk_strategy,\n",
        "                    'chunk_stats': chunk_stats\n",
        "                }\n",
        "                print(f\"\\nStored chunks in cache with key: {cache_key}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(\"\\nERROR in chunk creation:\")\n",
        "                print(f\"Error type: {type(e).__name__}\")\n",
        "                print(f\"Error message: {str(e)}\")\n",
        "                import traceback\n",
        "                print(\"\\nTraceback:\")\n",
        "                print(traceback.format_exc())\n",
        "                return {\n",
        "                    'texts': [],\n",
        "                    'strategy': chunk_strategy,\n",
        "                    'chunk_stats': {\n",
        "                        'num_chunks': 0,\n",
        "                        'avg_chunk_size': 0,\n",
        "                        'min_chunk_size': 0,\n",
        "                        'max_chunk_size': 0\n",
        "                    }\n",
        "                }\n",
        "        else:\n",
        "            print(f\"\\nRetrieving {len(self.chunk_cache[cache_key]['texts'])} existing chunks from cache\")\n",
        "\n",
        "        result = self.chunk_cache[cache_key]\n",
        "        print(f\"\\nFinal Output:\")\n",
        "        print(f\"Number of chunks: {len(result['texts'])}\")\n",
        "        print(f\"Strategy: {result['strategy']}\")\n",
        "        print(\"=\"*50)\n",
        "        return result\n",
        "\n",
        "    def run_cosine_search(self, query: str, threshold: int, chunk_strategy: str = \"semantic\", k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Run cosine similarity search with enhanced error handling and debugging\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"COSINE SEARCH DEBUG LOG\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Strategy: {chunk_strategy}\")\n",
        "        print(f\"Threshold: {threshold}\")\n",
        "        print(f\"Requested k: {k}\")\n",
        "\n",
        "        if chunk_strategy == \"semantic\":\n",
        "            cache_key = f\"{chunk_strategy}_{threshold}\"\n",
        "        else:\n",
        "            cache_key = f\"{chunk_strategy}_{CHUNKING_CONFIGS['max_chunk_size']}\"\n",
        "\n",
        "        print(\"\\nCache Status:\")\n",
        "        print(f\"Cache key: {cache_key}\")\n",
        "        print(f\"Available cache keys: {list(self.chunk_cache.keys())}\")\n",
        "        print(f\"Chunks cache hit: {cache_key in self.chunk_cache}\")\n",
        "        print(f\"Embeddings cache hit: {cache_key in self.embedding_cache}\")\n",
        "\n",
        "        # First, ensure we have chunks\n",
        "        if cache_key not in self.chunk_cache:\n",
        "            print(f\"\\nERROR: No chunks found in cache for {cache_key}\")\n",
        "            print(\"This suggests chunk creation failed or wasn't called\")\n",
        "            return []\n",
        "\n",
        "        chunks_data = self.chunk_cache[cache_key]\n",
        "        if not chunks_data['texts']:\n",
        "            print(\"\\nERROR: Chunks list is empty\")\n",
        "            print(\"This suggests chunk creation succeeded but produced no chunks\")\n",
        "            return []\n",
        "\n",
        "        print(f\"\\nFound {len(chunks_data['texts'])} chunks to search\")\n",
        "        print(f\"Sample chunk (first 100 chars): {chunks_data['texts'][0][:100]}...\")\n",
        "\n",
        "        try:\n",
        "            if self.embedding_model is None:\n",
        "                print(\"\\nInitializing embedding model\")\n",
        "                self.initialize_embedding_model()\n",
        "\n",
        "            if cache_key not in self.embedding_cache:\n",
        "                print(\"\\nGenerating embeddings for chunks...\")\n",
        "                chunk_embeddings = []\n",
        "\n",
        "                # Process in batches\n",
        "                batch_size = 32\n",
        "                total_batches = (len(chunks_data['texts']) + batch_size - 1) // batch_size\n",
        "\n",
        "                for i in range(0, len(chunks_data['texts']), batch_size):\n",
        "                    batch = chunks_data['texts'][i:i + batch_size]\n",
        "                    print(f\"\\nProcessing batch {i//batch_size + 1}/{total_batches}\")\n",
        "                    print(f\"Batch size: {len(batch)} chunks\")\n",
        "\n",
        "                    batch_embeddings = [self.embedding_model.get_text_embedding(text) for text in batch]\n",
        "                    chunk_embeddings.extend(batch_embeddings)\n",
        "                    print(f\"Total embeddings so far: {len(chunk_embeddings)}\")\n",
        "\n",
        "                print(\"\\nConverting to numpy array...\")\n",
        "                embeddings_array = np.array(chunk_embeddings).astype('float32')\n",
        "                print(f\"Embeddings shape: {embeddings_array.shape}\")\n",
        "\n",
        "                print(\"Normalizing embeddings...\")\n",
        "                norms = np.linalg.norm(embeddings_array, axis=1)[:, np.newaxis]\n",
        "                norms[norms == 0] = 1  # Prevent division by zero\n",
        "                normalized_embeddings = embeddings_array / norms\n",
        "\n",
        "                print(\"Creating FAISS index...\")\n",
        "                dimension = embeddings_array.shape[1]\n",
        "                index = faiss.IndexFlatIP(dimension)\n",
        "                index.add(normalized_embeddings)\n",
        "\n",
        "                self.embedding_cache[cache_key] = {\n",
        "                    'embeddings': embeddings_array,\n",
        "                    'index': index\n",
        "                }\n",
        "                print(\"Embeddings cached successfully\")\n",
        "\n",
        "            print(\"\\nProcessing query...\")\n",
        "            query_embedding = self.embedding_model.get_text_embedding(query)\n",
        "            query_embedding = np.array([query_embedding]).astype('float32')\n",
        "\n",
        "            print(\"Normalizing query embedding...\")\n",
        "            query_norm = np.linalg.norm(query_embedding)\n",
        "            if query_norm == 0:\n",
        "                print(\"ERROR: Zero query vector\")\n",
        "                return []\n",
        "            query_normalized = query_embedding / query_norm\n",
        "\n",
        "            print(f\"\\nSearching for top {k} matches...\")\n",
        "            distances, indices = self.embedding_cache[cache_key]['index'].search(\n",
        "                query_normalized, k\n",
        "            )\n",
        "\n",
        "            print(\"\\nFormatting results...\")\n",
        "            results = []\n",
        "            for score, idx in zip(distances[0], indices[0]):\n",
        "                if idx >= 0 and idx < len(chunks_data['texts']):\n",
        "                    results.append({\n",
        "                        'text': chunks_data['texts'][idx],\n",
        "                        'distance': float(score),\n",
        "                        'strategy': chunk_strategy\n",
        "                    })\n",
        "                    print(f\"\\nMatch {len(results)}:\")\n",
        "                    print(f\"Score: {float(score):.4f}\")\n",
        "                    print(f\"Text preview: {chunks_data['texts'][idx][:100]}...\")\n",
        "\n",
        "            print(f\"\\nTotal matches found: {len(results)}\")\n",
        "            print(\"=\"*50)\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"\\nERROR in cosine search:\")\n",
        "            print(f\"Error type: {type(e).__name__}\")\n",
        "            print(f\"Error message: {str(e)}\")\n",
        "            import traceback\n",
        "            print(\"\\nTraceback:\")\n",
        "            print(traceback.format_exc())\n",
        "            print(\"=\"*50)\n",
        "            return []\n",
        "\n",
        "    def generate_response(self, query: str, context_rag: list, model: Dict) -> dict:\n",
        "        \"\"\"Generate response using provided context with source tracking\"\"\"\n",
        "        try:\n",
        "            if not context_rag:\n",
        "                return {\n",
        "                    \"response_text\": \"No relevant context found.\",\n",
        "                    \"sources\": [],\n",
        "                    \"source_tracking\": {\n",
        "                        \"num_sources_provided\": 0,\n",
        "                        \"source_ids\": [],\n",
        "                        \"verification_status\": \"no_context\"\n",
        "                    },\n",
        "                    \"strategy\": None\n",
        "                }\n",
        "\n",
        "            print(\"\\n=== DEBUG: Context Chunks Passed to LLM ===\")\n",
        "            print(f\"Query: {query}\")\n",
        "            print(f\"Number of chunks: {len(context_rag)}\")\n",
        "\n",
        "            # Generate unique IDs for each source chunk\n",
        "            context_with_ids = []\n",
        "            for idx, doc in enumerate(context_rag):\n",
        "                source_id = f\"src_{idx}\"\n",
        "                context_with_ids.append({\n",
        "                    \"text\": doc['text'],\n",
        "                    \"id\": source_id,\n",
        "                    \"distance\": doc.get('distance', 0)\n",
        "                })\n",
        "                print(f\"\\nChunk {source_id}:\")\n",
        "                print(f\"Distance: {doc.get('distance', 'N/A')}\")\n",
        "                print(\"Text:\", doc['text'])\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            # Format context with source IDs\n",
        "            formatted_context = \"\\n\\n\".join([\n",
        "                f\"[{doc['id']}] {doc['text']}\"\n",
        "                for doc in context_with_ids\n",
        "            ])\n",
        "\n",
        "            prompt = PromptTemplate(template=\"\"\"\n",
        "            Instructions:\n",
        "\n",
        "            You are a helpful assistant who answers questions strictly from the provided context.\n",
        "            Given the context information, provide a direct and concise answer to the question: {query}\n",
        "\n",
        "            Important rules:\n",
        "            1. Only use information present in the context\n",
        "            2. If you don't know or can't find the information, say \"I don't know\"\n",
        "            3. You must cite the source IDs [src_X] for every piece of information you use\n",
        "            4. Do not make assumptions or use external knowledge\n",
        "\n",
        "            You must format your response as a JSON string object, starting with \"LLM_Response:\"\n",
        "\n",
        "            Your answer must follow this exact format:\n",
        "\n",
        "            LLM_Response:\n",
        "            {{\n",
        "                \"response_text\": \"Your detailed answer here with [src_X] citations inline\",\n",
        "                \"sources\": [\n",
        "                    \"Copy and paste here the exact text segments you used, with their source IDs\"\n",
        "                ],\n",
        "                \"source_ids_used\": [\"List of all source IDs referenced in your answer\"]\n",
        "            }}\n",
        "\n",
        "            Context (with source IDs):\n",
        "            ---------------\n",
        "            {context}\n",
        "            ---------------\n",
        "            \"\"\")\n",
        "\n",
        "            model_type = model['type']\n",
        "            llm = model['llm']\n",
        "\n",
        "            chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "            response = chain.invoke({\n",
        "                \"query\": query,\n",
        "                \"context\": formatted_context\n",
        "            })\n",
        "\n",
        "            response_text = response.split(\"LLM_Response:\")[-1].strip()\n",
        "\n",
        "            try:\n",
        "                if '{' in response_text and '}' in response_text:\n",
        "                    json_str = response_text[response_text.find('{'):response_text.rfind('}')+1]\n",
        "                    parsed_response = json.loads(json_str)\n",
        "\n",
        "                    # Verify source usage\n",
        "                    claimed_sources = set(parsed_response.get(\"source_ids_used\", []))\n",
        "                    available_sources = {doc[\"id\"] for doc in context_with_ids}\n",
        "\n",
        "                    verification_status = {\n",
        "                        \"status\": \"verified\" if claimed_sources.issubset(available_sources) else \"source_mismatch\",\n",
        "                        \"claimed_sources\": list(claimed_sources),\n",
        "                        \"available_sources\": list(available_sources),\n",
        "                        \"unauthorized_sources\": list(claimed_sources - available_sources)\n",
        "                    }\n",
        "\n",
        "                    return {\n",
        "                        \"response_text\": parsed_response.get(\"response_text\", response_text),\n",
        "                        \"sources\": parsed_response.get(\"sources\", []),\n",
        "                        \"source_tracking\": {\n",
        "                            \"num_sources_provided\": len(context_with_ids),\n",
        "                            \"source_ids\": [doc[\"id\"] for doc in context_with_ids],\n",
        "                            \"verification_status\": verification_status\n",
        "                        },\n",
        "                        \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                    }\n",
        "                else:\n",
        "                    return {\n",
        "                        \"response_text\": response_text,\n",
        "                        \"sources\": [],\n",
        "                        \"source_tracking\": {\n",
        "                            \"num_sources_provided\": len(context_with_ids),\n",
        "                            \"source_ids\": [doc[\"id\"] for doc in context_with_ids],\n",
        "                            \"verification_status\": {\n",
        "                                \"status\": \"parsing_failed\",\n",
        "                                \"error\": \"Response not in JSON format\"\n",
        "                            }\n",
        "                        },\n",
        "                        \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                    }\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                return {\n",
        "                    \"response_text\": response_text,\n",
        "                    \"sources\": [],\n",
        "                    \"source_tracking\": {\n",
        "                        \"num_sources_provided\": len(context_with_ids),\n",
        "                        \"source_ids\": [doc[\"id\"] for doc in context_with_ids],\n",
        "                        \"verification_status\": {\n",
        "                            \"status\": \"parsing_failed\",\n",
        "                            \"error\": \"JSON decode error\"\n",
        "                        }\n",
        "                    },\n",
        "                    \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {str(e)}\")\n",
        "            return {\n",
        "                \"response_text\": \"An error occurred while generating the response.\",\n",
        "                \"sources\": [],\n",
        "                \"source_tracking\": {\n",
        "                    \"num_sources_provided\": 0,\n",
        "                    \"source_ids\": [],\n",
        "                    \"verification_status\": {\n",
        "                        \"status\": \"error\",\n",
        "                        \"error\": str(e)\n",
        "                    }\n",
        "                },\n",
        "                \"strategy\": None\n",
        "            }"
      ],
      "metadata": {
        "id": "YY5rnivk-bAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ModelConfig Class"
      ],
      "metadata": {
        "id": "ljqi1Qg8j9F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelConfig:\n",
        "    \"\"\"Handles model configuration and management\"\"\"\n",
        "    def __init__(self,\n",
        "                 models: List[Dict],\n",
        "                 temperature: float = 0.3):\n",
        "        self.models = models\n",
        "        self.temperature = temperature\n",
        "        self.current_model = None\n",
        "        self.current_model_name = None\n",
        "\n",
        "\n",
        "    @contextmanager\n",
        "    def load_model(self, model_config: Dict):\n",
        "        \"\"\"Context manager for lazy loading and proper cleanup of models\"\"\"\n",
        "        try:\n",
        "            model_name = model_config[\"name\"]\n",
        "            model_type = model_config[\"type\"]\n",
        "\n",
        "            # Clear any existing model\n",
        "            self.cleanup_current_model()\n",
        "\n",
        "            if model_type == \"mistral_api\":\n",
        "                mistral_api_key = userdata.get('MISTRAL_API_KEY')\n",
        "                self.current_model = {\n",
        "                    'llm': ChatMistralAI(\n",
        "                        model=model_name,\n",
        "                        temperature=self.temperature,\n",
        "                        api_key=mistral_api_key\n",
        "                    ),\n",
        "                    'type': 'mistral_api'\n",
        "                }\n",
        "            else:  # huggingface\n",
        "                print(f\"Loading huggingface model: {model_name}\")\n",
        "\n",
        "                # Empty CUDA cache before loading new model\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                tokenizer = AutoTokenizer.from_pretrained(\n",
        "                    pretrained_model_name_or_path=model_config[\"tokenizer\"],\n",
        "                    trust_remote_code=True,\n",
        "                    use_fast=True,\n",
        "                    padding_side=\"left\"\n",
        "                )\n",
        "\n",
        "                model = AutoModelForCausalLM.from_pretrained(\n",
        "                    pretrained_model_name_or_path=model_name,\n",
        "                    device_map=\"auto\",\n",
        "                    trust_remote_code=True,\n",
        "                    torch_dtype=torch.float16,\n",
        "                    use_cache=True,\n",
        "                    low_cpu_mem_usage=True,\n",
        "                )\n",
        "\n",
        "                pipe = pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_new_tokens=512,\n",
        "                    temperature=self.temperature,\n",
        "                    top_p=0.95,\n",
        "                    top_k=50,\n",
        "                    do_sample=True,\n",
        "                    device_map=\"auto\"\n",
        "                )\n",
        "\n",
        "                self.current_model = {\n",
        "                    'llm': HuggingFacePipeline(pipeline=pipe),\n",
        "                    'type': 'huggingface',\n",
        "                    'model': model,  # Keep reference for cleanup\n",
        "                    'pipe': pipe     # Keep reference for cleanup\n",
        "                }\n",
        "\n",
        "            self.current_model_name = model_name\n",
        "            yield self.current_model\n",
        "\n",
        "        finally:\n",
        "            # Cleanup will happen in cleanup_current_model()\n",
        "            pass\n",
        "\n",
        "    def cleanup_current_model(self):\n",
        "        \"\"\"Clean up the current model and free memory\"\"\"\n",
        "        if self.current_model is not None:\n",
        "            if self.current_model['type'] == 'huggingface':\n",
        "                # Delete model components explicitly\n",
        "                del self.current_model['llm']\n",
        "                del self.current_model['model']\n",
        "                del self.current_model['pipe']\n",
        "\n",
        "                # Clear CUDA cache\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                # Run garbage collection\n",
        "                gc.collect()\n",
        "\n",
        "            self.current_model = None\n",
        "            self.current_model_name = None"
      ],
      "metadata": {
        "id": "tCzG7OE0IiDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ExperimentRunner Class"
      ],
      "metadata": {
        "id": "05gTul4pIW6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperimentRunner:\n",
        "    \"\"\"Handles experiment execution\"\"\"\n",
        "    def __init__(self,\n",
        "                 model_config: ModelConfig,\n",
        "                 questions: List[str],\n",
        "                 chunk_strategies: List[str],\n",
        "                 semantic_enabled: bool = False,\n",
        "                 semantic_thresholds: List[int] = None,\n",
        "                 rag_pipeline: RAGPipeline = None):\n",
        "        self.model_config = model_config\n",
        "        self.questions = questions\n",
        "        self.chunk_strategies = chunk_strategies\n",
        "        self.semantic_enabled = semantic_enabled\n",
        "        self.semantic_thresholds = semantic_thresholds if semantic_enabled else []\n",
        "\n",
        "        # Use existing RAG pipeline or create new one\n",
        "        global _GLOBAL_RAG_PIPELINE\n",
        "        if rag_pipeline:\n",
        "            self.rag_pipeline = rag_pipeline\n",
        "        elif _GLOBAL_RAG_PIPELINE:\n",
        "            self.rag_pipeline = _GLOBAL_RAG_PIPELINE\n",
        "        else:\n",
        "            print(\"Initializing new RAG pipeline\")\n",
        "            _GLOBAL_RAG_PIPELINE = RAGPipeline()\n",
        "            self.rag_pipeline = _GLOBAL_RAG_PIPELINE\n",
        "\n",
        "    def run_experiments(self) -> Dict:\n",
        "        results = {\n",
        "            \"metadata\": {\n",
        "                \"timestamp\": time.strftime(\"%Y%m%d-%H%M%S\"),\n",
        "                \"models_tested\": [model[\"name\"] for model in self.model_config.models],\n",
        "                \"semantic_enabled\": self.semantic_enabled,\n",
        "                \"semantic_thresholds\": self.semantic_thresholds if self.semantic_enabled else [],\n",
        "                \"chunk_strategies\": self.chunk_strategies,\n",
        "                \"temperature\": self.model_config.temperature\n",
        "            },\n",
        "            \"results\": []\n",
        "        }\n",
        "\n",
        "        for model_config in self.model_config.models:\n",
        "            model_name = model_config[\"name\"]\n",
        "            print(f\"\\nTesting model: {model_name}\")\n",
        "\n",
        "            with self.model_config.load_model(model_config) as model:\n",
        "                for strategy in self.chunk_strategies:\n",
        "                    # Handle thresholds based on strategy type\n",
        "                    if strategy == \"semantic\" and self.semantic_enabled:\n",
        "                        thresholds_to_test = self.semantic_thresholds\n",
        "                    else:\n",
        "                        thresholds_to_test = [None]\n",
        "\n",
        "                    for threshold in thresholds_to_test:\n",
        "                        chunks_data = self.rag_pipeline.create_chunks(\n",
        "                            documents,\n",
        "                            threshold=threshold,\n",
        "                            chunk_strategy=strategy\n",
        "                        )\n",
        "\n",
        "                        chunk_stats = {\n",
        "                            \"strategy\": strategy,\n",
        "                            \"threshold\": threshold,\n",
        "                            \"stats\": chunks_data[\"chunk_stats\"]\n",
        "                        }\n",
        "\n",
        "                        for question in self.questions:\n",
        "                            print(f\"Processing question: {question}\")\n",
        "\n",
        "                            context = self.rag_pipeline.run_cosine_search(\n",
        "                                query=question,\n",
        "                                threshold=threshold,\n",
        "                                chunk_strategy=strategy\n",
        "                            )\n",
        "\n",
        "                            answer = self.rag_pipeline.generate_response(\n",
        "                                query=question,\n",
        "                                context_rag=context,\n",
        "                                model=model\n",
        "                            )\n",
        "\n",
        "                            results[\"results\"].append({\n",
        "                                \"model\": model_name,\n",
        "                                \"threshold\": threshold if strategy == \"semantic\" else None,\n",
        "                                \"chunk_strategy\": strategy,\n",
        "                                \"question\": question,\n",
        "                                \"response\": answer,\n",
        "                                \"chunk_stats\": chunk_stats[\"stats\"]\n",
        "                            })\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "8hFyd9G1kC8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluator Class"
      ],
      "metadata": {
        "id": "EpjD-Qz54mfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperimentEvaluator:\n",
        "    \"\"\"Handles pure evaluation logic\"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.client = openai.OpenAI(api_key=api_key)\n",
        "        self.encoder = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "\n",
        "    def _get_baseline_answers(self, questions: List[str], source_docs: List) -> Dict[str, str]:\n",
        "        \"\"\"Get GPT-4o's own answers to the questions as baseline\"\"\"\n",
        "        print(\"\\n=== DEBUG: _get_baseline_answers ===\")\n",
        "        print(f\"Questions received: {questions}\")\n",
        "        print(f\"Number of document parts: {len(source_docs)}\")\n",
        "\n",
        "        # Concatenate all document parts\n",
        "        full_document = \"\\n\\n\".join([doc.text for doc in source_docs])\n",
        "        print(f\"\\nFull document length: {len(full_document)} characters\")\n",
        "\n",
        "        # Print sample from document\n",
        "        print(\"\\nSampling from document:\")\n",
        "        print(\"Start:\", full_document[:200], \"...\")\n",
        "        print(\"Middle:\", full_document[len(full_document)//2:len(full_document)//2 + 200], \"...\")\n",
        "        print(\"End:\", full_document[-200:], \"...\")\n",
        "\n",
        "        baseline_prompt = f\"\"\"Source Document:\n",
        "        {full_document}\n",
        "\n",
        "        Using ONLY the information from the source document above, answer these questions.\n",
        "        - If the exact information is found, provide it with specific numbers\n",
        "        - If information is not found, explicitly state that\n",
        "        - If there are metrics, make sure to include appropriate units\n",
        "\n",
        "        Format your response as a valid JSON object with questions as keys and answers as values.\n",
        "        Keep answers concise and factual.\n",
        "\n",
        "        Questions to answer:\n",
        "        {json.dumps(questions, indent=2)}\"\"\"\n",
        "\n",
        "        try:\n",
        "            print(\"\\n--- Getting Baseline Answers ---\")\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides JSON-formatted answers based on source documents.\"},\n",
        "                    {\"role\": \"user\", \"content\": baseline_prompt}\n",
        "                ],\n",
        "                temperature=0.1\n",
        "            )\n",
        "\n",
        "            content = response.choices[0].message.content\n",
        "            print(\"\\nRaw GPT-4 Response:\")\n",
        "            print(content)\n",
        "\n",
        "            if '{' in content and '}' in content:\n",
        "                json_str = content[content.find('{'):content.rfind('}')+1]\n",
        "                baseline_answers = json.loads(json_str)\n",
        "                print(\"\\nParsed Baseline Answers:\")\n",
        "                print(baseline_answers)\n",
        "                return baseline_answers\n",
        "            print(\"\\nWarning: No JSON structure found in response\")\n",
        "            return {\"error\": \"No JSON structure found\", \"questions\": questions}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError in _get_baseline_answers: {str(e)}\")\n",
        "            return {\"error\": str(e), \"questions\": questions}\n",
        "\n",
        "    def evaluate_experiments(self, experiment_results: Dict, *, source_docs: List) -> Dict:  # Updated signature\n",
        "        \"\"\"Core evaluation logic\"\"\"\n",
        "        try:\n",
        "            print(\"\\n=== DEBUG: evaluate_experiments ===\")\n",
        "            print(\"Getting questions...\")\n",
        "            questions = list(set(result[\"question\"] for result in experiment_results[\"results\"]))\n",
        "            print(f\"Questions extracted: {questions}\")\n",
        "\n",
        "            print(\"\\nGetting baseline answers...\")\n",
        "            baseline_answers = self._get_baseline_answers(questions, source_docs)  # Pass source_docs\n",
        "            print(f\"Baseline answers received: {baseline_answers}\")\n",
        "\n",
        "            model_strategy_combinations = set(\n",
        "                (result[\"model\"],\n",
        "                result[\"chunk_strategy\"],\n",
        "                result[\"threshold\"] if result[\"chunk_strategy\"] == \"semantic\" else None)\n",
        "                for result in experiment_results[\"results\"]\n",
        "            )\n",
        "\n",
        "            all_evaluations = []\n",
        "\n",
        "            for model, strategy, threshold in model_strategy_combinations:\n",
        "                relevant_results = [r for r in experiment_results[\"results\"]\n",
        "                                  if r[\"model\"] == model and\n",
        "                                     r[\"chunk_strategy\"] == strategy and\n",
        "                                     (r[\"threshold\"] == threshold if strategy == \"semantic\" else True)]\n",
        "\n",
        "                for result in relevant_results:\n",
        "                    print(f\"\\nEvaluating response for: {result['question']}\")\n",
        "                    baseline = baseline_answers.get(result[\"question\"], \"No baseline available\")\n",
        "                    print(f\"Using baseline answer: {baseline}\")\n",
        "\n",
        "                    evaluation = self._evaluate_single_response(result, baseline)\n",
        "                    all_evaluations.append(evaluation)\n",
        "\n",
        "            return {\n",
        "                \"metadata\": {\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"model_used\": \"gpt-4o\",\n",
        "                    \"num_combinations_evaluated\": len(model_strategy_combinations),\n",
        "                    \"num_questions_evaluated\": len(questions),\n",
        "                    \"evaluation_status\": \"success\"\n",
        "                },\n",
        "                \"evaluations\": all_evaluations,\n",
        "                \"summary\": self._generate_summary(all_evaluations)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nCritical error in evaluate_experiments: {str(e)}\")\n",
        "            return self._create_default_evaluation(experiment_results)\n",
        "\n",
        "    def _evaluate_single_response(self, result: Dict, baseline: str) -> Dict:\n",
        "        \"\"\"Evaluate a single response with clearer scoring criteria\"\"\"\n",
        "        evaluation_prompt = f\"\"\"Compare and evaluate this response. You must evaluate three separate aspects:\n",
        "\n",
        "    1. ACCURACY - Compare the model's answer against the baseline (ground truth)\n",
        "    2. SOURCE ATTRIBUTION - Check if the model's answer matches its cited sources\n",
        "    3. CONCISESNESS - Check if the model's answer is clear and direct\n",
        "\n",
        "    Question: {result[\"question\"]}\n",
        "\n",
        "    Baseline (Ground Truth): {baseline}\n",
        "\n",
        "    Model Response: {result.get(\"response\", {}).get(\"response_text\", \"\")}\n",
        "    Sources Cited: {json.dumps(result.get(\"response\", {}).get(\"sources\", []), indent=2)}\n",
        "\n",
        "    Scoring Criteria:\n",
        "\n",
        "    1. ACCURACY (0-100):\n",
        "      - Compare ONLY the model's answer against the baseline\n",
        "      - 100: Exact match with baseline (including numbers and units)\n",
        "      - 50: Partially correct but with some errors\n",
        "      - 0: Completely different from baseline or wrong\n",
        "\n",
        "    2. SOURCE ATTRIBUTION (0-100):\n",
        "      - Compare ONLY the model's answer against its cited sources\n",
        "      - 100: Answer exactly matches what appears in cited sources INCLUDING UNITS\n",
        "      - 50: Answer partially matches cited sources\n",
        "      - 0: Answer doesn't match cited sources or no sources cited\n",
        "\n",
        "      Note: For large numbers, different formats are acceptable (e.g., $19,000 million = $19 billion)\n",
        "      BUT the units must match what appears in the source document for full attribution score.\n",
        "      The units in the source document are authoritative.\n",
        "\n",
        "    3. CONCISENESS (0-100):\n",
        "      - 100: Clear, direct answer without extra information\n",
        "      - 50: Contains some irrelevant information\n",
        "      - 0: Verbose or unclear\n",
        "\n",
        "    Note: A response can have perfect source attribution (100) even if the answer is wrong,\n",
        "    as long as it accurately reflects what's in its cited sources.\n",
        "\n",
        "    Provide your evaluation in this exact JSON format:\n",
        "    {{\n",
        "        \"model\": \"{result[\"model\"]}\",\n",
        "        \"chunk_strategy\": \"{result[\"chunk_strategy\"]}\",\n",
        "        \"threshold\": {result[\"threshold\"] if result[\"chunk_strategy\"] == \"semantic\" else \"null\"},\n",
        "        \"question\": \"{result[\"question\"]}\",\n",
        "        \"baseline_answer\": \"{baseline}\",\n",
        "        \"model_response\": {json.dumps(result.get(\"response\", {}), indent=2)},\n",
        "        \"chunk_stats\": {json.dumps(result.get(\"chunk_stats\", {}), indent=2)},\n",
        "        \"scores\": {{\n",
        "            \"accuracy\": <score>,\n",
        "            \"source_attribution\": <score>,\n",
        "            \"conciseness\": <score>\n",
        "        }},\n",
        "        \"composite_score\": <average of scores>,\n",
        "        \"detailed_analysis\": {{\n",
        "            \"accuracy_analysis\": \"Explain ONLY how the answer compares to baseline. Explicitly state if numbers match or differ.\",\n",
        "            \"attribution_analysis\": \"Explain ONLY how well the answer matches its cited sources, regardless of accuracy.\",\n",
        "            \"conciseness_analysis\": \"Explain how clear and direct the answer is\"\n",
        "        }}\n",
        "    }}\n",
        "\n",
        "    Examples:\n",
        "\n",
        "    Bad Response (Perfect Attribution, Wrong Answer):\n",
        "    - If baseline is \"$10,347M\" but model answers \"$19,921M [src_2]\" and src_2 contains \"$19,921M\"\n",
        "    - Accuracy: 0 (completely different from baseline)\n",
        "    - Attribution: 100 (perfectly matches its cited source)\n",
        "\n",
        "    Good Response (Perfect Both):\n",
        "    - If baseline is \"$10,347M\" and model answers \"$10,347M [src_2]\" and src_2 contains \"$10,347M\"\n",
        "    - Accuracy: 100 (matches baseline)\n",
        "    - Attribution: 100 (matches source)\n",
        "    \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert at evaluating response accuracy against both baseline answers and source data.\"},\n",
        "                    {\"role\": \"user\", \"content\": evaluation_prompt}\n",
        "                ],\n",
        "                temperature=0.7,\n",
        "                max_tokens=1000\n",
        "            )\n",
        "\n",
        "            content = response.choices[0].message.content\n",
        "            if '{' in content and '}' in content:\n",
        "                json_str = content[content.find('{'):content.rfind('}')+1]\n",
        "                return json.loads(json_str)\n",
        "            return self._create_default_single_evaluation(result, baseline)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating response: {str(e)}\")\n",
        "            return self._create_default_single_evaluation(result, baseline)\n",
        "\n",
        "    def _create_default_single_evaluation(self, result: Dict, baseline: str) -> Dict:\n",
        "        \"\"\"Create a default evaluation for a single response when evaluation fails\"\"\"\n",
        "        return {\n",
        "            \"model\": result[\"model\"],\n",
        "            \"chunk_strategy\": result[\"chunk_strategy\"],\n",
        "            \"threshold\": result[\"threshold\"] if result[\"chunk_strategy\"] == \"semantic\" else None,\n",
        "            \"question\": result[\"question\"],\n",
        "            \"baseline_answer\": baseline,\n",
        "            \"model_response\": result.get(\"response\", {}),\n",
        "            \"scores\": {\n",
        "                \"source_accuracy\": 0,\n",
        "                \"source_attribution\": 0,\n",
        "                \"conciseness\": 0\n",
        "            },\n",
        "            \"composite_score\": 0,\n",
        "            \"detailed_analysis\": {\n",
        "                \"accuracy_analysis\": \"Evaluation failed\",\n",
        "                \"attribution_analysis\": \"Evaluation failed\",\n",
        "                \"conciseness_analysis\": \"Evaluation failed\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _generate_summary(self, evaluations: List[Dict]) -> Dict:\n",
        "        \"\"\"Generate summary statistics from evaluations with ordered results\"\"\"\n",
        "        if not evaluations:\n",
        "            return {\n",
        "                \"overall_performance\": \"No evaluations available\",\n",
        "                \"optimal_permutation\": \"Not available\",\n",
        "                \"performance_analysis\": \"Evaluation process failed\",\n",
        "                \"chunking_statistics\": {}\n",
        "            }\n",
        "\n",
        "        # Create ordered list of expected configurations\n",
        "        ordered_configs = []\n",
        "        if CHUNKING_CONFIGS[\"semantic_config\"][\"enabled\"]:\n",
        "            for threshold in CHUNKING_CONFIGS[\"semantic_config\"][\"thresholds\"]:\n",
        "                ordered_configs.append((\"semantic\", threshold))\n",
        "\n",
        "        for strategy in [s for s in CHUNKING_CONFIGS[\"strategies\"] if s != \"semantic\"]:\n",
        "            ordered_configs.append((strategy, None))\n",
        "\n",
        "        # Get unique models from evaluations\n",
        "        unique_models = sorted(set(eval[\"model\"] for eval in evaluations))\n",
        "\n",
        "        # Track chunk statistics and performance scores\n",
        "        chunking_statistics = {}\n",
        "        performance_scores = {}\n",
        "        ordered_analysis = {}\n",
        "\n",
        "        # Get document name from the documents list\n",
        "        document_name = os.path.basename(documents[0].metadata.get('file_path', 'Unknown Document'))\n",
        "\n",
        "        # Initialize tracking for all model-strategy combinations\n",
        "        for model in unique_models:\n",
        "            for strategy, threshold in ordered_configs:\n",
        "                key = (model, strategy, threshold)\n",
        "                performance_scores[key] = {\n",
        "                    \"count\": 0,\n",
        "                    \"total_composite\": 0\n",
        "                }\n",
        "\n",
        "        # First pass: calculate scores and collect statistics\n",
        "        best_score = 0\n",
        "        best_config = None\n",
        "\n",
        "        for eval in evaluations:\n",
        "            model = eval[\"model\"]\n",
        "            strategy = eval[\"chunk_strategy\"]\n",
        "            threshold = eval[\"threshold\"] if strategy == \"semantic\" else None\n",
        "            key = (model, strategy, threshold)\n",
        "\n",
        "            # Track performance scores\n",
        "            if key in performance_scores:\n",
        "                performance_scores[key][\"count\"] += 1\n",
        "                performance_scores[key][\"total_composite\"] += eval[\"composite_score\"]\n",
        "\n",
        "            # Track chunk statistics (only need one entry per strategy/threshold combination)\n",
        "            chunk_key = (strategy, threshold)\n",
        "            if chunk_key not in chunking_statistics:\n",
        "                chunk_stats = eval.get(\"chunk_stats\", {})\n",
        "                if chunk_stats:\n",
        "                    if threshold is not None:\n",
        "                        config_str = f\"{document_name} with {strategy} chunking (threshold: {threshold})\"\n",
        "                    else:\n",
        "                        config_str = f\"{document_name} with {strategy} chunking\"\n",
        "\n",
        "                    chunking_statistics[chunk_key] = {\n",
        "                        \"config_str\": config_str,\n",
        "                        \"stats\": {\n",
        "                            \"number_of_chunks\": chunk_stats.get(\"num_chunks\", \"N/A\"),\n",
        "                            \"average_chunk_size\": round(chunk_stats.get(\"avg_chunk_size\", 0), 2),\n",
        "                            \"min_chunk_size\": chunk_stats.get(\"min_chunk_size\", \"N/A\"),\n",
        "                            \"max_chunk_size\": chunk_stats.get(\"max_chunk_size\", \"N/A\")\n",
        "                        }\n",
        "                    }\n",
        "\n",
        "        # Second pass: create ordered performance analysis and chunk statistics\n",
        "        ordered_chunking_stats = {}\n",
        "        for strategy, threshold in ordered_configs:\n",
        "            # Add chunk statistics\n",
        "            chunk_key = (strategy, threshold)\n",
        "            if chunk_key in chunking_statistics:\n",
        "                config_str = chunking_statistics[chunk_key][\"config_str\"]\n",
        "                ordered_chunking_stats[config_str] = chunking_statistics[chunk_key][\"stats\"]\n",
        "\n",
        "            # Add performance analysis for each model\n",
        "            for model in unique_models:\n",
        "                key = (model, strategy, threshold)\n",
        "                scores = performance_scores[key]\n",
        "\n",
        "                if scores[\"count\"] > 0:\n",
        "                    avg_composite = round(scores[\"total_composite\"] / scores[\"count\"], 2)\n",
        "\n",
        "                    if threshold is not None:\n",
        "                        perf_key = f\"{model} with {strategy} chunking (threshold: {threshold})\"\n",
        "                    else:\n",
        "                        perf_key = f\"{model} with {strategy} chunking\"\n",
        "\n",
        "                    ordered_analysis[perf_key] = avg_composite\n",
        "\n",
        "                    if avg_composite > best_score:\n",
        "                        best_score = avg_composite\n",
        "                        best_config = perf_key\n",
        "\n",
        "        # Calculate overall average score\n",
        "        total_score = sum(eval[\"composite_score\"] for eval in evaluations)\n",
        "        avg_score = round(total_score / len(evaluations), 2) if evaluations else 0\n",
        "\n",
        "        return {\n",
        "            \"overall_performance\": f\"Average composite score across all evaluations: {avg_score:.2f}/100\",\n",
        "            \"optimal_permutation\": f\"Best performance: {best_config} (score: {best_score:.2f}/100)\",\n",
        "            \"performance_analysis\": ordered_analysis,\n",
        "            \"chunking_statistics\": ordered_chunking_stats\n",
        "        }\n",
        "\n",
        "\n",
        "    def _create_default_evaluation(self, experiment_results: Dict) -> Dict:\n",
        "        \"\"\"Create a default evaluation result when the evaluation process fails\"\"\"\n",
        "        return {\n",
        "            \"metadata\": {\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"model_used\": \"gpt-4o\",\n",
        "                \"num_combinations_evaluated\": 0,\n",
        "                \"num_questions_evaluated\": 0,\n",
        "                \"evaluation_status\": \"failed\"\n",
        "            },\n",
        "            \"evaluations\": [\n",
        "                self._create_default_single_evaluation(result, \"Evaluation failed\")\n",
        "                for result in experiment_results[\"results\"]\n",
        "            ],\n",
        "            \"summary\": {\n",
        "                \"overall_performance\": \"Evaluation failed\",\n",
        "                \"optimal_permutation\": \"Not available\",\n",
        "                \"performance_analysis\": \"Evaluation process failed\",\n",
        "                \"chunking_statistics\": {}\n",
        "            }\n",
        "        }"
      ],
      "metadata": {
        "id": "1rAK93yw4qCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results Manager Class"
      ],
      "metadata": {
        "id": "lJmy7VbumFk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResultsManager:\n",
        "    \"\"\"Handles formatting, saving, and displaying evaluation results\"\"\"\n",
        "    def __init__(self, save_directory: str):\n",
        "        self.save_directory = save_directory\n",
        "        os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "    def format_results(self, experiment_results: Dict, evaluation_results: Dict) -> Tuple[Dict, Dict]:\n",
        "        \"\"\"Format experiment and evaluation results into structured output\"\"\"\n",
        "        print(\"\\n=== Starting Results Formatting ===\")\n",
        "\n",
        "        # Format experiment results\n",
        "        formatted_experiment = {\n",
        "            \"metadata\": experiment_results.get(\"metadata\", {}),\n",
        "            \"results\": [{\n",
        "                \"model\": result[\"model\"],\n",
        "                \"chunk_strategy\": result[\"chunk_strategy\"],\n",
        "                \"threshold\": result[\"threshold\"],\n",
        "                \"question\": result[\"question\"],\n",
        "                \"response\": {\n",
        "                    \"answer\": result[\"response\"].get(\"response_text\", \"\"),\n",
        "                    \"sources\": result[\"response\"].get(\"sources\", [])\n",
        "                }\n",
        "            } for result in experiment_results[\"results\"]]\n",
        "        }\n",
        "\n",
        "        # Format evaluation results with baseline answer\n",
        "        formatted_evaluation = {\n",
        "            \"metadata\": evaluation_results[\"metadata\"],\n",
        "            \"evaluations\": [{\n",
        "                \"model\": eval.get(\"model\"),\n",
        "                \"chunk_strategy\": eval.get(\"chunk_strategy\"),\n",
        "                \"threshold\": eval.get(\"threshold\"),\n",
        "                \"question\": eval.get(\"question\"),\n",
        "                \"baseline_answer\": eval.get(\"baseline_answer\", \"No baseline available\"),  # Include baseline answer\n",
        "                \"model_response\": eval.get(\"model_response\", {}),\n",
        "                \"scores\": eval.get(\"scores\", {}),\n",
        "                \"composite_score\": eval.get(\"composite_score\"),\n",
        "                \"detailed_analysis\": eval.get(\"detailed_analysis\", {})\n",
        "            } for eval in evaluation_results.get(\"evaluations\", [])],\n",
        "            \"overall_summary\": evaluation_results.get(\"summary\", {})\n",
        "        }\n",
        "\n",
        "        return formatted_experiment, formatted_evaluation\n",
        "\n",
        "    def save_results(self, formatted_experiment: Dict, formatted_evaluation: Dict) -> Tuple[str, str]:\n",
        "        \"\"\"Save formatted results to JSON files\"\"\"\n",
        "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "        experiment_file = f\"{self.save_directory}/experiment_results_{timestamp}.json\"\n",
        "        evaluation_file = f\"{self.save_directory}/evaluation_results_{timestamp}.json\"\n",
        "\n",
        "        for filepath, data in [\n",
        "            (experiment_file, formatted_experiment),\n",
        "            (evaluation_file, formatted_evaluation)\n",
        "        ]:\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return experiment_file, evaluation_file\n",
        "\n",
        "    def display_results(self, evaluation_results: Dict):\n",
        "        \"\"\"Display evaluation results in a clear, formatted manner\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EVALUATION RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Display metadata\n",
        "        metadata = evaluation_results.get(\"metadata\", {})\n",
        "        print(\"\\nMETADATA:\")\n",
        "        print(\"-\"*80)\n",
        "        print(f\"Timestamp:           {metadata.get('timestamp', 'Not available')}\")\n",
        "        print(f\"Model Used:          {metadata.get('model_used', 'Not available')}\")\n",
        "        print(f\"Combinations:        {metadata.get('num_combinations_evaluated', 'Not available')}\")\n",
        "        print(f\"Questions:           {metadata.get('num_questions_evaluated', 'Not available')}\")\n",
        "        print(f\"Evaluation Status:   {metadata.get('evaluation_status', 'Not available')}\")\n",
        "\n",
        "        # Display evaluations\n",
        "        evaluations = evaluation_results.get(\"evaluations\", [])\n",
        "        if evaluations:\n",
        "            print(\"\\nDETAILED EVALUATIONS:\")\n",
        "            print(\"-\"*80)\n",
        "            for eval in evaluations:\n",
        "                print(f\"\\nQuestion: {eval.get('question', 'No question provided')}\")\n",
        "                print(f\"Model: {eval.get('model', 'No model specified')}\")\n",
        "                print(f\"Strategy: {eval.get('chunk_strategy', 'No strategy specified')}\")\n",
        "                if eval.get('threshold'):\n",
        "                    print(f\"Threshold: {eval.get('threshold')}\")\n",
        "\n",
        "                # Display baseline answer\n",
        "                print(\"\\nBaseline Answer:\")\n",
        "                baseline = eval.get('baseline_answer', 'No baseline answer available')\n",
        "                print(textwrap.fill(str(baseline), width=80))\n",
        "\n",
        "                print(\"\\nModel Response:\")\n",
        "                response = eval.get('model_response', {})\n",
        "                response_text = response.get('response_text', 'No response available')\n",
        "                if response_text:\n",
        "                    print(textwrap.fill(str(response_text), width=80))\n",
        "                else:\n",
        "                    print(\"No response available\")\n",
        "\n",
        "                print(\"\\nSource Data:\")\n",
        "                sources = response.get('sources', [])\n",
        "                if sources:\n",
        "                    for source in sources:\n",
        "                        if source:  # Check if source is not empty\n",
        "                            print(textwrap.fill(str(source), width=80))\n",
        "                else:\n",
        "                    print(\"No source data available\")\n",
        "\n",
        "                print(\"\\nScores:\")\n",
        "                scores = eval.get('scores', {})\n",
        "                for metric, score in scores.items():\n",
        "                    print(f\"- {metric.replace('_', ' ').capitalize()}: {score}/100\")\n",
        "                print(f\"Composite Score: {eval.get('composite_score', 0)}/100\")\n",
        "\n",
        "                print(\"\\nDetailed Analysis:\")\n",
        "                analysis = eval.get('detailed_analysis', {})\n",
        "                for aspect, details in analysis.items():\n",
        "                    if details:  # Check if details is not empty\n",
        "                        print(f\"\\n{aspect.replace('_', ' ').capitalize()}:\")\n",
        "                        print(textwrap.fill(str(details), width=80))\n",
        "\n",
        "        # Display summary\n",
        "        summary = evaluation_results.get(\"overall_summary\", {})\n",
        "        if summary:\n",
        "            print(\"\\nOVERALL SUMMARY:\")\n",
        "            print(\"-\"*80)\n",
        "\n",
        "            if \"overall_performance\" in summary:\n",
        "                print(\"\\nOverall Performance:\")\n",
        "                print(textwrap.fill(str(summary[\"overall_performance\"]), width=80))\n",
        "\n",
        "            if \"optimal_permutation\" in summary:\n",
        "                print(\"\\nOptimal Configuration:\")\n",
        "                print(textwrap.fill(str(summary[\"optimal_permutation\"]), width=80))\n",
        "\n",
        "            if \"chunking_statistics\" in summary:\n",
        "                print(\"\\nChunking Statistics:\")\n",
        "                chunk_stats = summary[\"chunking_statistics\"]\n",
        "                for config, stats in chunk_stats.items():\n",
        "                    print(f\"\\n{config}:\")\n",
        "                    print(f\"  Number of Chunks: {stats['number_of_chunks']}\")\n",
        "                    print(f\"  Average Chunk Size: {stats['average_chunk_size']}\")\n",
        "                    print(f\"  Min Chunk Size: {stats['min_chunk_size']}\")\n",
        "                    print(f\"  Max Chunk Size: {stats['max_chunk_size']}\")\n",
        "\n",
        "            if \"performance_analysis\" in summary:\n",
        "                print(\"\\nPerformance Analysis:\")\n",
        "                analysis = summary[\"performance_analysis\"]\n",
        "                if isinstance(analysis, dict):\n",
        "                    for config, score in analysis.items():\n",
        "                        print(f\"{config}: {score:.2f}\")\n",
        "                else:\n",
        "                    print(textwrap.fill(str(analysis), width=80))"
      ],
      "metadata": {
        "id": "vnMb5d8cmKQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "koQ5ZObJC2ek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qdI5iaXYsun",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Initialize configurations with semantic settings from config\n",
        "    semantic_enabled = CHUNKING_CONFIGS[\"semantic_config\"][\"enabled\"]\n",
        "    semantic_thresholds = CHUNKING_CONFIGS[\"semantic_config\"][\"thresholds\"]\n",
        "\n",
        "    # Update strategies list if semantic is enabled\n",
        "    strategies = CHUNKING_CONFIGS[\"strategies\"]\n",
        "    if semantic_enabled:\n",
        "        strategies = [\"semantic\"] + strategies\n",
        "\n",
        "    model_config = ModelConfig(\n",
        "        models=MODEL_CONFIGS[\"models\"],\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    # Initialize experiment runner with flexible configuration\n",
        "    experiment_runner = ExperimentRunner(\n",
        "        model_config=model_config,\n",
        "        questions=QUESTION_CONFIGS[\"questions\"],\n",
        "        chunk_strategies=strategies,\n",
        "        semantic_enabled=semantic_enabled,\n",
        "        semantic_thresholds=semantic_thresholds\n",
        "    )\n",
        "\n",
        "    print(\"Starting experiment with configurations:\")\n",
        "    print(f\"Models: {[model['name'] for model in model_config.models]}\")\n",
        "    if semantic_enabled:\n",
        "        print(f\"Semantic thresholds: {semantic_thresholds}\")\n",
        "    print(f\"Chunk strategies: {strategies}\")\n",
        "    print(f\"Number of questions: {len(QUESTION_CONFIGS['questions'])}\")\n",
        "\n",
        "    # Rest of the main function remains the same\n",
        "    experiment_results = experiment_runner.run_experiments()\n",
        "\n",
        "    print(\"\\nInitializing GPT-4o evaluation...\")\n",
        "    evaluator = ExperimentEvaluator(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "    evaluation_results = evaluator.evaluate_experiments(\n",
        "        experiment_results=experiment_results,\n",
        "        source_docs=documents\n",
        "    )\n",
        "\n",
        "    results_manager = ResultsManager(save_directory=FILE_CONFIGS['save_directory'])\n",
        "\n",
        "    formatted_experiment, formatted_evaluation = results_manager.format_results(\n",
        "        experiment_results=experiment_results,\n",
        "        evaluation_results=evaluation_results\n",
        "    )\n",
        "\n",
        "    experiment_file, evaluation_file = results_manager.save_results(\n",
        "        formatted_experiment=formatted_experiment,\n",
        "        formatted_evaluation=formatted_evaluation\n",
        "    )\n",
        "\n",
        "    results_manager.display_results(evaluation_results=formatted_evaluation)\n",
        "\n",
        "    print(\"\\nExperiment complete!\")\n",
        "    print(f\"Results saved to:\")\n",
        "    print(f\"  Experiment results: {experiment_file}\")\n",
        "    print(f\"  Evaluation results: {evaluation_file}\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return formatted_experiment, formatted_evaluation\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results, evaluation = main()"
      ]
    }
  ]
}