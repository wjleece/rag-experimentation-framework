{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOIX3/eavLrFihf3pa6Tjsn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjleece/rag-experimentation-framework/blob/main/RAG_Experimentation_Framework_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you use this code, please cite:\n",
        "\n",
        "{\n",
        "  title = {RAG Experimentation Framework},\n",
        "\n",
        "  author = {Bill Leece},\n",
        "\n",
        "  year = {2024}\n",
        "}"
      ],
      "metadata": {
        "id": "wZ0kV_UtQn5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "_lHNBLR-92Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers --quiet\n",
        "#!pip install -U optimum --quiet\n",
        "!pip install -U accelerate  --quiet\n",
        "#!pip install -U bitsandbytes  --quiet\n",
        "!pip install -U torch --quiet\n",
        "!pip install -U sentencepiece --quiet\n",
        "!pip install -U llama-index --quiet\n",
        "!pip install -U llama-index-llms-mistralai --quiet\n",
        "!pip install -U llama-index-embeddings-mistralai --quiet\n",
        "!pip install -U llama-index-llms-langchain --quiet\n",
        "!pip install -U langchain --quiet\n",
        "!pip install -U langchain-community --quiet\n",
        "!pip install -U langchain-mistralai --quiet\n",
        "!pip install -U langchain_huggingface --quiet\n",
        "!pip install -U faiss-gpu --quiet"
      ],
      "metadata": {
        "id": "4g_Vs7wgZW-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c33d8681-a611-4943-fbe7-11fd24d03909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.0/189.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.6/254.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "import transformers\n",
        "import torch\n",
        "import gc\n",
        "import openai\n",
        "import json\n",
        "import tiktoken\n",
        "import textwrap\n",
        "import time\n",
        "from google.colab import drive, userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "from llama_index.core import SimpleDirectoryReader, Settings\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "import time\n",
        "from typing import List, Dict, Tuple\n",
        "from contextlib import contextmanager\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any"
      ],
      "metadata": {
        "id": "Ao7eaSfq-TKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get('MISTRAL_API_KEY')\n",
        "api_key = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "YvGHY024-OXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' #Use GPUs when possible"
      ],
      "metadata": {
        "id": "mxAHV7T_-Xlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment Configurations"
      ],
      "metadata": {
        "id": "jkqEV8M_HUKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup configurations\n",
        "MODEL_CONFIGS = {\n",
        "    \"models\": [\n",
        "    #    {\n",
        "    #        \"name\": \"open-mixtral-8x7b\",\n",
        "    #        \"type\": \"mistral_api\",\n",
        "    #        \"tokenizer\": None,  # Not needed for API models\n",
        "    #    },\n",
        "         {\n",
        "            \"name\": \"open-mistral-nemo\",\n",
        "            \"type\": \"mistral_api\",\n",
        "            \"tokenizer\": None,  # Not needed for API models\n",
        "         },\n",
        "   #     {\n",
        "   #         \"name\": \"ministral-8b-latest\",\n",
        "   #         \"type\": \"mistral_api\",\n",
        "   #         \"tokenizer\": None,  # Not needed for API models\n",
        "   #     },\n",
        "   #   {\n",
        "   #         \"name\": \"wjleece/quantized-mistral-7b\",\n",
        "   #         \"type\": \"huggingface_quantized\",\n",
        "   #         \"tokenizer\": \"mistralai/Mixtral-8x7B-v0.1\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "   #          \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "   #             \"load_in_4bit\": True,\n",
        "   #             \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "   #             \"bnb_4bit_quant_type\": \"nf4\",\n",
        "   #             \"bnb_4bit_use_double_quant\": False\n",
        "   #         }\n",
        "   #     },\n",
        "   #   {\n",
        "   #           \"name\": \"wjleece/quantized-mistral-nemo-12b\",\n",
        "   #           \"type\": \"huggingface_quantized\",\n",
        "   #           \"tokenizer\": \"mistralai/Mistral-Nemo-Instruct-2407\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "   #           \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "   #               \"load_in_4bit\": True,\n",
        "   #               \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "   #               \"bnb_4bit_quant_type\": \"nf4\",\n",
        "   #               \"bnb_4bit_use_double_quant\": False\n",
        "   #          }\n",
        "   #       },\n",
        "     #  {\n",
        "     #         \"name\": \"wjleece/quantized-mistral-8b\",\n",
        "     #         \"type\": \"huggingface_quantized\",\n",
        "     #         \"tokenizer\": \"mistralai/Ministral-8B-Instruct-2410\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "     #         \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "     #             \"load_in_4bit\": True,\n",
        "     #             \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "     #             \"bnb_4bit_quant_type\": \"nf4\",\n",
        "     #             \"bnb_4bit_use_double_quant\": False\n",
        "     #         }\n",
        "     #     }\n",
        "       ]\n",
        "}\n",
        "\n",
        "\n",
        "CHUNKING_CONFIGS = {\n",
        "    \"strategies\": [\"semantic\", \"paragraph\", \"header\"], #results will be saved in this order, with thesholds applicable for semantic only (if it is included)\n",
        "    \"thresholds\": [85, 95], #semantic threshold, only applicable for semantic chunking\n",
        "    \"max_chunk_size\": 2048, #only used in paragraph and header chunking\n",
        "    \"chunk_overlap\": 100, #only used in paragraph and header chunking\n",
        "    \"min_chunk_size\": 35 #if a chunk is only 35 characters - about 5 words - just ignore it\n",
        "}\n",
        "\n",
        "QUESTION_CONFIGS = {\n",
        "    \"questions\": [\n",
        "        \"What were cloud revenues in Q2 2024?\",\n",
        "       # \"What were the main drivers of revenue growth in Q2?\",\n",
        "       # \"How much did YouTube ad revenues grow in Q2 in APAC?\",\n",
        "       # \"Can you summarize recent key antitrust matters?\",\n",
        "       # \"Compare the revenue growth across all geographic regions and explain the main factors for each region.\",\n",
        "       # \"Summarize all mentioned risk factors related to international operations.\",\n",
        "       # \"What were the major changes in operating expenses across all categories and their stated reasons?\"\n",
        "    ] #These quetsions should relate to the RAG document --> these are your 'business use cases'\n",
        "}\n",
        "\n",
        "FILE_CONFIGS = {\n",
        "    \"save_directory\": '/content/drive/My Drive/AI/Model_Analysis'\n",
        "}"
      ],
      "metadata": {
        "id": "YDjgk_JhHWkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load RAG Document"
      ],
      "metadata": {
        "id": "e_wxgOGc95sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "documents = SimpleDirectoryReader(input_files=[\"/content/drive/My Drive/AI/Datasets/Google-10-q/goog-10-q-q2-2024.pdf\"]).load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS4Lemk09v9Y",
        "outputId": "ee1694c6-8667-4ea0-e171-f84f7249f601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RAG Pipeline Class"
      ],
      "metadata": {
        "id": "MgLxma5M-bZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global singleton instance\n",
        "_GLOBAL_RAG_PIPELINE = None\n",
        "\n",
        "class RAGPipeline:\n",
        "    def __init__(self):\n",
        "        self.chunk_cache = {}\n",
        "        self.embedding_cache = {}\n",
        "        self.embedding_model = None\n",
        "\n",
        "    @classmethod\n",
        "    def get_instance(cls):\n",
        "        \"\"\"Get or create singleton instance\"\"\"\n",
        "        global _GLOBAL_RAG_PIPELINE\n",
        "        if _GLOBAL_RAG_PIPELINE is None:\n",
        "            _GLOBAL_RAG_PIPELINE = cls()\n",
        "        return _GLOBAL_RAG_PIPELINE\n",
        "\n",
        "\n",
        "    def initialize_embedding_model(self):\n",
        "        \"\"\"Initialize the embedding model if not already initialized\"\"\"\n",
        "        if self.embedding_model is None:\n",
        "            mistral_api_key = userdata.get('MISTRAL_API_KEY')\n",
        "            self.embedding_model = MistralAIEmbedding(\n",
        "                model_name=\"mistral-embed\",\n",
        "                api_key=mistral_api_key\n",
        "            )\n",
        "        return self.embedding_model\n",
        "\n",
        "    def convert_to_markdown_headers(self, text):\n",
        "        \"\"\"Convert document section titles to markdown headers\"\"\"\n",
        "        import re\n",
        "\n",
        "        patterns = [\n",
        "            (r'^(?:ITEM|Section)\\s+\\d+[.:]\\s*(.+)$', '# '),\n",
        "            (r'^\\d+\\.\\d+\\s+(.+)$', '## '),\n",
        "            (r'^\\([a-z]\\)\\s+(.+)$', '### ')\n",
        "        ]\n",
        "\n",
        "        lines = text.split('\\n')\n",
        "        markdown_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            converted = False\n",
        "\n",
        "            for pattern, header_mark in patterns:\n",
        "                if re.match(pattern, line, re.IGNORECASE):\n",
        "                    markdown_lines.append(f\"{header_mark}{line}\")\n",
        "                    converted = True\n",
        "                    break\n",
        "\n",
        "            if not converted:\n",
        "                markdown_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(markdown_lines)\n",
        "\n",
        "\n",
        "    def create_chunks(self, documents: List, threshold: int, chunk_strategy: str = \"semantic\") -> Dict:\n",
        "        \"\"\"Create or retrieve chunks based on specified strategy\"\"\"\n",
        "\n",
        "        MAX_CHUNK_SIZE = CHUNKING_CONFIGS['max_chunk_size']\n",
        "        CHUNK_OVERLAP = CHUNKING_CONFIGS['chunk_overlap']\n",
        "        MIN_CHUNK_SIZE = CHUNKING_CONFIGS['min_chunk_size']\n",
        "\n",
        "        if chunk_strategy == \"semantic\":\n",
        "            cache_key = f\"{chunk_strategy}_{threshold}\"\n",
        "        else:\n",
        "            cache_key = f\"{chunk_strategy}_{MAX_CHUNK_SIZE}\"\n",
        "\n",
        "        if cache_key not in self.chunk_cache:\n",
        "            print(\"\\n=== CHUNK CREATION DEBUG ===\")\n",
        "            print(f\"Strategy: {chunk_strategy}\")\n",
        "            print(f\"Cache key: {cache_key}\")\n",
        "            if chunk_strategy == \"semantic\":\n",
        "                print(f\"Using semantic threshold: {threshold}\")\n",
        "            else:\n",
        "                print(f\"Using max chunk size: {MAX_CHUNK_SIZE} characters with {CHUNK_OVERLAP} character overlap\")\n",
        "\n",
        "            if len(self.chunk_cache) > 2:\n",
        "                oldest_key = min(self.chunk_cache.keys())\n",
        "                if oldest_key != cache_key:\n",
        "                    del self.chunk_cache[oldest_key]\n",
        "                    if oldest_key in self.embedding_cache:\n",
        "                        del self.embedding_cache[oldest_key]\n",
        "                    gc.collect()\n",
        "\n",
        "            if chunk_strategy == \"semantic\":\n",
        "                if self.embedding_model is None:\n",
        "                    self.initialize_embedding_model()\n",
        "\n",
        "                splitter = SemanticSplitterNodeParser(\n",
        "                    buffer_size=1,\n",
        "                    breakpoint_percentile_threshold=threshold,\n",
        "                    embed_model=self.embedding_model\n",
        "                )\n",
        "                nodes = splitter.get_nodes_from_documents(documents)\n",
        "                texts = [node.text for node in nodes]\n",
        "\n",
        "            elif chunk_strategy == \"paragraph\":\n",
        "                text_splitter = RecursiveCharacterTextSplitter(\n",
        "                    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "                    chunk_size=MAX_CHUNK_SIZE,\n",
        "                    chunk_overlap=CHUNK_OVERLAP,\n",
        "                    length_function=len\n",
        "                )\n",
        "                texts = []\n",
        "                for doc in documents:\n",
        "                    chunks = text_splitter.split_text(doc.text)\n",
        "                    texts.extend(chunks)\n",
        "\n",
        "            elif chunk_strategy == \"header\":\n",
        "                headers_to_split_on = [\n",
        "                    (\"#\", \"Header 1\"),\n",
        "                    (\"##\", \"Header 2\"),\n",
        "                    (\"###\", \"Header 3\"),\n",
        "                ]\n",
        "\n",
        "                header_splitter = MarkdownHeaderTextSplitter(\n",
        "                    headers_to_split_on=headers_to_split_on\n",
        "                )\n",
        "\n",
        "                text_splitter = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size=MAX_CHUNK_SIZE,\n",
        "                    chunk_overlap=CHUNK_OVERLAP,\n",
        "                    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "                )\n",
        "\n",
        "                texts = []\n",
        "                for doc in documents:\n",
        "                    md_text = self.convert_to_markdown_headers(doc.text)\n",
        "                    header_splits = header_splitter.split_text(md_text)\n",
        "\n",
        "                    for split in header_splits:\n",
        "                        if len(split.page_content) > MAX_CHUNK_SIZE:\n",
        "                            chunks = text_splitter.split_text(split.page_content)\n",
        "                            texts.extend(chunks)\n",
        "                        else:\n",
        "                            texts.append(split.page_content)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown chunk strategy: {chunk_strategy}\")\n",
        "\n",
        "            # Filter out chunks that are too small\n",
        "            texts = [t for t in texts if len(t.strip()) >= MIN_CHUNK_SIZE]\n",
        "\n",
        "            if texts:\n",
        "                chunk_lengths = [len(t) for t in texts]\n",
        "                chunk_stats = {\n",
        "                    'num_chunks': len(texts),\n",
        "                    'avg_chunk_size': sum(chunk_lengths)/len(texts),\n",
        "                    'min_chunk_size': min(chunk_lengths),\n",
        "                    'max_chunk_size': max(chunk_lengths)\n",
        "                }\n",
        "\n",
        "                self.chunk_cache[cache_key] = {\n",
        "                    'texts': texts,\n",
        "                    'strategy': chunk_strategy,\n",
        "                    'chunk_stats': chunk_stats\n",
        "                }\n",
        "\n",
        "        return self.chunk_cache[cache_key]\n",
        "\n",
        "    def run_cosine_search(self, query: str, threshold: int, chunk_strategy: str = \"semantic\", k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Run cosine similarity search with memory optimization and debugging\"\"\"\n",
        "        print(\"\\n=== COSINE SEARCH DEBUG ===\")\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Strategy: {chunk_strategy}\")\n",
        "        print(f\"Threshold: {threshold}\")\n",
        "        print(f\"Requested k: {k}\")\n",
        "\n",
        "        if self.embedding_model is None:\n",
        "            self.initialize_embedding_model()\n",
        "\n",
        "        FIXED_CHUNK_SIZE = 1024\n",
        "\n",
        "        if chunk_strategy == \"semantic\":\n",
        "            cache_key = f\"{chunk_strategy}_{threshold}\"\n",
        "        else:\n",
        "            cache_key = f\"{chunk_strategy}_{FIXED_CHUNK_SIZE}\"\n",
        "\n",
        "        print(f\"Cache key: {cache_key}\")\n",
        "\n",
        "        if cache_key not in self.embedding_cache:\n",
        "            try:\n",
        "                texts = self.chunk_cache[cache_key]['texts']\n",
        "                print(f\"Creating embeddings for {len(texts)} chunks\")\n",
        "            except KeyError:\n",
        "                print(f\"Warning: No chunks found for strategy {chunk_strategy}\")\n",
        "                return []\n",
        "\n",
        "            batch_size = 32\n",
        "            embeddings = []\n",
        "\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch_texts = texts[i:i + batch_size]\n",
        "                batch_embeddings = [self.embedding_model.get_text_embedding(text)\n",
        "                                  for text in batch_texts]\n",
        "                embeddings.extend(batch_embeddings)\n",
        "\n",
        "                if i % (batch_size * 4) == 0:\n",
        "                    gc.collect()\n",
        "\n",
        "            embeddings_array = np.array(embeddings).astype('float32')\n",
        "            normalized_embeddings = embeddings_array / np.linalg.norm(embeddings_array, axis=1)[:, np.newaxis]\n",
        "\n",
        "            dimension = embeddings_array.shape[1]\n",
        "            cosine_index = faiss.IndexFlatIP(dimension)\n",
        "            cosine_index.add(normalized_embeddings)\n",
        "\n",
        "            self.embedding_cache[cache_key] = {\n",
        "                'embeddings': embeddings_array,\n",
        "                'cosine_index': cosine_index\n",
        "            }\n",
        "\n",
        "        query_vector = self.embedding_model.get_text_embedding(query)\n",
        "        query_vector = np.array([query_vector]).astype('float32')\n",
        "        query_normalized = query_vector / np.linalg.norm(query_vector)\n",
        "\n",
        "        distances, indices = self.embedding_cache[cache_key]['cosine_index'].search(\n",
        "            query_normalized.reshape(1, -1).astype('float32'), k\n",
        "        )\n",
        "\n",
        "        return [\n",
        "            {\n",
        "                'text': self.chunk_cache[cache_key]['texts'][idx],\n",
        "                'distance': float(score),\n",
        "                'strategy': chunk_strategy\n",
        "            }\n",
        "            for score, idx in zip(distances[0], indices[0])\n",
        "        ]\n",
        "\n",
        "    def generate_response(self, query: str, context_rag: list, model: Dict) -> dict:\n",
        "        \"\"\"Generate response using provided context\"\"\"\n",
        "        try:\n",
        "            context_texts = [doc['text'] for doc in context_rag]\n",
        "            if not context_texts:\n",
        "                return {\"response_text\": \"No relevant context found.\", \"sources\": [], \"strategy\": context_rag[0]['strategy'] if context_rag else None}\n",
        "\n",
        "            context = \"\\n\\n\".join(context_texts)\n",
        "\n",
        "            prompt = PromptTemplate(template=\"\"\"\n",
        "            Instructions:\n",
        "\n",
        "            You are a helpful assistant who answers questions from context that has been provided to you.\n",
        "            Given the context information, provide a direct and concise answer to the question: {query}\n",
        "\n",
        "            Focus only on information present in the context. If you don't know the answer, say \"I don't know.\"\n",
        "            You must format your response as a JSON string object, starting with the word \"LLM_Response:\"\n",
        "\n",
        "            Your answer to {query} will be a JSON string object that starts with \"LLM_Response:\" as shown below:\n",
        "\n",
        "            LLM_Response:\n",
        "            {{\n",
        "                \"response_text\": \"Your detailed answer here\",\n",
        "                \"sources\": [\n",
        "                    \"Copy and paste here the exact text segments from the context that you used to generate your answer. Include all relevant segments, verbatim.\"\n",
        "                ]\n",
        "            }}\n",
        "\n",
        "            Important: In your response, the \"sources\" field must contain the exact text passages from the provided context that you used to formulate your answer. Copy these passages word-for-word.\n",
        "\n",
        "            Do not include a hypothetical example in your answer, only include your final answer after \"LLM_Response:\"\n",
        "\n",
        "            The context information that you will use for your answer is below:\n",
        "\n",
        "            ---------------\n",
        "            {context}\n",
        "            ---------------\n",
        "            \"\"\")\n",
        "\n",
        "            model_type = model['type']\n",
        "            llm = model['llm']\n",
        "\n",
        "            chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "            response = chain.invoke({\n",
        "                \"query\": query,\n",
        "                \"context\": context\n",
        "               })\n",
        "\n",
        "            response_text = response.split(\"LLM_Response:\")[-1].strip()\n",
        "\n",
        "            try:\n",
        "                if '{' in response_text and '}' in response_text:\n",
        "                    json_str = response_text[response_text.find('{'):response_text.rfind('}')+1]\n",
        "                    parsed_response = json.loads(json_str)\n",
        "                    return {\n",
        "                        \"response_text\": parsed_response.get(\"response_text\", response_text),\n",
        "                        \"sources\": parsed_response.get(\"sources\", []),\n",
        "                        \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                    }\n",
        "                else:\n",
        "                    return {\n",
        "                        \"response_text\": response_text,\n",
        "                        \"sources\": [],\n",
        "                        \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                    }\n",
        "            except json.JSONDecodeError:\n",
        "                return {\n",
        "                    \"response_text\": response_text,\n",
        "                    \"sources\": [],\n",
        "                    \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {str(e)}\")\n",
        "            return {\"response_text\": \"An error occurred while generating the response.\", \"sources\": []}"
      ],
      "metadata": {
        "id": "YY5rnivk-bAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ModelConfig Class"
      ],
      "metadata": {
        "id": "ljqi1Qg8j9F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelConfig:\n",
        "    \"\"\"Handles model configuration and management\"\"\"\n",
        "    def __init__(self,\n",
        "                 models: List[Dict],\n",
        "                 temperature: float = 0.3):\n",
        "        self.models = models\n",
        "        self.temperature = temperature\n",
        "        self.current_model = None\n",
        "        self.current_model_name = None\n",
        "\n",
        "\n",
        "    @contextmanager\n",
        "    def load_model(self, model_config: Dict):\n",
        "        \"\"\"Context manager for lazy loading and proper cleanup of models\"\"\"\n",
        "        try:\n",
        "            model_name = model_config[\"name\"]\n",
        "            model_type = model_config[\"type\"]\n",
        "\n",
        "            # Clear any existing model\n",
        "            self.cleanup_current_model()\n",
        "\n",
        "            if model_type == \"mistral_api\":\n",
        "                mistral_api_key = userdata.get('MISTRAL_API_KEY')\n",
        "                self.current_model = {\n",
        "                    'llm': ChatMistralAI(\n",
        "                        model=model_name,\n",
        "                        temperature=self.temperature,\n",
        "                        api_key=mistral_api_key\n",
        "                    ),\n",
        "                    'type': 'mistral_api'\n",
        "                }\n",
        "            else:  # huggingface_quantized\n",
        "                print(f\"Loading quantized model: {model_name}\")\n",
        "\n",
        "                # Empty CUDA cache before loading new model\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                tokenizer = AutoTokenizer.from_pretrained(\n",
        "                    pretrained_model_name_or_path=model_config[\"tokenizer\"],\n",
        "                    trust_remote_code=True,\n",
        "                    use_fast=True,\n",
        "                    padding_side=\"left\"\n",
        "                )\n",
        "\n",
        "                model = AutoModelForCausalLM.from_pretrained(\n",
        "                    pretrained_model_name_or_path=model_name,\n",
        "                    device_map=\"auto\",\n",
        "                    trust_remote_code=True,\n",
        "                    torch_dtype=torch.float16,\n",
        "                    use_cache=True,\n",
        "                    low_cpu_mem_usage=True,\n",
        "                )\n",
        "\n",
        "                pipe = pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_new_tokens=512,\n",
        "                    temperature=self.temperature,\n",
        "                    top_p=0.95,\n",
        "                    top_k=50,\n",
        "                    do_sample=True,\n",
        "                    device_map=\"auto\"\n",
        "                )\n",
        "\n",
        "                self.current_model = {\n",
        "                    'llm': HuggingFacePipeline(pipeline=pipe),\n",
        "                    'type': 'huggingface_quantized',\n",
        "                    'model': model,  # Keep reference for cleanup\n",
        "                    'pipe': pipe     # Keep reference for cleanup\n",
        "                }\n",
        "\n",
        "            self.current_model_name = model_name\n",
        "            yield self.current_model\n",
        "\n",
        "        finally:\n",
        "            # Cleanup will happen in cleanup_current_model()\n",
        "            pass\n",
        "\n",
        "    def cleanup_current_model(self):\n",
        "        \"\"\"Clean up the current model and free memory\"\"\"\n",
        "        if self.current_model is not None:\n",
        "            if self.current_model['type'] == 'huggingface_quantized':\n",
        "                # Delete model components explicitly\n",
        "                del self.current_model['llm']\n",
        "                del self.current_model['model']\n",
        "                del self.current_model['pipe']\n",
        "\n",
        "                # Clear CUDA cache\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                # Run garbage collection\n",
        "                gc.collect()\n",
        "\n",
        "            self.current_model = None\n",
        "            self.current_model_name = None"
      ],
      "metadata": {
        "id": "tCzG7OE0IiDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ExperimentRunner Class"
      ],
      "metadata": {
        "id": "05gTul4pIW6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperimentRunner:\n",
        "    \"\"\"Handles experiment execution\"\"\"\n",
        "    def __init__(self,\n",
        "                 model_config: ModelConfig,\n",
        "                 thresholds: List[int],\n",
        "                 questions: List[str],\n",
        "                 chunk_strategies: List[str],\n",
        "                 rag_pipeline: RAGPipeline = None):\n",
        "        self.model_config = model_config\n",
        "        self.thresholds = thresholds\n",
        "        self.questions = questions\n",
        "        self.chunk_strategies = chunk_strategies\n",
        "\n",
        "        # Use existing RAG pipeline or create new one\n",
        "        global _GLOBAL_RAG_PIPELINE\n",
        "        if rag_pipeline:\n",
        "            self.rag_pipeline = rag_pipeline\n",
        "        elif _GLOBAL_RAG_PIPELINE:\n",
        "            self.rag_pipeline = _GLOBAL_RAG_PIPELINE\n",
        "        else:\n",
        "            print(\"Initializing new RAG pipeline\")\n",
        "            _GLOBAL_RAG_PIPELINE = RAGPipeline()\n",
        "            self.rag_pipeline = _GLOBAL_RAG_PIPELINE\n",
        "\n",
        "    def run_experiments(self) -> Dict:\n",
        "        results = {\n",
        "            \"metadata\": {\n",
        "                \"timestamp\": time.strftime(\"%Y%m%d-%H%M%S\"),\n",
        "                \"models_tested\": [model[\"name\"] for model in self.model_config.models],\n",
        "                \"thresholds_tested\": self.thresholds,\n",
        "                \"chunk_strategies_tested\": self.chunk_strategies,\n",
        "                \"temperature\": self.model_config.temperature\n",
        "            },\n",
        "            \"results\": []\n",
        "        }\n",
        "\n",
        "        for strategy in self.chunk_strategies:\n",
        "            if strategy == \"semantic\":\n",
        "                thresholds_to_test = self.thresholds\n",
        "            else:\n",
        "                thresholds_to_test = [None]\n",
        "\n",
        "            for threshold in thresholds_to_test:\n",
        "                actual_threshold = threshold if strategy == \"semantic\" else 0\n",
        "\n",
        "                # Get chunks and their stats\n",
        "                chunks_data = self.rag_pipeline.create_chunks(\n",
        "                    documents,\n",
        "                    threshold=actual_threshold,\n",
        "                    chunk_strategy=strategy\n",
        "                )\n",
        "\n",
        "                # Store chunk stats in a format that will persist through the pipeline\n",
        "                chunk_stats = {\n",
        "                    \"strategy\": strategy,\n",
        "                    \"threshold\": threshold,\n",
        "                    \"stats\": chunks_data[\"chunk_stats\"]\n",
        "                }\n",
        "\n",
        "                for model_config in self.model_config.models:\n",
        "                    model_name = model_config[\"name\"]\n",
        "                    print(f\"\\nTesting model: {model_name}\")\n",
        "\n",
        "                    with self.model_config.load_model(model_config) as model:\n",
        "                        for question in self.questions:\n",
        "                            print(f\"Processing question: {question}\")\n",
        "\n",
        "                            context = self.rag_pipeline.run_cosine_search(\n",
        "                                query=question,\n",
        "                                threshold=threshold,\n",
        "                                chunk_strategy=strategy\n",
        "                            )\n",
        "\n",
        "                            answer = self.rag_pipeline.generate_response(\n",
        "                                query=question,\n",
        "                                context_rag=context,\n",
        "                                model=model\n",
        "                            )\n",
        "\n",
        "                            # Include chunk stats directly in the results\n",
        "                            results[\"results\"].append({\n",
        "                                \"model\": model_name,\n",
        "                                \"threshold\": threshold if strategy == \"semantic\" else None,\n",
        "                                \"chunk_strategy\": strategy,\n",
        "                                \"question\": question,\n",
        "                                \"response\": answer,\n",
        "                                \"chunk_stats\": chunk_stats[\"stats\"]  # Include the complete chunk stats here\n",
        "                            })\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "8hFyd9G1kC8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluator Class"
      ],
      "metadata": {
        "id": "EpjD-Qz54mfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperimentEvaluator:\n",
        "    \"\"\"Handles pure evaluation logic\"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.client = openai.OpenAI(api_key=api_key)\n",
        "        self.encoder = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "\n",
        "    def _get_baseline_answers(self, questions: List[str], source_docs: List) -> Dict[str, str]:\n",
        "        \"\"\"Get GPT-4o's own answers to the questions as baseline\"\"\"\n",
        "        print(\"\\n=== DEBUG: _get_baseline_answers ===\")\n",
        "        print(f\"Questions received: {questions}\")\n",
        "        print(f\"Number of document parts: {len(source_docs)}\")\n",
        "\n",
        "        # Concatenate all document parts\n",
        "        full_document = \"\\n\\n\".join([doc.text for doc in source_docs])\n",
        "        print(f\"\\nFull document length: {len(full_document)} characters\")\n",
        "\n",
        "        # Print sample from document\n",
        "        print(\"\\nSampling from document:\")\n",
        "        print(\"Start:\", full_document[:200], \"...\")\n",
        "        print(\"Middle:\", full_document[len(full_document)//2:len(full_document)//2 + 200], \"...\")\n",
        "        print(\"End:\", full_document[-200:], \"...\")\n",
        "\n",
        "        baseline_prompt = f\"\"\"Source Document:\n",
        "        {full_document}\n",
        "\n",
        "        Using ONLY the information from the source document above, answer these questions.\n",
        "        - If the exact information is found, provide it with specific numbers\n",
        "        - If information is not found, explicitly state that\n",
        "        - If there are metrics, make sure to include appropriate units\n",
        "\n",
        "        Format your response as a valid JSON object with questions as keys and answers as values.\n",
        "        Keep answers concise and factual.\n",
        "\n",
        "        Questions to answer:\n",
        "        {json.dumps(questions, indent=2)}\"\"\"\n",
        "\n",
        "        try:\n",
        "            print(\"\\n--- Getting Baseline Answers ---\")\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides JSON-formatted answers based on source documents.\"},\n",
        "                    {\"role\": \"user\", \"content\": baseline_prompt}\n",
        "                ],\n",
        "                temperature=0.1\n",
        "            )\n",
        "\n",
        "            content = response.choices[0].message.content\n",
        "            print(\"\\nRaw GPT-4 Response:\")\n",
        "            print(content)\n",
        "\n",
        "            if '{' in content and '}' in content:\n",
        "                json_str = content[content.find('{'):content.rfind('}')+1]\n",
        "                baseline_answers = json.loads(json_str)\n",
        "                print(\"\\nParsed Baseline Answers:\")\n",
        "                print(baseline_answers)\n",
        "                return baseline_answers\n",
        "            print(\"\\nWarning: No JSON structure found in response\")\n",
        "            return {\"error\": \"No JSON structure found\", \"questions\": questions}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError in _get_baseline_answers: {str(e)}\")\n",
        "            return {\"error\": str(e), \"questions\": questions}\n",
        "\n",
        "    def evaluate_experiments(self, experiment_results: Dict, *, source_docs: List) -> Dict:  # Updated signature\n",
        "        \"\"\"Core evaluation logic\"\"\"\n",
        "        try:\n",
        "            print(\"\\n=== DEBUG: evaluate_experiments ===\")\n",
        "            print(\"Getting questions...\")\n",
        "            questions = list(set(result[\"question\"] for result in experiment_results[\"results\"]))\n",
        "            print(f\"Questions extracted: {questions}\")\n",
        "\n",
        "            print(\"\\nGetting baseline answers...\")\n",
        "            baseline_answers = self._get_baseline_answers(questions, source_docs)  # Pass source_docs\n",
        "            print(f\"Baseline answers received: {baseline_answers}\")\n",
        "\n",
        "            model_strategy_combinations = set(\n",
        "                (result[\"model\"],\n",
        "                result[\"chunk_strategy\"],\n",
        "                result[\"threshold\"] if result[\"chunk_strategy\"] == \"semantic\" else None)\n",
        "                for result in experiment_results[\"results\"]\n",
        "            )\n",
        "\n",
        "            all_evaluations = []\n",
        "\n",
        "            for model, strategy, threshold in model_strategy_combinations:\n",
        "                relevant_results = [r for r in experiment_results[\"results\"]\n",
        "                                  if r[\"model\"] == model and\n",
        "                                     r[\"chunk_strategy\"] == strategy and\n",
        "                                     (r[\"threshold\"] == threshold if strategy == \"semantic\" else True)]\n",
        "\n",
        "                for result in relevant_results:\n",
        "                    print(f\"\\nEvaluating response for: {result['question']}\")\n",
        "                    baseline = baseline_answers.get(result[\"question\"], \"No baseline available\")\n",
        "                    print(f\"Using baseline answer: {baseline}\")\n",
        "\n",
        "                    evaluation = self._evaluate_single_response(result, baseline)\n",
        "                    all_evaluations.append(evaluation)\n",
        "\n",
        "            return {\n",
        "                \"metadata\": {\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"model_used\": \"gpt-4o\",\n",
        "                    \"num_combinations_evaluated\": len(model_strategy_combinations),\n",
        "                    \"num_questions_evaluated\": len(questions),\n",
        "                    \"evaluation_status\": \"success\"\n",
        "                },\n",
        "                \"evaluations\": all_evaluations,\n",
        "                \"summary\": self._generate_summary(all_evaluations)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nCritical error in evaluate_experiments: {str(e)}\")\n",
        "            return self._create_default_evaluation(experiment_results)\n",
        "\n",
        "    def _evaluate_single_response(self, result: Dict, baseline: str) -> Dict:\n",
        "        \"\"\"Evaluate a single response against both baseline and source data\"\"\"\n",
        "        evaluation_prompt = f\"\"\"Compare and evaluate this response:\n",
        "\n",
        "        Question: {result[\"question\"]}\n",
        "\n",
        "        Baseline Answer from GPT-4: {baseline}\n",
        "        Source Data: {json.dumps(result.get(\"response\", {}).get(\"sources\", []), indent=2)}\n",
        "        Model Response: {json.dumps(result.get(\"response\", {}).get(\"response_text\", \"\"), indent=2)}\n",
        "\n",
        "        Score the response on these criteria (0-100):\n",
        "        - Source Accuracy: How well the response matches the actual source data provided\n",
        "        - Source Attribution: Whether appropriate source text is cited as evidence\n",
        "        - Conciseness: Clear, direct answer without extra information\n",
        "\n",
        "        Provide your evaluation in this exact JSON format:\n",
        "        {{\n",
        "            \"model\": \"{result[\"model\"]}\",\n",
        "            \"chunk_strategy\": \"{result[\"chunk_strategy\"]}\",\n",
        "            \"threshold\": {result[\"threshold\"] if result[\"chunk_strategy\"] == \"semantic\" else \"null\"},\n",
        "            \"question\": \"{result[\"question\"]}\",\n",
        "            \"baseline_answer\": \"{baseline}\",\n",
        "            \"model_response\": {json.dumps(result.get(\"response\", {}), indent=2)},\n",
        "            \"chunk_stats\": {json.dumps(result.get(\"chunk_stats\", {}), indent=2)},\n",
        "            \"scores\": {{\n",
        "                \"source_accuracy\": <score>,\n",
        "                \"source_attribution\": <score>,\n",
        "                \"conciseness\": <score>\n",
        "            }},\n",
        "            \"composite_score\": <average of scores>,\n",
        "            \"detailed_analysis\": {{\n",
        "                \"accuracy_analysis\": \"explanation focusing on match with source data, including comparison with baseline\",\n",
        "                \"attribution_analysis\": \"explanation of source usage\",\n",
        "                \"conciseness_analysis\": \"explanation of clarity and directness\"\n",
        "            }}\n",
        "        }}\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert at evaluating response accuracy against both baseline answers and source data.\"},\n",
        "                    {\"role\": \"user\", \"content\": evaluation_prompt}\n",
        "                ],\n",
        "                temperature=0.7,\n",
        "                max_tokens=1000\n",
        "            )\n",
        "\n",
        "            content = response.choices[0].message.content\n",
        "            if '{' in content and '}' in content:\n",
        "                json_str = content[content.find('{'):content.rfind('}')+1]\n",
        "                return json.loads(json_str)\n",
        "            return self._create_default_single_evaluation(result, baseline)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating response: {str(e)}\")\n",
        "            return self._create_default_single_evaluation(result, baseline)\n",
        "\n",
        "    def _create_default_single_evaluation(self, result: Dict, baseline: str) -> Dict:\n",
        "        \"\"\"Create a default evaluation for a single response when evaluation fails\"\"\"\n",
        "        return {\n",
        "            \"model\": result[\"model\"],\n",
        "            \"chunk_strategy\": result[\"chunk_strategy\"],\n",
        "            \"threshold\": result[\"threshold\"] if result[\"chunk_strategy\"] == \"semantic\" else None,\n",
        "            \"question\": result[\"question\"],\n",
        "            \"baseline_answer\": baseline,\n",
        "            \"model_response\": result.get(\"response\", {}),\n",
        "            \"scores\": {\n",
        "                \"source_accuracy\": 0,\n",
        "                \"source_attribution\": 0,\n",
        "                \"conciseness\": 0\n",
        "            },\n",
        "            \"composite_score\": 0,\n",
        "            \"detailed_analysis\": {\n",
        "                \"accuracy_analysis\": \"Evaluation failed\",\n",
        "                \"attribution_analysis\": \"Evaluation failed\",\n",
        "                \"conciseness_analysis\": \"Evaluation failed\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _generate_summary(self, evaluations: List[Dict]) -> Dict:\n",
        "        \"\"\"Generate summary statistics from evaluations with ordered results\"\"\"\n",
        "        if not evaluations:\n",
        "            return {\n",
        "                \"overall_performance\": \"No evaluations available\",\n",
        "                \"optimal_permutation\": \"Not available\",\n",
        "                \"performance_analysis\": \"Evaluation process failed\",\n",
        "                \"chunking_statistics\": {}\n",
        "            }\n",
        "\n",
        "        # Create ordered list of expected configurations\n",
        "        ordered_configs = []\n",
        "        for strategy in CHUNKING_CONFIGS[\"strategies\"]:\n",
        "            if strategy == \"semantic\":\n",
        "                for threshold in CHUNKING_CONFIGS[\"thresholds\"]:\n",
        "                    ordered_configs.append((strategy, threshold))\n",
        "            else:\n",
        "                ordered_configs.append((strategy, None))\n",
        "\n",
        "        # Calculate scores while maintaining order\n",
        "        strategy_scores = {}\n",
        "        best_score = 0\n",
        "        best_config = None\n",
        "        ordered_analysis = {}\n",
        "\n",
        "        # Track chunk statistics with ordered dictionary\n",
        "        chunking_statistics = {}\n",
        "        ordered_chunking_statistics = {}\n",
        "\n",
        "        # First pass: calculate average scores and collect chunk statistics\n",
        "        for eval in evaluations:\n",
        "            key = (eval[\"chunk_strategy\"],\n",
        "                  eval[\"threshold\"] if eval[\"chunk_strategy\"] == \"semantic\" else None)\n",
        "\n",
        "            # Handling scores\n",
        "            if key not in strategy_scores:\n",
        "                strategy_scores[key] = {\n",
        "                    \"count\": 0,\n",
        "                    \"total_composite\": 0\n",
        "                }\n",
        "\n",
        "            scores = strategy_scores[key]\n",
        "            scores[\"count\"] += 1\n",
        "            scores[\"total_composite\"] += eval[\"composite_score\"]\n",
        "\n",
        "            # Check if this configuration's chunk stats are already recorded\n",
        "            if key not in chunking_statistics:\n",
        "                # Extract chunk stats from the evaluation\n",
        "                chunk_stats = eval.get(\"chunk_stats\", {})\n",
        "                if chunk_stats:  # Only add if we have stats\n",
        "                    if eval[\"threshold\"] is not None:\n",
        "                        config_str = f\"{eval['model']} with {eval['chunk_strategy']} chunking (threshold: {eval['threshold']})\"\n",
        "                    else:\n",
        "                        config_str = f\"{eval['model']} with {eval['chunk_strategy']} chunking\"\n",
        "\n",
        "                    chunking_statistics[key] = {\n",
        "                        \"config_str\": config_str,\n",
        "                        \"stats\": {\n",
        "                            \"number_of_chunks\": chunk_stats.get(\"num_chunks\", \"N/A\"),\n",
        "                            \"average_chunk_size\": round(chunk_stats.get(\"avg_chunk_size\", 0), 2),\n",
        "                            \"min_chunk_size\": chunk_stats.get(\"min_chunk_size\", \"N/A\"),\n",
        "                            \"max_chunk_size\": chunk_stats.get(\"max_chunk_size\", \"N/A\")\n",
        "                        }\n",
        "                    }\n",
        "\n",
        "        # Second pass: create ordered performance analysis and ordered chunk statistics\n",
        "        for strategy, threshold in ordered_configs:\n",
        "            key = (strategy, threshold)\n",
        "            if key in strategy_scores:\n",
        "                avg_composite = strategy_scores[key][\"total_composite\"] / strategy_scores[key][\"count\"]\n",
        "\n",
        "                if key in chunking_statistics:\n",
        "                    config_str = chunking_statistics[key][\"config_str\"]\n",
        "                    ordered_chunking_statistics[config_str] = chunking_statistics[key][\"stats\"]\n",
        "                    ordered_analysis[config_str] = avg_composite\n",
        "\n",
        "                if avg_composite > best_score:\n",
        "                    best_score = avg_composite\n",
        "                    best_config = chunking_statistics[key][\"config_str\"]\n",
        "\n",
        "        return {\n",
        "            \"overall_performance\": f\"Average composite score across all evaluations: {sum(e['composite_score'] for e in evaluations)/len(evaluations):.2f}/100\",\n",
        "            \"optimal_permutation\": f\"Best performance: {best_config} (score: {best_score:.2f}/100)\",\n",
        "            \"performance_analysis\": ordered_analysis,\n",
        "            \"chunking_statistics\": ordered_chunking_statistics\n",
        "        }\n",
        "\n",
        "\n",
        "    def _create_default_evaluation(self, experiment_results: Dict) -> Dict:\n",
        "        \"\"\"Create a default evaluation result when the evaluation process fails\"\"\"\n",
        "        return {\n",
        "            \"metadata\": {\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"model_used\": \"gpt-4o\",\n",
        "                \"num_combinations_evaluated\": 0,\n",
        "                \"num_questions_evaluated\": 0,\n",
        "                \"evaluation_status\": \"failed\"\n",
        "            },\n",
        "            \"evaluations\": [\n",
        "                self._create_default_single_evaluation(result, \"Evaluation failed\")\n",
        "                for result in experiment_results[\"results\"]\n",
        "            ],\n",
        "            \"summary\": {\n",
        "                \"overall_performance\": \"Evaluation failed\",\n",
        "                \"optimal_permutation\": \"Not available\",\n",
        "                \"performance_analysis\": \"Evaluation process failed\",\n",
        "                \"chunking_statistics\": {}\n",
        "            }\n",
        "        }"
      ],
      "metadata": {
        "id": "1rAK93yw4qCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results Manager Class"
      ],
      "metadata": {
        "id": "lJmy7VbumFk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResultsManager:\n",
        "    \"\"\"Handles formatting, saving, and displaying evaluation results\"\"\"\n",
        "    def __init__(self, save_directory: str):\n",
        "        self.save_directory = save_directory\n",
        "        os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "    def format_results(self, experiment_results: Dict, evaluation_results: Dict) -> Tuple[Dict, Dict]:\n",
        "        \"\"\"Format experiment and evaluation results into structured output\"\"\"\n",
        "        print(\"\\n=== Starting Results Formatting ===\")\n",
        "\n",
        "        # Format experiment results\n",
        "        formatted_experiment = {\n",
        "            \"metadata\": experiment_results.get(\"metadata\", {}),\n",
        "            \"results\": [{\n",
        "                \"model\": result[\"model\"],\n",
        "                \"chunk_strategy\": result[\"chunk_strategy\"],\n",
        "                \"threshold\": result[\"threshold\"],\n",
        "                \"question\": result[\"question\"],\n",
        "                \"response\": {\n",
        "                    \"answer\": result[\"response\"].get(\"response_text\", \"\"),\n",
        "                    \"sources\": result[\"response\"].get(\"sources\", [])\n",
        "                }\n",
        "            } for result in experiment_results[\"results\"]]\n",
        "        }\n",
        "\n",
        "        # Format evaluation results with baseline answer\n",
        "        formatted_evaluation = {\n",
        "            \"metadata\": evaluation_results[\"metadata\"],\n",
        "            \"evaluations\": [{\n",
        "                \"model\": eval.get(\"model\"),\n",
        "                \"chunk_strategy\": eval.get(\"chunk_strategy\"),\n",
        "                \"threshold\": eval.get(\"threshold\"),\n",
        "                \"question\": eval.get(\"question\"),\n",
        "                \"baseline_answer\": eval.get(\"baseline_answer\", \"No baseline available\"),  # Include baseline answer\n",
        "                \"model_response\": eval.get(\"model_response\", {}),\n",
        "                \"scores\": eval.get(\"scores\", {}),\n",
        "                \"composite_score\": eval.get(\"composite_score\"),\n",
        "                \"detailed_analysis\": eval.get(\"detailed_analysis\", {})\n",
        "            } for eval in evaluation_results.get(\"evaluations\", [])],\n",
        "            \"overall_summary\": evaluation_results.get(\"summary\", {})\n",
        "        }\n",
        "\n",
        "        return formatted_experiment, formatted_evaluation\n",
        "\n",
        "    def save_results(self, formatted_experiment: Dict, formatted_evaluation: Dict) -> Tuple[str, str]:\n",
        "        \"\"\"Save formatted results to JSON files\"\"\"\n",
        "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "        experiment_file = f\"{self.save_directory}/experiment_results_{timestamp}.json\"\n",
        "        evaluation_file = f\"{self.save_directory}/evaluation_results_{timestamp}.json\"\n",
        "\n",
        "        for filepath, data in [\n",
        "            (experiment_file, formatted_experiment),\n",
        "            (evaluation_file, formatted_evaluation)\n",
        "        ]:\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return experiment_file, evaluation_file\n",
        "\n",
        "    def display_results(self, evaluation_results: Dict):\n",
        "        \"\"\"Display evaluation results in a clear, formatted manner\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EVALUATION RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Display metadata\n",
        "        metadata = evaluation_results.get(\"metadata\", {})\n",
        "        print(\"\\nMETADATA:\")\n",
        "        print(\"-\"*80)\n",
        "        print(f\"Timestamp:           {metadata.get('timestamp', 'Not available')}\")\n",
        "        print(f\"Model Used:          {metadata.get('model_used', 'Not available')}\")\n",
        "        print(f\"Combinations:        {metadata.get('num_combinations_evaluated', 'Not available')}\")\n",
        "        print(f\"Questions:           {metadata.get('num_questions_evaluated', 'Not available')}\")\n",
        "        print(f\"Evaluation Status:   {metadata.get('evaluation_status', 'Not available')}\")\n",
        "\n",
        "        # Display evaluations\n",
        "        evaluations = evaluation_results.get(\"evaluations\", [])\n",
        "        if evaluations:\n",
        "            print(\"\\nDETAILED EVALUATIONS:\")\n",
        "            print(\"-\"*80)\n",
        "            for eval in evaluations:\n",
        "                print(f\"\\nQuestion: {eval.get('question', 'No question provided')}\")\n",
        "                print(f\"Model: {eval.get('model', 'No model specified')}\")\n",
        "                print(f\"Strategy: {eval.get('chunk_strategy', 'No strategy specified')}\")\n",
        "                if eval.get('threshold'):\n",
        "                    print(f\"Threshold: {eval.get('threshold')}\")\n",
        "\n",
        "                # Display baseline answer\n",
        "                print(\"\\nBaseline Answer:\")\n",
        "                baseline = eval.get('baseline_answer', 'No baseline answer available')\n",
        "                print(textwrap.fill(str(baseline), width=80))\n",
        "\n",
        "                print(\"\\nModel Response:\")\n",
        "                response = eval.get('model_response', {})\n",
        "                response_text = response.get('response_text', 'No response available')\n",
        "                if response_text:\n",
        "                    print(textwrap.fill(str(response_text), width=80))\n",
        "                else:\n",
        "                    print(\"No response available\")\n",
        "\n",
        "                print(\"\\nSource Data:\")\n",
        "                sources = response.get('sources', [])\n",
        "                if sources:\n",
        "                    for source in sources:\n",
        "                        if source:  # Check if source is not empty\n",
        "                            print(textwrap.fill(str(source), width=80))\n",
        "                else:\n",
        "                    print(\"No source data available\")\n",
        "\n",
        "                print(\"\\nScores:\")\n",
        "                scores = eval.get('scores', {})\n",
        "                for metric, score in scores.items():\n",
        "                    print(f\"- {metric.replace('_', ' ').capitalize()}: {score}/100\")\n",
        "                print(f\"Composite Score: {eval.get('composite_score', 0)}/100\")\n",
        "\n",
        "                print(\"\\nDetailed Analysis:\")\n",
        "                analysis = eval.get('detailed_analysis', {})\n",
        "                for aspect, details in analysis.items():\n",
        "                    if details:  # Check if details is not empty\n",
        "                        print(f\"\\n{aspect.replace('_', ' ').capitalize()}:\")\n",
        "                        print(textwrap.fill(str(details), width=80))\n",
        "\n",
        "        # Display summary\n",
        "        summary = evaluation_results.get(\"overall_summary\", {})\n",
        "        if summary:\n",
        "            print(\"\\nOVERALL SUMMARY:\")\n",
        "            print(\"-\"*80)\n",
        "\n",
        "            if \"overall_performance\" in summary:\n",
        "                print(\"\\nOverall Performance:\")\n",
        "                print(textwrap.fill(str(summary[\"overall_performance\"]), width=80))\n",
        "\n",
        "            if \"optimal_permutation\" in summary:\n",
        "                print(\"\\nOptimal Configuration:\")\n",
        "                print(textwrap.fill(str(summary[\"optimal_permutation\"]), width=80))\n",
        "\n",
        "            if \"chunking_statistics\" in summary:\n",
        "                print(\"\\nChunking Statistics:\")\n",
        "                chunk_stats = summary[\"chunking_statistics\"]\n",
        "                for config, stats in chunk_stats.items():\n",
        "                    print(f\"\\n{config}:\")\n",
        "                    print(f\"  Number of Chunks: {stats['number_of_chunks']}\")\n",
        "                    print(f\"  Average Chunk Size: {stats['average_chunk_size']}\")\n",
        "                    print(f\"  Min Chunk Size: {stats['min_chunk_size']}\")\n",
        "                    print(f\"  Max Chunk Size: {stats['max_chunk_size']}\")\n",
        "\n",
        "            if \"performance_analysis\" in summary:\n",
        "                print(\"\\nPerformance Analysis:\")\n",
        "                analysis = summary[\"performance_analysis\"]\n",
        "                if isinstance(analysis, dict):\n",
        "                    for config, score in analysis.items():\n",
        "                        print(f\"{config}: {score:.2f}\")\n",
        "                else:\n",
        "                    print(textwrap.fill(str(analysis), width=80))"
      ],
      "metadata": {
        "id": "vnMb5d8cmKQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "koQ5ZObJC2ek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qdI5iaXYsun",
        "outputId": "f01af3b2-d799-43a9-a935-89b9673aa9ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing new RAG pipeline\n",
            "Starting experiment with configurations:\n",
            "Models: ['open-mistral-nemo']\n",
            "Thresholds: [85, 95]\n",
            "Chunk strategies: ['semantic', 'paragraph', 'header']\n",
            "Number of questions: 1\n",
            "\n",
            "=== CHUNK CREATION DEBUG ===\n",
            "Strategy: semantic\n",
            "Cache key: semantic_85\n",
            "Using semantic threshold: 85\n",
            "\n",
            "Testing model: open-mistral-nemo\n",
            "Processing question: What were cloud revenues in Q2 2024?\n",
            "\n",
            "=== COSINE SEARCH DEBUG ===\n",
            "Query: What were cloud revenues in Q2 2024?\n",
            "Strategy: semantic\n",
            "Threshold: 85\n",
            "Requested k: 5\n",
            "Cache key: semantic_85\n",
            "Creating embeddings for 137 chunks\n",
            "\n",
            "=== CHUNK CREATION DEBUG ===\n",
            "Strategy: semantic\n",
            "Cache key: semantic_95\n",
            "Using semantic threshold: 95\n",
            "\n",
            "Testing model: open-mistral-nemo\n",
            "Processing question: What were cloud revenues in Q2 2024?\n",
            "\n",
            "=== COSINE SEARCH DEBUG ===\n",
            "Query: What were cloud revenues in Q2 2024?\n",
            "Strategy: semantic\n",
            "Threshold: 95\n",
            "Requested k: 5\n",
            "Cache key: semantic_95\n",
            "Creating embeddings for 91 chunks\n",
            "\n",
            "=== CHUNK CREATION DEBUG ===\n",
            "Strategy: paragraph\n",
            "Cache key: paragraph_2048\n",
            "Using max chunk size: 2048 characters with 100 character overlap\n",
            "\n",
            "Testing model: open-mistral-nemo\n",
            "Processing question: What were cloud revenues in Q2 2024?\n",
            "\n",
            "=== COSINE SEARCH DEBUG ===\n",
            "Query: What were cloud revenues in Q2 2024?\n",
            "Strategy: paragraph\n",
            "Threshold: None\n",
            "Requested k: 5\n",
            "Cache key: paragraph_1024\n",
            "Warning: No chunks found for strategy paragraph\n",
            "\n",
            "=== CHUNK CREATION DEBUG ===\n",
            "Strategy: header\n",
            "Cache key: header_2048\n",
            "Using max chunk size: 2048 characters with 100 character overlap\n",
            "\n",
            "Testing model: open-mistral-nemo\n",
            "Processing question: What were cloud revenues in Q2 2024?\n",
            "\n",
            "=== COSINE SEARCH DEBUG ===\n",
            "Query: What were cloud revenues in Q2 2024?\n",
            "Strategy: header\n",
            "Threshold: None\n",
            "Requested k: 5\n",
            "Cache key: header_1024\n",
            "Warning: No chunks found for strategy header\n",
            "\n",
            "Initializing GPT-4o evaluation...\n",
            "\n",
            "=== DEBUG: evaluate_experiments ===\n",
            "Getting questions...\n",
            "Questions extracted: ['What were cloud revenues in Q2 2024?']\n",
            "\n",
            "Getting baseline answers...\n",
            "\n",
            "=== DEBUG: _get_baseline_answers ===\n",
            "Questions received: ['What were cloud revenues in Q2 2024?']\n",
            "Number of document parts: 52\n",
            "\n",
            "Full document length: 147898 characters\n",
            "\n",
            "Sampling from document:\n",
            "Start: UNITED STATES\n",
            "SECURITIES AND EXCHANGE COMMISSION\n",
            "Washington, D.C. 20549\n",
            "________________________________________________________________________________________\n",
            "FORM 10-Q \n",
            "____________________________ ...\n",
            "Middle: llocation of \n",
            "undistributed earnings  8,600  1,275  8,493  18,368  10,046  1,484  9,623  21,153 \n",
            "Net income $ 8,600 $ 1,275 $ 8,493 $ 18,368 $ 11,219 $ 1,657 $ 10,743 $ 23,619 \n",
            "Denominator\n",
            "Number of s ...\n",
            "End: ment Officer; Chief Financial Officer\n",
            "ALPHABET INC.\n",
            "July 23, 2024 By: /s/    AMIE THUENER O'TOOLE        \n",
            "Amie Thuener O'Toole\n",
            "Vice President, Corporate Controller and Principal Accounting \n",
            "Officer\n",
            "52 ...\n",
            "\n",
            "--- Getting Baseline Answers ---\n",
            "\n",
            "Raw GPT-4 Response:\n",
            "```json\n",
            "{\n",
            "  \"What were cloud revenues in Q2 2024?\": \"$10,347 million\"\n",
            "}\n",
            "```\n",
            "\n",
            "Parsed Baseline Answers:\n",
            "{'What were cloud revenues in Q2 2024?': '$10,347 million'}\n",
            "Baseline answers received: {'What were cloud revenues in Q2 2024?': '$10,347 million'}\n",
            "\n",
            "Evaluating response for: What were cloud revenues in Q2 2024?\n",
            "Using baseline answer: $10,347 million\n",
            "\n",
            "Evaluating response for: What were cloud revenues in Q2 2024?\n",
            "Using baseline answer: $10,347 million\n",
            "\n",
            "Evaluating response for: What were cloud revenues in Q2 2024?\n",
            "Using baseline answer: $10,347 million\n",
            "\n",
            "Evaluating response for: What were cloud revenues in Q2 2024?\n",
            "Using baseline answer: $10,347 million\n",
            "\n",
            "=== Starting Results Formatting ===\n",
            "\n",
            "================================================================================\n",
            "EVALUATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "METADATA:\n",
            "--------------------------------------------------------------------------------\n",
            "Timestamp:           2024-11-16T20:03:59.689047\n",
            "Model Used:          gpt-4o\n",
            "Combinations:        4\n",
            "Questions:           1\n",
            "Evaluation Status:   success\n",
            "\n",
            "DETAILED EVALUATIONS:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Question: What were cloud revenues in Q2 2024?\n",
            "Model: open-mistral-nemo\n",
            "Strategy: header\n",
            "\n",
            "Baseline Answer:\n",
            "$10,347 million\n",
            "\n",
            "Model Response:\n",
            "No relevant context found.\n",
            "\n",
            "Source Data:\n",
            "No source data available\n",
            "\n",
            "Scores:\n",
            "- Source accuracy: 0/100\n",
            "- Source attribution: 0/100\n",
            "- Conciseness: 100/100\n",
            "Composite Score: 33.33/100\n",
            "\n",
            "Detailed Analysis:\n",
            "\n",
            "Accuracy analysis:\n",
            "The response does not provide any information on cloud revenues for Q2 2024 and\n",
            "thus does not match the baseline answer of $10,347 million. Since no source data\n",
            "was provided, the accuracy score is zero.\n",
            "\n",
            "Attribution analysis:\n",
            "The response indicates no relevant context was found, which means there are no\n",
            "sources cited. In the absence of sources, it is impossible to evaluate the\n",
            "appropriateness of source usage, resulting in a score of zero.\n",
            "\n",
            "Conciseness analysis:\n",
            "The response is concise and direct, stating clearly that no relevant context was\n",
            "found. This clarity results in a high conciseness score.\n",
            "\n",
            "Question: What were cloud revenues in Q2 2024?\n",
            "Model: open-mistral-nemo\n",
            "Strategy: semantic\n",
            "Threshold: 95\n",
            "\n",
            "Baseline Answer:\n",
            "$10,347 million\n",
            "\n",
            "Model Response:\n",
            "Google Cloud revenues in Q2 2024 were $10.347 billion.\n",
            "\n",
            "Source Data:\n",
            "Revenues by type (in millions): Three Months Ended Six Months Ended June 30,\n",
            "June 30, 2023 2024 2023 2024 Google Cloud  8,031  10,347  15,485  19,921\n",
            "\n",
            "Scores:\n",
            "- Source accuracy: 100/100\n",
            "- Source attribution: 100/100\n",
            "- Conciseness: 100/100\n",
            "Composite Score: 100/100\n",
            "\n",
            "Detailed Analysis:\n",
            "\n",
            "Accuracy analysis:\n",
            "The model response accurately reflects the source data, converting the $10,347\n",
            "million figure from the source into $10.347 billion, which is correct and\n",
            "equivalent.\n",
            "\n",
            "Attribution analysis:\n",
            "The model response includes the correct interpretation of the provided source\n",
            "data, which confirms the figure stated in the answer.\n",
            "\n",
            "Conciseness analysis:\n",
            "The model response is concise and directly answers the question without adding\n",
            "unnecessary information.\n",
            "\n",
            "Question: What were cloud revenues in Q2 2024?\n",
            "Model: open-mistral-nemo\n",
            "Strategy: semantic\n",
            "Threshold: 85\n",
            "\n",
            "Baseline Answer:\n",
            "$10,347 million\n",
            "\n",
            "Model Response:\n",
            "Google Cloud revenues in Q2 2024 were $10,347 million.\n",
            "\n",
            "Source Data:\n",
            "Google Cloud revenues increased $2.3 billion and $4.4 billion from the three and\n",
            "six months ended June 30, 2023 to the three and six months ended June 30, 2024,\n",
            "respectively.\n",
            "\n",
            "Scores:\n",
            "- Source accuracy: 0/100\n",
            "- Source attribution: 50/100\n",
            "- Conciseness: 100/100\n",
            "Composite Score: 50/100\n",
            "\n",
            "Detailed Analysis:\n",
            "\n",
            "Accuracy analysis:\n",
            "The response states that Google Cloud revenues in Q2 2024 were $10,347 million.\n",
            "However, the source data only provides information on the increase in revenue,\n",
            "not the actual revenue amount. Thus, the response does not accurately reflect\n",
            "the source data.\n",
            "\n",
            "Attribution analysis:\n",
            "The response attempts to cite a source, but the source does not provide the\n",
            "specific revenue figure stated. Therefore, the source citation is partially\n",
            "correct in terms of effort but incorrect in terms of content.\n",
            "\n",
            "Conciseness analysis:\n",
            "The response is concise and directly answers the question without any additional\n",
            "or unnecessary information.\n",
            "\n",
            "Question: What were cloud revenues in Q2 2024?\n",
            "Model: open-mistral-nemo\n",
            "Strategy: paragraph\n",
            "\n",
            "Baseline Answer:\n",
            "$10,347 million\n",
            "\n",
            "Model Response:\n",
            "No relevant context found.\n",
            "\n",
            "Source Data:\n",
            "No source data available\n",
            "\n",
            "Scores:\n",
            "- Source accuracy: 0/100\n",
            "- Source attribution: 0/100\n",
            "- Conciseness: 100/100\n",
            "Composite Score: 33.33/100\n",
            "\n",
            "Detailed Analysis:\n",
            "\n",
            "Accuracy analysis:\n",
            "The response 'No relevant context found.' does not provide an answer to the\n",
            "question and does not match the baseline answer of '$10,347 million'. Since the\n",
            "source data is empty, there is no reference point to evaluate against, resulting\n",
            "in a score of 0 for source accuracy.\n",
            "\n",
            "Attribution analysis:\n",
            "There are no sources cited as evidence for the model’s response, leading to a\n",
            "score of 0 for source attribution. Without relevant source data, the model\n",
            "cannot attribute any information.\n",
            "\n",
            "Conciseness analysis:\n",
            "The response is concise, as it directly states that no relevant context was\n",
            "found, leading to a score of 100 for conciseness. However, it does not address\n",
            "the question with an answer.\n",
            "\n",
            "OVERALL SUMMARY:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Overall Performance:\n",
            "Average composite score across all evaluations: 54.16/100\n",
            "\n",
            "Optimal Configuration:\n",
            "Best performance: open-mistral-nemo with semantic chunking (threshold: 95)\n",
            "(score: 100.00/100)\n",
            "\n",
            "Chunking Statistics:\n",
            "\n",
            "open-mistral-nemo with semantic chunking (threshold: 85):\n",
            "  Number of Chunks: 137\n",
            "  Average Chunk Size: 1078.0\n",
            "  Min Chunk Size: 43\n",
            "  Max Chunk Size: 3859\n",
            "\n",
            "open-mistral-nemo with semantic chunking (threshold: 95):\n",
            "  Number of Chunks: 91\n",
            "  Average Chunk Size: 1623.07\n",
            "  Min Chunk Size: 52\n",
            "  Max Chunk Size: 3956\n",
            "\n",
            "open-mistral-nemo with paragraph chunking:\n",
            "  Number of Chunks: 98\n",
            "  Average Chunk Size: 1520.2\n",
            "  Min Chunk Size: 69\n",
            "  Max Chunk Size: 2047\n",
            "\n",
            "open-mistral-nemo with header chunking:\n",
            "  Number of Chunks: 102\n",
            "  Average Chunk Size: 1437.7\n",
            "  Min Chunk Size: 57\n",
            "  Max Chunk Size: 2048\n",
            "\n",
            "Performance Analysis:\n",
            "open-mistral-nemo with semantic chunking (threshold: 85): 50.00\n",
            "open-mistral-nemo with semantic chunking (threshold: 95): 100.00\n",
            "open-mistral-nemo with paragraph chunking: 33.33\n",
            "open-mistral-nemo with header chunking: 33.33\n",
            "\n",
            "Experiment complete!\n",
            "Results saved to:\n",
            "  Experiment results: /content/drive/My Drive/AI/Model_Analysis/experiment_results_20241116-200359.json\n",
            "  Evaluation results: /content/drive/My Drive/AI/Model_Analysis/evaluation_results_20241116-200359.json\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Initialize configurations\n",
        "    model_config = ModelConfig(\n",
        "        models=MODEL_CONFIGS[\"models\"],\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    # Initialize experiment runner\n",
        "    experiment_runner = ExperimentRunner(\n",
        "        model_config=model_config,\n",
        "        thresholds=CHUNKING_CONFIGS[\"thresholds\"],\n",
        "        questions=QUESTION_CONFIGS[\"questions\"],\n",
        "        chunk_strategies=CHUNKING_CONFIGS[\"strategies\"]\n",
        "    )\n",
        "\n",
        "    print(\"Starting experiment with configurations:\")\n",
        "    print(f\"Models: {[model['name'] for model in model_config.models]}\")\n",
        "    print(f\"Thresholds: {CHUNKING_CONFIGS['thresholds']}\")\n",
        "    print(f\"Chunk strategies: {CHUNKING_CONFIGS['strategies']}\")\n",
        "    print(f\"Number of questions: {len(QUESTION_CONFIGS['questions'])}\")\n",
        "\n",
        "    # Run experiments\n",
        "    experiment_results = experiment_runner.run_experiments()\n",
        "\n",
        "   # Initialize evaluator with full documents list\n",
        "    print(\"\\nInitializing GPT-4o evaluation...\")\n",
        "    evaluator = ExperimentEvaluator(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "    # Run evaluation with full documents list\n",
        "    evaluation_results = evaluator.evaluate_experiments(\n",
        "        experiment_results=experiment_results,\n",
        "        source_docs=documents  # Pass the full documents list\n",
        "    )\n",
        "\n",
        "    # Initialize results manager\n",
        "    results_manager = ResultsManager(save_directory=FILE_CONFIGS['save_directory'])\n",
        "\n",
        "    # Format results\n",
        "    formatted_experiment, formatted_evaluation = results_manager.format_results(\n",
        "        experiment_results=experiment_results,\n",
        "        evaluation_results=evaluation_results\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    experiment_file, evaluation_file = results_manager.save_results(\n",
        "        formatted_experiment=formatted_experiment,\n",
        "        formatted_evaluation=formatted_evaluation\n",
        "    )\n",
        "\n",
        "    # Display results\n",
        "    results_manager.display_results(evaluation_results=formatted_evaluation)\n",
        "\n",
        "    print(\"\\nExperiment complete!\")\n",
        "    print(f\"Results saved to:\")\n",
        "    print(f\"  Experiment results: {experiment_file}\")\n",
        "    print(f\"  Evaluation results: {evaluation_file}\")\n",
        "\n",
        "    return formatted_experiment, formatted_evaluation\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results, evaluation = main()"
      ]
    }
  ]
}