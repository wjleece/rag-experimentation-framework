{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNfNdFiDa4HHubBG+aecXcP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjleece/rag-experimentation-framework/blob/main/RAG_Experimentation_Framework_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you use this code, please cite:\n",
        "\n",
        "{\n",
        "  title = {RAG Experimentation Framework},\n",
        "\n",
        "  author = {Bill Leece},\n",
        "\n",
        "  year = {2024}\n",
        "}"
      ],
      "metadata": {
        "id": "wZ0kV_UtQn5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "_lHNBLR-92Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers --quiet\n",
        "#!pip install -U optimum --quiet\n",
        "!pip install -U accelerate  --quiet\n",
        "#!pip install -U bitsandbytes  --quiet\n",
        "!pip install -U torch --quiet\n",
        "!pip install -U sentencepiece --quiet\n",
        "!pip install -U llama-index --quiet\n",
        "!pip install -U llama-index-llms-mistralai --quiet\n",
        "!pip install -U llama-index-embeddings-mistralai --quiet\n",
        "!pip install -U llama-index-llms-langchain --quiet\n",
        "!pip install -U langchain --quiet\n",
        "!pip install -U langchain-community --quiet\n",
        "!pip install -U langchain-mistralai --quiet\n",
        "!pip install -U langchain_huggingface --quiet\n",
        "!pip install -U faiss-gpu --quiet"
      ],
      "metadata": {
        "id": "4g_Vs7wgZW-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d79740-7d33-4948-fa4f-2a46144b1085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.0/189.0 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.6/254.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "import transformers\n",
        "import torch\n",
        "import gc\n",
        "from google.colab import drive, userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "from llama_index.core import SimpleDirectoryReader, Settings\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "import time\n",
        "from typing import List, Dict, Tuple\n",
        "from contextlib import contextmanager\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "Ao7eaSfq-TKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get('MISTRAL_API_KEY')\n",
        "api_key = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "YvGHY024-OXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' #Use GPUs when possible"
      ],
      "metadata": {
        "id": "mxAHV7T_-Xlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment Configurations"
      ],
      "metadata": {
        "id": "jkqEV8M_HUKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup configurations\n",
        "MODEL_CONFIGS = {\n",
        "    \"models\": [\n",
        "    #    {\n",
        "    #        \"name\": \"open-mixtral-8x7b\",\n",
        "    #        \"type\": \"mistral_api\",\n",
        "    #        \"tokenizer\": None,  # Not needed for API models\n",
        "    #    },\n",
        "         {\n",
        "            \"name\": \"open-mistral-nemo\",\n",
        "            \"type\": \"mistral_api\",\n",
        "            \"tokenizer\": None,  # Not needed for API models\n",
        "         },\n",
        "   #     {\n",
        "   #         \"name\": \"ministral-8b-latest\",\n",
        "   #         \"type\": \"mistral_api\",\n",
        "   #         \"tokenizer\": None,  # Not needed for API models\n",
        "   #     },\n",
        "   #   {\n",
        "   #         \"name\": \"wjleece/quantized-mistral-7b\",\n",
        "   #         \"type\": \"huggingface_quantized\",\n",
        "   #         \"tokenizer\": \"mistralai/Mixtral-8x7B-v0.1\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "   #          \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "   #             \"load_in_4bit\": True,\n",
        "   #             \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "   #             \"bnb_4bit_quant_type\": \"nf4\",\n",
        "   #             \"bnb_4bit_use_double_quant\": False\n",
        "   #         }\n",
        "   #     },\n",
        "   #   {\n",
        "   #           \"name\": \"wjleece/quantized-mistral-nemo-12b\",\n",
        "   #           \"type\": \"huggingface_quantized\",\n",
        "   #           \"tokenizer\": \"mistralai/Mistral-Nemo-Instruct-2407\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "   #           \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "   #               \"load_in_4bit\": True,\n",
        "   #               \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "   #               \"bnb_4bit_quant_type\": \"nf4\",\n",
        "   #               \"bnb_4bit_use_double_quant\": False\n",
        "   #          }\n",
        "   #       },\n",
        "     #  {\n",
        "     #         \"name\": \"wjleece/quantized-mistral-8b\",\n",
        "     #         \"type\": \"huggingface_quantized\",\n",
        "     #         \"tokenizer\": \"mistralai/Ministral-8B-Instruct-2410\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "     #         \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "     #             \"load_in_4bit\": True,\n",
        "     #             \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "     #             \"bnb_4bit_quant_type\": \"nf4\",\n",
        "     #             \"bnb_4bit_use_double_quant\": False\n",
        "     #         }\n",
        "     #     }\n",
        "       ]\n",
        "}\n",
        "\n",
        "\n",
        "CHUNKING_CONFIGS = {\n",
        "    \"strategies\": [\"semantic\", \"paragraph\", \"header\"], #results will be saved in this order, with thesholds applicable for semantic only (if it is included)\n",
        "    \"thresholds\": [85, 95], #semantic threshold, only applicable for semantic chunking\n",
        "    \"max_chunk_size\": 2048, #only used in paragraph and header chunking\n",
        "    \"chunk_overlap\": 100, #only used in paragraph and header chunking\n",
        "    \"min_chunk_size\": 35 #if a chunk is only 35 characters - about 5 words - just ignore it\n",
        "}\n",
        "\n",
        "QUESTION_CONFIGS = {\n",
        "    \"questions\": [\n",
        "        \"What were cloud revenues in Q2 2024?\",\n",
        "       # \"What were the main drivers of revenue growth in Q2?\",\n",
        "       # \"How much did YouTube ad revenues grow in Q2 in APAC?\",\n",
        "       # \"Can you summarize recent key antitrust matters?\",\n",
        "       # \"Compare the revenue growth across all geographic regions and explain the main factors for each region.\",\n",
        "       # \"Summarize all mentioned risk factors related to international operations.\",\n",
        "       # \"What were the major changes in operating expenses across all categories and their stated reasons?\"\n",
        "    ] #These quetsions should relate to the RAG document --> these are your 'business use cases'\n",
        "}\n",
        "\n",
        "FILE_CONFIGS = {\n",
        "    \"save_directory\": '/content/drive/My Drive/AI/Model_Analysis'\n",
        "}"
      ],
      "metadata": {
        "id": "YDjgk_JhHWkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load RAG Document"
      ],
      "metadata": {
        "id": "e_wxgOGc95sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "documents = SimpleDirectoryReader(input_files=[\"/content/drive/My Drive/AI/Datasets/Google-10-q/goog-10-q-q2-2024.pdf\"]).load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS4Lemk09v9Y",
        "outputId": "aec15a3b-4eaa-4b84-9ebb-339245fddda6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RAG Pipeline Class"
      ],
      "metadata": {
        "id": "MgLxma5M-bZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global singleton instance\n",
        "_GLOBAL_RAG_PIPELINE = None\n",
        "\n",
        "class RAGPipeline:\n",
        "    def __init__(self):\n",
        "        self.chunk_cache = {}\n",
        "        self.embedding_cache = {}\n",
        "        self.embedding_model = None\n",
        "\n",
        "    @classmethod\n",
        "    def get_instance(cls):\n",
        "        \"\"\"Get or create singleton instance\"\"\"\n",
        "        global _GLOBAL_RAG_PIPELINE\n",
        "        if _GLOBAL_RAG_PIPELINE is None:\n",
        "            _GLOBAL_RAG_PIPELINE = cls()\n",
        "        return _GLOBAL_RAG_PIPELINE\n",
        "\n",
        "\n",
        "    def initialize_embedding_model(self):\n",
        "        \"\"\"Initialize the embedding model if not already initialized\"\"\"\n",
        "        if self.embedding_model is None:\n",
        "            mistral_api_key = userdata.get('MISTRAL_API_KEY')\n",
        "            self.embedding_model = MistralAIEmbedding(\n",
        "                model_name=\"mistral-embed\",\n",
        "                api_key=mistral_api_key\n",
        "            )\n",
        "        return self.embedding_model\n",
        "\n",
        "    def convert_to_markdown_headers(self, text):\n",
        "        \"\"\"Convert document section titles to markdown headers\"\"\"\n",
        "        import re\n",
        "\n",
        "        patterns = [\n",
        "            (r'^(?:ITEM|Section)\\s+\\d+[.:]\\s*(.+)$', '# '),\n",
        "            (r'^\\d+\\.\\d+\\s+(.+)$', '## '),\n",
        "            (r'^\\([a-z]\\)\\s+(.+)$', '### ')\n",
        "        ]\n",
        "\n",
        "        lines = text.split('\\n')\n",
        "        markdown_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            converted = False\n",
        "\n",
        "            for pattern, header_mark in patterns:\n",
        "                if re.match(pattern, line, re.IGNORECASE):\n",
        "                    markdown_lines.append(f\"{header_mark}{line}\")\n",
        "                    converted = True\n",
        "                    break\n",
        "\n",
        "            if not converted:\n",
        "                markdown_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(markdown_lines)\n",
        "\n",
        "\n",
        "    def create_chunks(self, documents: List, threshold: int, chunk_strategy: str = \"semantic\") -> Dict:\n",
        "        \"\"\"Create or retrieve chunks based on specified strategy\"\"\"\n",
        "\n",
        "        MAX_CHUNK_SIZE = CHUNKING_CONFIGS['max_chunk_size']\n",
        "        CHUNK_OVERLAP = CHUNKING_CONFIGS['chunk_overlap']\n",
        "        MIN_CHUNK_SIZE = CHUNKING_CONFIGS['min_chunk_size']\n",
        "\n",
        "        if chunk_strategy == \"semantic\":\n",
        "            cache_key = f\"{chunk_strategy}_{threshold}\"\n",
        "        else:\n",
        "            cache_key = f\"{chunk_strategy}_{MAX_CHUNK_SIZE}\"\n",
        "\n",
        "        if cache_key not in self.chunk_cache:\n",
        "            print(\"\\n=== CHUNK CREATION DEBUG ===\")\n",
        "            print(f\"Strategy: {chunk_strategy}\")\n",
        "            print(f\"Cache key: {cache_key}\")\n",
        "            if chunk_strategy == \"semantic\":\n",
        "                print(f\"Using semantic threshold: {threshold}\")\n",
        "            else:\n",
        "                print(f\"Using max chunk size: {MAX_CHUNK_SIZE} characters with {CHUNK_OVERLAP} character overlap\")\n",
        "\n",
        "            if len(self.chunk_cache) > 2:\n",
        "                oldest_key = min(self.chunk_cache.keys())\n",
        "                if oldest_key != cache_key:\n",
        "                    del self.chunk_cache[oldest_key]\n",
        "                    if oldest_key in self.embedding_cache:\n",
        "                        del self.embedding_cache[oldest_key]\n",
        "                    gc.collect()\n",
        "\n",
        "            if chunk_strategy == \"semantic\":\n",
        "                if self.embedding_model is None:\n",
        "                    self.initialize_embedding_model()\n",
        "\n",
        "                splitter = SemanticSplitterNodeParser(\n",
        "                    buffer_size=1,\n",
        "                    breakpoint_percentile_threshold=threshold,\n",
        "                    embed_model=self.embedding_model\n",
        "                )\n",
        "                nodes = splitter.get_nodes_from_documents(documents)\n",
        "                texts = [node.text for node in nodes]\n",
        "\n",
        "            elif chunk_strategy == \"paragraph\":\n",
        "                text_splitter = RecursiveCharacterTextSplitter(\n",
        "                    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "                    chunk_size=MAX_CHUNK_SIZE,\n",
        "                    chunk_overlap=CHUNK_OVERLAP,\n",
        "                    length_function=len\n",
        "                )\n",
        "                texts = []\n",
        "                for doc in documents:\n",
        "                    chunks = text_splitter.split_text(doc.text)\n",
        "                    texts.extend(chunks)\n",
        "\n",
        "            elif chunk_strategy == \"header\":\n",
        "                headers_to_split_on = [\n",
        "                    (\"#\", \"Header 1\"),\n",
        "                    (\"##\", \"Header 2\"),\n",
        "                    (\"###\", \"Header 3\"),\n",
        "                ]\n",
        "\n",
        "                header_splitter = MarkdownHeaderTextSplitter(\n",
        "                    headers_to_split_on=headers_to_split_on\n",
        "                )\n",
        "\n",
        "                text_splitter = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size=MAX_CHUNK_SIZE,\n",
        "                    chunk_overlap=CHUNK_OVERLAP,\n",
        "                    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "                )\n",
        "\n",
        "                texts = []\n",
        "                for doc in documents:\n",
        "                    md_text = self.convert_to_markdown_headers(doc.text)\n",
        "                    header_splits = header_splitter.split_text(md_text)\n",
        "\n",
        "                    for split in header_splits:\n",
        "                        if len(split.page_content) > MAX_CHUNK_SIZE:\n",
        "                            chunks = text_splitter.split_text(split.page_content)\n",
        "                            texts.extend(chunks)\n",
        "                        else:\n",
        "                            texts.append(split.page_content)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown chunk strategy: {chunk_strategy}\")\n",
        "\n",
        "            # Filter out chunks that are too small\n",
        "            texts = [t for t in texts if len(t.strip()) >= MIN_CHUNK_SIZE]\n",
        "\n",
        "            if texts:\n",
        "                chunk_lengths = [len(t) for t in texts]\n",
        "                chunk_stats = {\n",
        "                    'num_chunks': len(texts),\n",
        "                    'avg_chunk_size': sum(chunk_lengths)/len(texts),\n",
        "                    'min_chunk_size': min(chunk_lengths),\n",
        "                    'max_chunk_size': max(chunk_lengths)\n",
        "                }\n",
        "\n",
        "                self.chunk_cache[cache_key] = {\n",
        "                    'texts': texts,\n",
        "                    'strategy': chunk_strategy,\n",
        "                    'chunk_stats': chunk_stats\n",
        "                }\n",
        "\n",
        "        return self.chunk_cache[cache_key]\n",
        "\n",
        "    def run_cosine_search(self, query: str, threshold: int, chunk_strategy: str = \"semantic\", k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Run cosine similarity search with memory optimization and debugging\"\"\"\n",
        "        print(\"\\n=== COSINE SEARCH DEBUG ===\")\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Strategy: {chunk_strategy}\")\n",
        "        print(f\"Threshold: {threshold}\")\n",
        "        print(f\"Requested k: {k}\")\n",
        "\n",
        "        if self.embedding_model is None:\n",
        "            self.initialize_embedding_model()\n",
        "\n",
        "        FIXED_CHUNK_SIZE = 1024\n",
        "\n",
        "        if chunk_strategy == \"semantic\":\n",
        "            cache_key = f\"{chunk_strategy}_{threshold}\"\n",
        "        else:\n",
        "            cache_key = f\"{chunk_strategy}_{FIXED_CHUNK_SIZE}\"\n",
        "\n",
        "        print(f\"Cache key: {cache_key}\")\n",
        "\n",
        "        if cache_key not in self.embedding_cache:\n",
        "            try:\n",
        "                texts = self.chunk_cache[cache_key]['texts']\n",
        "                print(f\"Creating embeddings for {len(texts)} chunks\")\n",
        "            except KeyError:\n",
        "                print(f\"Warning: No chunks found for strategy {chunk_strategy}\")\n",
        "                return []\n",
        "\n",
        "            batch_size = 32\n",
        "            embeddings = []\n",
        "\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch_texts = texts[i:i + batch_size]\n",
        "                batch_embeddings = [self.embedding_model.get_text_embedding(text)\n",
        "                                  for text in batch_texts]\n",
        "                embeddings.extend(batch_embeddings)\n",
        "\n",
        "                if i % (batch_size * 4) == 0:\n",
        "                    gc.collect()\n",
        "\n",
        "            embeddings_array = np.array(embeddings).astype('float32')\n",
        "            normalized_embeddings = embeddings_array / np.linalg.norm(embeddings_array, axis=1)[:, np.newaxis]\n",
        "\n",
        "            dimension = embeddings_array.shape[1]\n",
        "            cosine_index = faiss.IndexFlatIP(dimension)\n",
        "            cosine_index.add(normalized_embeddings)\n",
        "\n",
        "            self.embedding_cache[cache_key] = {\n",
        "                'embeddings': embeddings_array,\n",
        "                'cosine_index': cosine_index\n",
        "            }\n",
        "\n",
        "        query_vector = self.embedding_model.get_text_embedding(query)\n",
        "        query_vector = np.array([query_vector]).astype('float32')\n",
        "        query_normalized = query_vector / np.linalg.norm(query_vector)\n",
        "\n",
        "        distances, indices = self.embedding_cache[cache_key]['cosine_index'].search(\n",
        "            query_normalized.reshape(1, -1).astype('float32'), k\n",
        "        )\n",
        "\n",
        "        return [\n",
        "            {\n",
        "                'text': self.chunk_cache[cache_key]['texts'][idx],\n",
        "                'distance': float(score),\n",
        "                'strategy': chunk_strategy\n",
        "            }\n",
        "            for score, idx in zip(distances[0], indices[0])\n",
        "        ]\n",
        "\n",
        "    def generate_response(self, query: str, context_rag: list, model: Dict) -> dict:\n",
        "        \"\"\"Generate response using provided context\"\"\"\n",
        "        try:\n",
        "            context_texts = [doc['text'] for doc in context_rag]\n",
        "            if not context_texts:\n",
        "                return {\"response_text\": \"No relevant context found.\", \"sources\": [], \"strategy\": context_rag[0]['strategy'] if context_rag else None}\n",
        "\n",
        "            context = \"\\n\\n\".join(context_texts)\n",
        "\n",
        "            prompt = PromptTemplate(template=\"\"\"\n",
        "            Instructions:\n",
        "\n",
        "            You are a helpful assistant who answers questions from context that has been provided to you.\n",
        "            Given the context information, provide a direct and concise answer to the question: {query}\n",
        "\n",
        "            Focus only on information present in the context. If you don't know the answer, say \"I don't know.\"\n",
        "            You must format your response as a JSON string object, starting with the word \"LLM_Response:\"\n",
        "\n",
        "            Your answer to {query} will be a JSON string object that starts with \"LLM_Response:\" as shown below:\n",
        "\n",
        "            LLM_Response:\n",
        "            {{\n",
        "                \"response_text\": \"Your detailed answer here\",\n",
        "                \"sources\": [\n",
        "                    \"Copy and paste here the exact text segments from the context that you used to generate your answer. Include all relevant segments, verbatim.\"\n",
        "                ]\n",
        "            }}\n",
        "\n",
        "            Important: In your response, the \"sources\" field must contain the exact text passages from the provided context that you used to formulate your answer. Copy these passages word-for-word.\n",
        "\n",
        "            Do not include a hypothetical example in your answer, only include your final answer after \"LLM_Response:\"\n",
        "\n",
        "            The context information that you will use for your answer is below:\n",
        "\n",
        "            ---------------\n",
        "            {context}\n",
        "            ---------------\n",
        "            \"\"\")\n",
        "\n",
        "            model_type = model['type']\n",
        "            llm = model['llm']\n",
        "\n",
        "            chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "            response = chain.invoke({\n",
        "                \"query\": query,\n",
        "                \"context\": context\n",
        "               })\n",
        "\n",
        "            response_text = response.split(\"LLM_Response:\")[-1].strip()\n",
        "\n",
        "            try:\n",
        "                if '{' in response_text and '}' in response_text:\n",
        "                    json_str = response_text[response_text.find('{'):response_text.rfind('}')+1]\n",
        "                    parsed_response = json.loads(json_str)\n",
        "                    return {\n",
        "                        \"response_text\": parsed_response.get(\"response_text\", response_text),\n",
        "                        \"sources\": parsed_response.get(\"sources\", []),\n",
        "                        \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                    }\n",
        "                else:\n",
        "                    return {\n",
        "                        \"response_text\": response_text,\n",
        "                        \"sources\": [],\n",
        "                        \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                    }\n",
        "            except json.JSONDecodeError:\n",
        "                return {\n",
        "                    \"response_text\": response_text,\n",
        "                    \"sources\": [],\n",
        "                    \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {str(e)}\")\n",
        "            return {\"response_text\": \"An error occurred while generating the response.\", \"sources\": []}"
      ],
      "metadata": {
        "id": "YY5rnivk-bAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ModelConfig Class"
      ],
      "metadata": {
        "id": "ljqi1Qg8j9F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelConfig:\n",
        "    \"\"\"Handles model configuration and management\"\"\"\n",
        "    def __init__(self,\n",
        "                 models: List[Dict],\n",
        "                 temperature: float = 0.3):\n",
        "        self.models = models\n",
        "        self.temperature = temperature\n",
        "        self.current_model = None\n",
        "        self.current_model_name = None\n",
        "\n",
        "\n",
        "    @contextmanager\n",
        "    def load_model(self, model_config: Dict):\n",
        "        \"\"\"Context manager for lazy loading and proper cleanup of models\"\"\"\n",
        "        try:\n",
        "            model_name = model_config[\"name\"]\n",
        "            model_type = model_config[\"type\"]\n",
        "\n",
        "            # Clear any existing model\n",
        "            self.cleanup_current_model()\n",
        "\n",
        "            if model_type == \"mistral_api\":\n",
        "                mistral_api_key = userdata.get('MISTRAL_API_KEY')\n",
        "                self.current_model = {\n",
        "                    'llm': ChatMistralAI(\n",
        "                        model=model_name,\n",
        "                        temperature=self.temperature,\n",
        "                        api_key=mistral_api_key\n",
        "                    ),\n",
        "                    'type': 'mistral_api'\n",
        "                }\n",
        "            else:  # huggingface_quantized\n",
        "                print(f\"Loading quantized model: {model_name}\")\n",
        "\n",
        "                # Empty CUDA cache before loading new model\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                tokenizer = AutoTokenizer.from_pretrained(\n",
        "                    pretrained_model_name_or_path=model_config[\"tokenizer\"],\n",
        "                    trust_remote_code=True,\n",
        "                    use_fast=True,\n",
        "                    padding_side=\"left\"\n",
        "                )\n",
        "\n",
        "                model = AutoModelForCausalLM.from_pretrained(\n",
        "                    pretrained_model_name_or_path=model_name,\n",
        "                    device_map=\"auto\",\n",
        "                    trust_remote_code=True,\n",
        "                    torch_dtype=torch.float16,\n",
        "                    use_cache=True,\n",
        "                    low_cpu_mem_usage=True,\n",
        "                )\n",
        "\n",
        "                pipe = pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_new_tokens=512,\n",
        "                    temperature=self.temperature,\n",
        "                    top_p=0.95,\n",
        "                    top_k=50,\n",
        "                    do_sample=True,\n",
        "                    device_map=\"auto\"\n",
        "                )\n",
        "\n",
        "                self.current_model = {\n",
        "                    'llm': HuggingFacePipeline(pipeline=pipe),\n",
        "                    'type': 'huggingface_quantized',\n",
        "                    'model': model,  # Keep reference for cleanup\n",
        "                    'pipe': pipe     # Keep reference for cleanup\n",
        "                }\n",
        "\n",
        "            self.current_model_name = model_name\n",
        "            yield self.current_model\n",
        "\n",
        "        finally:\n",
        "            # Cleanup will happen in cleanup_current_model()\n",
        "            pass\n",
        "\n",
        "    def cleanup_current_model(self):\n",
        "        \"\"\"Clean up the current model and free memory\"\"\"\n",
        "        if self.current_model is not None:\n",
        "            if self.current_model['type'] == 'huggingface_quantized':\n",
        "                # Delete model components explicitly\n",
        "                del self.current_model['llm']\n",
        "                del self.current_model['model']\n",
        "                del self.current_model['pipe']\n",
        "\n",
        "                # Clear CUDA cache\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                # Run garbage collection\n",
        "                gc.collect()\n",
        "\n",
        "            self.current_model = None\n",
        "            self.current_model_name = None"
      ],
      "metadata": {
        "id": "tCzG7OE0IiDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ExperimentRunner Class"
      ],
      "metadata": {
        "id": "05gTul4pIW6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperimentRunner:\n",
        "    \"\"\"Handles experiment execution\"\"\"\n",
        "    def __init__(self,\n",
        "                 model_config: ModelConfig,\n",
        "                 thresholds: List[int],\n",
        "                 questions: List[str],\n",
        "                 chunk_strategies: List[str],\n",
        "                 rag_pipeline: RAGPipeline = None):\n",
        "        self.model_config = model_config\n",
        "        self.thresholds = thresholds\n",
        "        self.questions = questions\n",
        "        self.chunk_strategies = chunk_strategies\n",
        "\n",
        "        # Use existing RAG pipeline or create new one\n",
        "        global _GLOBAL_RAG_PIPELINE\n",
        "        if rag_pipeline:\n",
        "            self.rag_pipeline = rag_pipeline\n",
        "        elif _GLOBAL_RAG_PIPELINE:\n",
        "            self.rag_pipeline = _GLOBAL_RAG_PIPELINE\n",
        "        else:\n",
        "            print(\"Initializing new RAG pipeline\")\n",
        "            _GLOBAL_RAG_PIPELINE = RAGPipeline()\n",
        "            self.rag_pipeline = _GLOBAL_RAG_PIPELINE\n",
        "\n",
        "    def run_experiments(self) -> Dict:\n",
        "        results = {\n",
        "            \"metadata\": {\n",
        "                \"timestamp\": time.strftime(\"%Y%m%d-%H%M%S\"),\n",
        "                \"models_tested\": [model[\"name\"] for model in self.model_config.models],\n",
        "                \"thresholds_tested\": self.thresholds,\n",
        "                \"chunk_strategies_tested\": self.chunk_strategies,\n",
        "                \"temperature\": self.model_config.temperature\n",
        "            },\n",
        "            \"results\": []\n",
        "        }\n",
        "\n",
        "        for strategy in self.chunk_strategies:\n",
        "            if strategy == \"semantic\":\n",
        "                thresholds_to_test = self.thresholds\n",
        "            else:\n",
        "                thresholds_to_test = [None]\n",
        "\n",
        "            for threshold in thresholds_to_test:\n",
        "                actual_threshold = threshold if strategy == \"semantic\" else 0\n",
        "\n",
        "                # Get chunks and their stats\n",
        "                chunks_data = self.rag_pipeline.create_chunks(\n",
        "                    documents,\n",
        "                    threshold=actual_threshold,\n",
        "                    chunk_strategy=strategy\n",
        "                )\n",
        "\n",
        "                # Store chunk stats in a format that will persist through the pipeline\n",
        "                chunk_stats = {\n",
        "                    \"strategy\": strategy,\n",
        "                    \"threshold\": threshold,\n",
        "                    \"stats\": chunks_data[\"chunk_stats\"]\n",
        "                }\n",
        "\n",
        "                for model_config in self.model_config.models:\n",
        "                    model_name = model_config[\"name\"]\n",
        "                    print(f\"\\nTesting model: {model_name}\")\n",
        "\n",
        "                    with self.model_config.load_model(model_config) as model:\n",
        "                        for question in self.questions:\n",
        "                            print(f\"Processing question: {question}\")\n",
        "\n",
        "                            context = self.rag_pipeline.run_cosine_search(\n",
        "                                query=question,\n",
        "                                threshold=threshold,\n",
        "                                chunk_strategy=strategy\n",
        "                            )\n",
        "\n",
        "                            answer = self.rag_pipeline.generate_response(\n",
        "                                query=question,\n",
        "                                context_rag=context,\n",
        "                                model=model\n",
        "                            )\n",
        "\n",
        "                            # Include chunk stats in results\n",
        "                            results[\"results\"].append({\n",
        "                                \"model\": model_name,\n",
        "                                \"threshold\": threshold if strategy == \"semantic\" else None,\n",
        "                                \"chunk_strategy\": strategy,\n",
        "                                \"question\": question,\n",
        "                                \"response\": answer,\n",
        "                                \"chunk_stats\": chunk_stats[\"stats\"]  # Include the stats here\n",
        "                            })\n",
        "\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "8hFyd9G1kC8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluator Class"
      ],
      "metadata": {
        "id": "EpjD-Qz54mfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "import tiktoken\n",
        "import textwrap\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "class ExperimentEvaluator:\n",
        "    \"\"\"Handles pure evaluation logic\"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.client = openai.OpenAI(api_key=api_key)\n",
        "        self.encoder = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "\n",
        "    def _get_baseline_answers(self, questions: List[str], source_doc: str) -> Dict[str, str]:\n",
        "        \"\"\"Get GPT-4o's own answers to the questions as baseline\"\"\"\n",
        "        baseline_prompt = f\"\"\"Source Document:\n",
        "        {source_doc}\n",
        "\n",
        "        Using only the information from the source document above, answer these questions.\n",
        "        Format your response as a valid JSON object with questions as keys and answers as values.\n",
        "        Keep answers concise and factual.\n",
        "\n",
        "        Questions to answer:\n",
        "        {json.dumps(questions, indent=2)}\"\"\"\n",
        "\n",
        "        try:\n",
        "            print(\"\\n--- Getting Baseline Answers ---\")\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides JSON-formatted answers based on source documents.\"},\n",
        "                    {\"role\": \"user\", \"content\": baseline_prompt}\n",
        "                ],\n",
        "                temperature=0.1\n",
        "            )\n",
        "\n",
        "            content = response.choices[0].message.content\n",
        "            if '{' in content and '}' in content:\n",
        "                json_str = content[content.find('{'):content.rfind('}')+1]\n",
        "                return json.loads(json_str)\n",
        "            return {\"error\": \"No JSON structure found\", \"questions\": questions}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error getting baseline answers: {str(e)}\")\n",
        "            return {\"error\": str(e), \"questions\": questions}\n",
        "\n",
        "    def evaluate_experiments(self, experiment_results: Dict, source_doc: str) -> Dict:\n",
        "        \"\"\"Core evaluation logic\"\"\"\n",
        "        try:\n",
        "            print(\"\\n=== Starting Evaluation Process ===\")\n",
        "            questions = list(set(result[\"question\"] for result in experiment_results[\"results\"]))\n",
        "            model_strategy_combinations = set(\n",
        "                (result[\"model\"],\n",
        "                result[\"chunk_strategy\"],\n",
        "                result[\"threshold\"] if result[\"chunk_strategy\"] == \"semantic\" else None)\n",
        "                for result in experiment_results[\"results\"]\n",
        "            )\n",
        "\n",
        "            baseline_answers = self._get_baseline_answers(questions, source_doc)\n",
        "            all_evaluations = []\n",
        "\n",
        "            for model, strategy, threshold in model_strategy_combinations:\n",
        "                relevant_results = [r for r in experiment_results[\"results\"]\n",
        "                                  if r[\"model\"] == model and\n",
        "                                     r[\"chunk_strategy\"] == strategy and\n",
        "                                     (r[\"threshold\"] == threshold if strategy == \"semantic\" else True)]\n",
        "\n",
        "                for result in relevant_results:\n",
        "                    evaluation = self._evaluate_single_response(\n",
        "                        result, baseline_answers.get(result[\"question\"], \"No baseline available\")\n",
        "                    )\n",
        "                    all_evaluations.append(evaluation)\n",
        "\n",
        "            return {\n",
        "                \"metadata\": {\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"model_used\": \"gpt-4o\",\n",
        "                    \"num_combinations_evaluated\": len(model_strategy_combinations),\n",
        "                    \"num_questions_evaluated\": len(questions),\n",
        "                    \"evaluation_status\": \"success\"\n",
        "                },\n",
        "                \"evaluations\": all_evaluations,\n",
        "                \"summary\": self._generate_summary(all_evaluations)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nCritical error in evaluation process: {str(e)}\")\n",
        "            return self._create_default_evaluation(experiment_results)\n",
        "\n",
        "    def _evaluate_single_response(self, result: Dict, baseline: str) -> Dict:\n",
        "        \"\"\"Evaluate a single response with chunk stats preservation\"\"\"\n",
        "        evaluation_prompt = self._construct_evaluation_prompt(result, baseline)\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert at evaluating LLM responses for accuracy and quality.\"},\n",
        "                    {\"role\": \"user\", \"content\": evaluation_prompt}\n",
        "                ],\n",
        "                temperature=0.7,\n",
        "                max_tokens=1000\n",
        "            )\n",
        "\n",
        "            content = response.choices[0].message.content\n",
        "            if '{' in content and '}' in content:\n",
        "                json_str = content[content.find('{'):content.rfind('}')+1]\n",
        "                evaluation = json.loads(json_str)\n",
        "\n",
        "                # Preserve chunk stats in evaluation\n",
        "                if \"chunk_stats\" in result:\n",
        "                    evaluation[\"chunk_stats\"] = result[\"chunk_stats\"]\n",
        "\n",
        "                return evaluation\n",
        "\n",
        "            return self._create_default_single_evaluation(result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error evaluating response: {str(e)}\")\n",
        "            return self._create_default_single_evaluation(result)\n",
        "\n",
        "\n",
        "    def _construct_evaluation_prompt(self, result: Dict, baseline: str) -> str:\n",
        "\n",
        "\n",
        "        return f\"\"\"You are evaluating a RAG (Retrieval Augmented Generation) system's response to a specific question. Your task is to:\n",
        "              1. First analyze if the requested information exists in the source document\n",
        "              2. Then evaluate how well the system retrieved and presented that information\n",
        "\n",
        "              EVALUATION CONTEXT:\n",
        "              Question: {result[\"question\"]}\n",
        "              Your Analysis of Source Document: {baseline}\n",
        "\n",
        "\n",
        "              System Response: {json.dumps(result[\"response\"], indent=2)}\n",
        "\n",
        "              SCORING CRITERIA:\n",
        "\n",
        "              1. ACCURACY (0-100):\n",
        "              - If information exists in source:\n",
        "                * 90-100: Retrieved complete and correct information with proper context\n",
        "                * 70-89: Retrieved correct information but with incomplete context\n",
        "                * 0-30: Failed to find existing information\n",
        "              - If information does not exist in source:\n",
        "                * 90-100: Correctly stated information cannot be found\n",
        "                * 0-30: Incorrectly claimed information is unavailable\n",
        "\n",
        "              2. SOURCE ATTRIBUTION (0-100):\n",
        "              - If information exists in source:\n",
        "                * 90-100: Provided full context (e.g., complete table data)\n",
        "                * 70-89: Provided partial but relevant context\n",
        "                * 0-30: Failed to provide sources for available information\n",
        "              - If information does not exist:\n",
        "                * 90-100: Correctly indicated no relevant sources\n",
        "                * 0-30: Failed to indicate lack of sources\n",
        "\n",
        "              3. CONCISENESS (0-100):\n",
        "              - 90-100: Direct answer with necessary context\n",
        "              - 70-89: Includes some unnecessary details\n",
        "              - 0-69: Verbose or missing important context\n",
        "\n",
        "              4. REASONABLENESS (0-100):\n",
        "              - 90-100: Confidence matches strength of evidence\n",
        "              - 70-89: Slightly mismatched confidence level\n",
        "              - 0-69: Inappropriate confidence given available evidence\n",
        "\n",
        "              Evaluate evidence quality:\n",
        "              - Table data > Direct quotes > Derived information > No sources\n",
        "\n",
        "            Provide your evaluation in this exact JSON format:\n",
        "            {{\n",
        "                \"model\": \"{result[\"model\"]}\",\n",
        "                \"chunk_strategy\": \"{result[\"chunk_strategy\"]}\",\n",
        "                \"threshold\": {result[\"threshold\"] if result[\"chunk_strategy\"] == \"semantic\" else \"null\"},\n",
        "                \"question\": \"{result[\"question\"]}\",\n",
        "                \"scores\": {{\n",
        "                    \"accuracy\": <score>,\n",
        "                    \"conciseness\": <score>,\n",
        "                    \"source_attribution\": <score>,\n",
        "                    \"reasonableness\": <score>\n",
        "                }},\n",
        "                \"composite_score\": <average of all scores>,\n",
        "                \"explanation\": \"Detailed explanation comparing the response to your baseline understanding and justifying each score\"\n",
        "            }}\"\"\"\n",
        "\n",
        "\n",
        "    def _create_default_evaluation(self, experiment_results: Dict) -> Dict:\n",
        "        \"\"\"Create a default evaluation structure when parsing fails\"\"\"\n",
        "        print(\"\\n--- Creating Default Evaluation Due to Failure ---\")\n",
        "        default_eval = {\n",
        "            \"metadata\": {\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"model_used\": \"gpt-4o\",\n",
        "                \"num_permutations_evaluated\": len(experiment_results[\"results\"]),\n",
        "                \"num_questions_evaluated\": len(set(r[\"question\"] for r in experiment_results[\"results\"])),\n",
        "                \"evaluation_status\": \"failed\"\n",
        "            },\n",
        "            \"evaluations\": [],\n",
        "            \"summary\": {\n",
        "                \"overall_performance\": \"Evaluation failed - using default structure\",\n",
        "                \"optimal_permutation\": \"Not available\",\n",
        "                \"performance_analysis\": \"Evaluation process encountered errors\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for result in experiment_results[\"results\"]:\n",
        "            default_eval[\"evaluations\"].append({\n",
        "                \"model\": result[\"model\"],\n",
        "                \"threshold\": result[\"threshold\"],\n",
        "                \"question\": result[\"question\"],\n",
        "                \"scores\": {\n",
        "                    \"accuracy\": 0,\n",
        "                    \"conciseness\": 0,\n",
        "                    \"source_attribution\": 0,\n",
        "                    \"reasonableness\": 0\n",
        "                },\n",
        "                \"composite_score\": 0,\n",
        "                \"explanation\": \"Evaluation failed - default scores assigned\"\n",
        "            })\n",
        "\n",
        "        print(\"Created default evaluation with\", len(default_eval[\"evaluations\"]), \"empty evaluations\")\n",
        "        return default_eval\n",
        "\n",
        "    def _create_default_single_evaluation(self, result: Dict) -> Dict:\n",
        "        \"\"\"Create a default evaluation for a single response when evaluation fails\"\"\"\n",
        "        return {\n",
        "            \"model\": result[\"model\"],\n",
        "            \"chunk_strategy\": result[\"chunk_strategy\"],\n",
        "            \"threshold\": result[\"threshold\"] if result[\"chunk_strategy\"] == \"semantic\" else None,\n",
        "            \"question\": result[\"question\"],\n",
        "            \"scores\": {\n",
        "                \"accuracy\": 0,\n",
        "                \"conciseness\": 0,\n",
        "                \"source_attribution\": 0,\n",
        "                \"reasonableness\": 0\n",
        "            },\n",
        "            \"composite_score\": 0,\n",
        "            \"explanation\": \"Evaluation failed - default scores assigned\"\n",
        "        }\n",
        "\n",
        "\n",
        "    def _generate_summary(self, evaluations: List[Dict]) -> Dict:\n",
        "        \"\"\"Generate summary statistics from evaluations with ordered results\"\"\"\n",
        "        if not evaluations:\n",
        "            return {\n",
        "                \"overall_performance\": \"No evaluations available\",\n",
        "                \"optimal_permutation\": \"Not available\",\n",
        "                \"performance_analysis\": \"Evaluation process failed\",\n",
        "                \"chunk_stats\": {}\n",
        "            }\n",
        "\n",
        "        # Calculate average scores by model/strategy combination\n",
        "        strategy_scores = {}\n",
        "        chunk_stats_by_config = {}\n",
        "\n",
        "        # Generate desired order from configs\n",
        "        desired_model_order = [model[\"name\"] for model in MODEL_CONFIGS[\"models\"]]\n",
        "        desired_chunking_order = []\n",
        "        for strategy in CHUNKING_CONFIGS[\"strategies\"]:\n",
        "            if strategy == \"semantic\":\n",
        "                for threshold in sorted(CHUNKING_CONFIGS[\"thresholds\"]):\n",
        "                    desired_chunking_order.append((strategy, threshold))\n",
        "            else:\n",
        "                desired_chunking_order.append((strategy, None))\n",
        "\n",
        "        # First pass: collect all scores and stats\n",
        "        for eval in evaluations:\n",
        "            # Handle scoring calculations\n",
        "            key = (eval[\"model\"], eval[\"chunk_strategy\"])\n",
        "            if \"threshold\" in eval and eval[\"chunk_strategy\"] == \"semantic\":\n",
        "                key = (eval[\"model\"], eval[\"chunk_strategy\"], eval[\"threshold\"])\n",
        "\n",
        "            if key not in strategy_scores:\n",
        "                strategy_scores[key] = {\n",
        "                    \"count\": 0,\n",
        "                    \"total_composite\": 0\n",
        "                }\n",
        "\n",
        "            scores = strategy_scores[key]\n",
        "            scores[\"count\"] += 1\n",
        "            scores[\"total_composite\"] += eval[\"composite_score\"]\n",
        "\n",
        "            # Collect chunk statistics\n",
        "            config_key = (eval[\"chunk_strategy\"],\n",
        "                        eval.get(\"threshold\") if eval[\"chunk_strategy\"] == \"semantic\" else None)\n",
        "\n",
        "            if config_key not in chunk_stats_by_config and \"chunk_stats\" in eval:\n",
        "                chunk_stats_by_config[config_key] = eval[\"chunk_stats\"]\n",
        "\n",
        "        # Calculate performance analysis with ordering\n",
        "        best_score = 0\n",
        "        best_config = None\n",
        "        temp_strategy_analysis = {}\n",
        "        ordered_strategy_analysis = {}\n",
        "\n",
        "        # Calculate all scores first\n",
        "        for key, scores in strategy_scores.items():\n",
        "            avg_composite = scores[\"total_composite\"] / scores[\"count\"]\n",
        "            model, strategy = key[:2]\n",
        "            threshold = key[2] if len(key) == 3 else None\n",
        "\n",
        "            config_str = (f\"{model} with {strategy} chunking\" +\n",
        "                        (f\" (threshold: {threshold})\" if threshold is not None else \"\"))\n",
        "\n",
        "            temp_strategy_analysis[key] = {\n",
        "                'config_str': config_str,\n",
        "                'score': avg_composite\n",
        "            }\n",
        "\n",
        "            if avg_composite > best_score:\n",
        "                best_score = avg_composite\n",
        "                best_config = config_str\n",
        "\n",
        "        # Order performance analysis by model and then chunking strategy\n",
        "        for model_name in desired_model_order:\n",
        "            for strategy_key in desired_chunking_order:\n",
        "                strategy, threshold = strategy_key\n",
        "                key = (model_name, strategy, threshold) if threshold is not None else (model_name, strategy)\n",
        "                if key in temp_strategy_analysis:\n",
        "                    ordered_strategy_analysis[temp_strategy_analysis[key]['config_str']] = \\\n",
        "                        temp_strategy_analysis[key]['score']\n",
        "\n",
        "        # Format chunk statistics with same ordering as chunking strategies\n",
        "        final_chunk_stats = {}\n",
        "        temp_stats = {}\n",
        "        for (strategy, threshold), stats in chunk_stats_by_config.items():\n",
        "            key = f\"{strategy}\" + (f\"_{threshold}\" if threshold is not None else \"\")\n",
        "            temp_stats[key] = {\n",
        "                \"num_chunks\": stats[\"num_chunks\"],\n",
        "                \"min_size\": stats[\"min_chunk_size\"],\n",
        "                \"max_size\": stats[\"max_chunk_size\"],\n",
        "                \"median_size\": stats.get(\"median_size\",\n",
        "                                      int((stats[\"min_chunk_size\"] + stats[\"max_chunk_size\"]) / 2)),\n",
        "                \"avg_size\": stats[\"avg_chunk_size\"]\n",
        "            }\n",
        "\n",
        "        # Create ordered chunk stats\n",
        "        for strategy, threshold in desired_chunking_order:\n",
        "            key = f\"{strategy}\" + (f\"_{threshold}\" if threshold is not None else \"\")\n",
        "            if key in temp_stats:\n",
        "                final_chunk_stats[key] = temp_stats[key]\n",
        "\n",
        "        return {\n",
        "            \"overall_performance\": f\"Average composite score across all evaluations: {sum(e['composite_score'] for e in evaluations)/len(evaluations):.2f}/100\",\n",
        "            \"optimal_permutation\": f\"Best performance: {best_config} (score: {best_score:.2f}/100)\",\n",
        "            \"performance_analysis\": ordered_strategy_analysis,\n",
        "            \"chunk_stats\": final_chunk_stats\n",
        "        }\n"
      ],
      "metadata": {
        "id": "1rAK93yw4qCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results Manager Class"
      ],
      "metadata": {
        "id": "lJmy7VbumFk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResultsManager:\n",
        "    \"\"\"Handles formatting, saving, and displaying evaluation results with simplified logic\"\"\"\n",
        "    def __init__(self, save_directory: str):\n",
        "        self.save_directory = save_directory\n",
        "        os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "    def format_results(self, experiment_results: Dict, evaluation_results: Dict) -> Tuple[Dict, Dict]:\n",
        "        \"\"\"Format experiment and evaluation results into structured output\"\"\"\n",
        "        print(\"\\n=== Starting Results Formatting ===\")\n",
        "\n",
        "        # Format experiment results - removing chunk_stats\n",
        "        formatted_experiment = {\n",
        "            \"metadata\": experiment_results.get(\"metadata\", {}),\n",
        "            \"results\": [{\n",
        "                \"model\": result[\"model\"],\n",
        "                \"chunk_strategy\": result[\"chunk_strategy\"],\n",
        "                \"threshold\": result[\"threshold\"],\n",
        "                \"question\": result[\"question\"],\n",
        "                \"response\": {\n",
        "                    \"answer\": result[\"response\"].get(\"response_text\", \"\"),\n",
        "                    \"sources\": result[\"response\"].get(\"sources\", [])\n",
        "                }\n",
        "            } for result in experiment_results[\"results\"]]\n",
        "        }\n",
        "\n",
        "        # Format evaluation results - chunk_stats remain here\n",
        "        formatted_evaluation = {\n",
        "            \"metadata\": evaluation_results[\"metadata\"],\n",
        "            \"evaluations\": evaluation_results.get(\"evaluations\", []),\n",
        "            \"overall_summary\": evaluation_results.get(\"summary\", {})\n",
        "        }\n",
        "\n",
        "        return formatted_experiment, formatted_evaluation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def save_results(self, formatted_experiment: Dict, formatted_evaluation: Dict) -> Tuple[str, str]:\n",
        "        \"\"\"Save formatted results to JSON files\"\"\"\n",
        "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "        experiment_file = f\"{self.save_directory}/experiment_results_{timestamp}.json\"\n",
        "        evaluation_file = f\"{self.save_directory}/evaluation_results_{timestamp}.json\"\n",
        "\n",
        "        for filepath, data in [\n",
        "            (experiment_file, formatted_experiment),\n",
        "            (evaluation_file, formatted_evaluation)\n",
        "        ]:\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return experiment_file, evaluation_file\n",
        "\n",
        "    def display_results(self, evaluation_results: Dict):\n",
        "        \"\"\"Display evaluation results in a clear, formatted manner\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EVALUATION RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Display metadata\n",
        "        metadata = evaluation_results.get(\"metadata\", {})\n",
        "        print(\"\\nMETADATA:\")\n",
        "        print(\"-\"*80)\n",
        "        print(f\"Timestamp:           {metadata.get('timestamp', 'Not available')}\")\n",
        "        print(f\"Model Used:          {metadata.get('model_used', 'Not available')}\")\n",
        "        print(f\"Combinations:        {metadata.get('num_combinations_evaluated', 'Not available')}\")\n",
        "        print(f\"Questions:           {metadata.get('num_questions_evaluated', 'Not available')}\")\n",
        "        print(f\"Evaluation Status:   {metadata.get('evaluation_status', 'Not available')}\")\n",
        "\n",
        "        # Display summary\n",
        "        summary = evaluation_results.get(\"summary\", {})\n",
        "        if summary:\n",
        "            print(\"\\nOVERALL ANALYSIS:\")\n",
        "            print(\"-\"*80)\n",
        "\n",
        "            # Overall performance\n",
        "            if \"overall_performance\" in summary:\n",
        "                print(\"\\nPerformance Summary:\")\n",
        "                print(textwrap.fill(summary[\"overall_performance\"], width=80))\n",
        "\n",
        "            # Optimal configuration\n",
        "            if \"optimal_permutation\" in summary:\n",
        "                print(\"\\nOptimal Configuration:\")\n",
        "                print(textwrap.fill(summary[\"optimal_permutation\"], width=80))\n",
        "\n",
        "            # Performance analysis\n",
        "            if \"performance_analysis\" in summary:\n",
        "                print(\"\\nPerformance Analysis:\")\n",
        "                analysis = summary[\"performance_analysis\"]\n",
        "                if isinstance(analysis, dict):\n",
        "                    for config, score in analysis.items():\n",
        "                        print(f\"{config}: {score:.2f}\")\n",
        "                else:\n",
        "                    print(textwrap.fill(str(analysis), width=80))\n",
        "\n",
        "            # Chunk statistics\n",
        "            if \"chunk_stats\" in summary and summary[\"chunk_stats\"]:\n",
        "                print(\"\\nChunk Statistics:\")\n",
        "                for config, stats in summary[\"chunk_stats\"].items():\n",
        "                    print(f\"\\n{config}:\")\n",
        "                    print(f\"  Number of chunks: {stats['num_chunks']}\")\n",
        "                    print(f\"  Min chunk size: {stats['min_size']} chars\")\n",
        "                    print(f\"  Max chunk size: {stats['max_size']} chars\")\n",
        "                    print(f\"  Median chunk size: {stats['median_size']} chars\")\n",
        "                    print(f\"  Average chunk size: {stats['avg_size']:.1f} chars\")"
      ],
      "metadata": {
        "id": "vnMb5d8cmKQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "koQ5ZObJC2ek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qdI5iaXYsun",
        "outputId": "042ba609-605f-4741-b3d3-a48d65d683c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting experiment with configurations:\n",
            "Models: ['open-mistral-nemo']\n",
            "Thresholds: [85, 95]\n",
            "Chunk strategies: ['semantic', 'paragraph', 'header']\n",
            "Number of questions: 1\n",
            "\n",
            "Testing model: open-mistral-nemo\n",
            "Processing question: What were cloud revenues in Q2 2024?\n",
            "\n",
            "=== COSINE SEARCH DEBUG ===\n",
            "Query: What were cloud revenues in Q2 2024?\n",
            "Strategy: semantic\n",
            "Threshold: 85\n",
            "Requested k: 5\n",
            "Cache key: semantic_85\n",
            "\n",
            "Testing model: open-mistral-nemo\n",
            "Processing question: What were cloud revenues in Q2 2024?\n",
            "\n",
            "=== COSINE SEARCH DEBUG ===\n",
            "Query: What were cloud revenues in Q2 2024?\n",
            "Strategy: semantic\n",
            "Threshold: 95\n",
            "Requested k: 5\n",
            "Cache key: semantic_95\n",
            "\n",
            "=== CHUNK CREATION DEBUG ===\n",
            "Strategy: paragraph\n",
            "Cache key: paragraph_2048\n",
            "Using max chunk size: 2048 characters with 100 character overlap\n",
            "\n",
            "Testing model: open-mistral-nemo\n",
            "Processing question: What were cloud revenues in Q2 2024?\n",
            "\n",
            "=== COSINE SEARCH DEBUG ===\n",
            "Query: What were cloud revenues in Q2 2024?\n",
            "Strategy: paragraph\n",
            "Threshold: None\n",
            "Requested k: 5\n",
            "Cache key: paragraph_1024\n",
            "Warning: No chunks found for strategy paragraph\n",
            "\n",
            "=== CHUNK CREATION DEBUG ===\n",
            "Strategy: header\n",
            "Cache key: header_2048\n",
            "Using max chunk size: 2048 characters with 100 character overlap\n",
            "\n",
            "Testing model: open-mistral-nemo\n",
            "Processing question: What were cloud revenues in Q2 2024?\n",
            "\n",
            "=== COSINE SEARCH DEBUG ===\n",
            "Query: What were cloud revenues in Q2 2024?\n",
            "Strategy: header\n",
            "Threshold: None\n",
            "Requested k: 5\n",
            "Cache key: header_1024\n",
            "Warning: No chunks found for strategy header\n",
            "\n",
            "Initializing GPT-4o evaluation...\n",
            "\n",
            "=== Starting Evaluation Process ===\n",
            "\n",
            "--- Getting Baseline Answers ---\n",
            "\n",
            "=== Starting Results Formatting ===\n",
            "\n",
            "================================================================================\n",
            "EVALUATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "METADATA:\n",
            "--------------------------------------------------------------------------------\n",
            "Timestamp:           2024-11-15T23:07:02.516646\n",
            "Model Used:          gpt-4o\n",
            "Combinations:        4\n",
            "Questions:           1\n",
            "Evaluation Status:   success\n",
            "\n",
            "Experiment complete!\n",
            "Results saved to:\n",
            "  Experiment results: /content/drive/My Drive/AI/Model_Analysis/experiment_results_20241115-230702.json\n",
            "  Evaluation results: /content/drive/My Drive/AI/Model_Analysis/evaluation_results_20241115-230702.json\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Initialize configurations\n",
        "    model_config = ModelConfig(\n",
        "        models=MODEL_CONFIGS[\"models\"],\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    # Initialize experiment runner\n",
        "    experiment_runner = ExperimentRunner(\n",
        "        model_config=model_config,\n",
        "        thresholds=CHUNKING_CONFIGS[\"thresholds\"],\n",
        "        questions=QUESTION_CONFIGS[\"questions\"],\n",
        "        chunk_strategies=CHUNKING_CONFIGS[\"strategies\"]\n",
        "    )\n",
        "\n",
        "    print(\"Starting experiment with configurations:\")\n",
        "    print(f\"Models: {[model['name'] for model in model_config.models]}\")\n",
        "    print(f\"Thresholds: {CHUNKING_CONFIGS['thresholds']}\")\n",
        "    print(f\"Chunk strategies: {CHUNKING_CONFIGS['strategies']}\")\n",
        "    print(f\"Number of questions: {len(QUESTION_CONFIGS['questions'])}\")\n",
        "\n",
        "    # Run experiments\n",
        "    experiment_results = experiment_runner.run_experiments()\n",
        "\n",
        "    # Get source document text\n",
        "    source_doc = documents[0].text\n",
        "\n",
        "    # Initialize evaluator\n",
        "    print(\"\\nInitializing GPT-4o evaluation...\")\n",
        "    evaluator = ExperimentEvaluator(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "    # Run evaluation\n",
        "    evaluation_results = evaluator.evaluate_experiments(\n",
        "        experiment_results=experiment_results,\n",
        "        source_doc=source_doc\n",
        "    )\n",
        "\n",
        "    # Initialize results manager\n",
        "    results_manager = ResultsManager(save_directory=FILE_CONFIGS['save_directory'])\n",
        "\n",
        "    # Format results\n",
        "    formatted_experiment, formatted_evaluation = results_manager.format_results(\n",
        "        experiment_results=experiment_results,\n",
        "        evaluation_results=evaluation_results\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    experiment_file, evaluation_file = results_manager.save_results(\n",
        "        formatted_experiment=formatted_experiment,\n",
        "        formatted_evaluation=formatted_evaluation\n",
        "    )\n",
        "\n",
        "    # Display results\n",
        "    results_manager.display_results(evaluation_results=formatted_evaluation)\n",
        "\n",
        "    print(\"\\nExperiment complete!\")\n",
        "    print(f\"Results saved to:\")\n",
        "    print(f\"  Experiment results: {experiment_file}\")\n",
        "    print(f\"  Evaluation results: {evaluation_file}\")\n",
        "\n",
        "    return formatted_experiment, formatted_evaluation\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results, evaluation = main()"
      ]
    }
  ]
}