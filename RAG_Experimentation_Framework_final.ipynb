{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNNTt79i3oNjLzGiollDl4K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjleece/rag-experimentation-framework/blob/main/RAG_Experimentation_Framework_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you use this code, please cite:\n",
        "\n",
        "{\n",
        "  title = {RAG Experimentation Framework},\n",
        "\n",
        "  author = {Bill Leece},\n",
        "\n",
        "  year = {2024}\n",
        "}"
      ],
      "metadata": {
        "id": "wZ0kV_UtQn5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "_lHNBLR-92Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers --quiet\n",
        "#!pip install -U optimum --quiet\n",
        "!pip install -U accelerate  --quiet\n",
        "#!pip install -U bitsandbytes  --quiet\n",
        "!pip install -U torch --quiet\n",
        "!pip install -U sentencepiece --quiet\n",
        "!pip install -U llama-index --quiet\n",
        "!pip install -U llama-index-llms-mistralai --quiet\n",
        "!pip install -U llama-index-embeddings-mistralai --quiet\n",
        "!pip install -U llama-index-llms-langchain --quiet\n",
        "!pip install -U langchain --quiet\n",
        "!pip install -U langchain-community --quiet\n",
        "!pip install -U langchain-mistralai --quiet\n",
        "!pip install -U langchain_huggingface --quiet\n",
        "!pip install -U faiss-gpu --quiet"
      ],
      "metadata": {
        "id": "4g_Vs7wgZW-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9bf7266-2aef-40d7-ac18-720563ab2fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.0/189.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.6/254.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "import transformers\n",
        "import torch\n",
        "import gc\n",
        "from google.colab import drive, userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "from llama_index.core import SimpleDirectoryReader, Settings\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "import time\n",
        "from typing import List, Dict, Tuple\n",
        "from contextlib import contextmanager\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "Ao7eaSfq-TKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get('MISTRAL_API_KEY')"
      ],
      "metadata": {
        "id": "YvGHY024-OXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' #Use GPUs when possible"
      ],
      "metadata": {
        "id": "mxAHV7T_-Xlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfUKkdCs-wQJ",
        "outputId": "033e2fbf-717c-43a4-c970-f88a5f1fdc41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 13 22:08:13 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   29C    P8              12W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment Configurations"
      ],
      "metadata": {
        "id": "jkqEV8M_HUKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup configurations\n",
        "MODEL_CONFIGS = {\n",
        "    \"models\": [\n",
        "    #    {\n",
        "    #        \"name\": \"open-mixtral-8x7b\",\n",
        "    #        \"type\": \"mistral_api\",\n",
        "    #        \"tokenizer\": None,  # Not needed for API models\n",
        "    #    },\n",
        "         {\n",
        "            \"name\": \"open-mistral-nemo\",\n",
        "            \"type\": \"mistral_api\",\n",
        "            \"tokenizer\": None,  # Not needed for API models\n",
        "         },\n",
        "   #     {\n",
        "   #         \"name\": \"ministral-8b-latest\",\n",
        "   #         \"type\": \"mistral_api\",\n",
        "   #         \"tokenizer\": None,  # Not needed for API models\n",
        "   #     },\n",
        "   #   {\n",
        "   #         \"name\": \"wjleece/quantized-mistral-7b\",\n",
        "   #         \"type\": \"huggingface_quantized\",\n",
        "   #         \"tokenizer\": \"mistralai/Mixtral-8x7B-v0.1\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "   #          \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "   #             \"load_in_4bit\": True,\n",
        "   #             \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "   #             \"bnb_4bit_quant_type\": \"nf4\",\n",
        "   #             \"bnb_4bit_use_double_quant\": False\n",
        "   #         }\n",
        "   #     },\n",
        "   #   {\n",
        "   #           \"name\": \"wjleece/quantized-mistral-nemo-12b\",\n",
        "   #           \"type\": \"huggingface_quantized\",\n",
        "   #           \"tokenizer\": \"mistralai/Mistral-Nemo-Instruct-2407\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "   #           \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "   #               \"load_in_4bit\": True,\n",
        "   #               \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "   #               \"bnb_4bit_quant_type\": \"nf4\",\n",
        "   #               \"bnb_4bit_use_double_quant\": False\n",
        "   #          }\n",
        "   #       },\n",
        "     #  {\n",
        "     #         \"name\": \"wjleece/quantized-mistral-8b\",\n",
        "     #         \"type\": \"huggingface_quantized\",\n",
        "     #         \"tokenizer\": \"mistralai/Ministral-8B-Instruct-2410\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "     #         \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "     #             \"load_in_4bit\": True,\n",
        "     #             \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "     #             \"bnb_4bit_quant_type\": \"nf4\",\n",
        "     #             \"bnb_4bit_use_double_quant\": False\n",
        "     #         }\n",
        "     #     }\n",
        "       ],\n",
        "    \"thresholds\": [85, 95], #RAG semantic chunking thresholds (higher thresholds --> fewer RAG chunks created)\n",
        "    \"chunk_strategies\": [\"semantic\", \"paragraph\", \"header\"] #got some surprising results here at times\n",
        "}\n",
        "\"\"\n",
        "QUESTION_CONFIGS = {\n",
        "    \"questions\": [\n",
        "        \"What were cloud revenues in Q2 2024?\",\n",
        "        \"What were the main drivers of revenue growth in Q2?\",\n",
        "        \"How much did YouTube ad revenues grow in Q2 in APAC?\",\n",
        "        \"Can you summarize recent key antitrust matters?\",\n",
        "        \"What were YouTube ad revenues in Q2?\"\n",
        "    ] #These quetsions should relate to the RAG document --> these are your 'business use cases'\n",
        "}\n",
        "\n",
        "FILE_CONFIGS = {\n",
        "    \"save_directory\": '/content/drive/My Drive/AI/Model_Analysis'\n",
        "}"
      ],
      "metadata": {
        "id": "YDjgk_JhHWkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load RAG Document"
      ],
      "metadata": {
        "id": "e_wxgOGc95sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "documents = SimpleDirectoryReader(input_files=[\"/content/drive/My Drive/AI/Datasets/Google-10-q/goog-10-q-q2-2024.pdf\"]).load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS4Lemk09v9Y",
        "outputId": "24ce29cb-3e33-4879-b8d1-1329b0e729b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RAG Pipeline Class"
      ],
      "metadata": {
        "id": "MgLxma5M-bZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGPipeline:\n",
        "    def __init__(self):\n",
        "        self.chunk_cache = {}\n",
        "        self.embedding_cache = {}\n",
        "        self.embedding_model = None\n",
        "\n",
        "    def initialize_embedding_model(self):\n",
        "        \"\"\"Initialize the embedding model if not already initialized\"\"\"\n",
        "        if self.embedding_model is None:\n",
        "            mistral_api_key = userdata.get('MISTRAL_API_KEY')\n",
        "            self.embedding_model = MistralAIEmbedding(\n",
        "                model_name=\"mistral-embed\",\n",
        "                api_key=mistral_api_key\n",
        "            )\n",
        "        return self.embedding_model\n",
        "\n",
        "    def create_chunks(self, documents: List, threshold: int, chunk_strategy: str = \"semantic\") -> Dict:\n",
        "        \"\"\"Create or retrieve chunks based on specified strategy\"\"\"\n",
        "        from langchain.text_splitter import (\n",
        "            RecursiveCharacterTextSplitter,\n",
        "            MarkdownHeaderTextSplitter\n",
        "        )\n",
        "\n",
        "        cache_key = f\"{chunk_strategy}_{threshold}\"\n",
        "        if cache_key not in self.chunk_cache:\n",
        "            print(f\"Creating new chunks for strategy {chunk_strategy} with threshold {threshold}\")\n",
        "\n",
        "            # Clear other cache entries if memory pressure is high\n",
        "            if len(self.chunk_cache) > 2:\n",
        "                oldest_key = min(self.chunk_cache.keys())\n",
        "                if oldest_key != cache_key:\n",
        "                    del self.chunk_cache[oldest_key]\n",
        "                    if oldest_key in self.embedding_cache:\n",
        "                        del self.embedding_cache[oldest_key]\n",
        "                    gc.collect()\n",
        "\n",
        "            if chunk_strategy == \"semantic\":\n",
        "                if self.embedding_model is None:\n",
        "                    self.initialize_embedding_model()\n",
        "\n",
        "                splitter = SemanticSplitterNodeParser(\n",
        "                    buffer_size=1,\n",
        "                    breakpoint_percentile_threshold=threshold,\n",
        "                    embed_model=self.embedding_model\n",
        "                )\n",
        "                nodes = splitter.get_nodes_from_documents(documents)\n",
        "                texts = [node.text for node in nodes]\n",
        "\n",
        "            elif chunk_strategy == \"paragraph\":\n",
        "                # Using RecursiveCharacterTextSplitter with paragraph breaks\n",
        "                text_splitter = RecursiveCharacterTextSplitter(\n",
        "                    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "                    chunk_size=threshold,  # Using threshold as approximate chunk size\n",
        "                    chunk_overlap=50,\n",
        "                    length_function=len\n",
        "                )\n",
        "                texts = []\n",
        "                for doc in documents:\n",
        "                    chunks = text_splitter.split_text(doc.text)\n",
        "                    texts.extend(chunks)\n",
        "\n",
        "            elif chunk_strategy == \"header\":\n",
        "                # Using MarkdownHeaderTextSplitter for section-based splitting\n",
        "                headers_to_split_on = [\n",
        "                    (\"#\", \"Header 1\"),\n",
        "                    (\"##\", \"Header 2\"),\n",
        "                    (\"###\", \"Header 3\"),\n",
        "                ]\n",
        "\n",
        "                # First split by headers\n",
        "                header_splitter = MarkdownHeaderTextSplitter(\n",
        "                    headers_to_split_on=headers_to_split_on\n",
        "                )\n",
        "\n",
        "                # Then split large sections into smaller chunks\n",
        "                text_splitter = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size=threshold,\n",
        "                    chunk_overlap=50\n",
        "                )\n",
        "\n",
        "                texts = []\n",
        "                for doc in documents:\n",
        "                    # Convert document to markdown-style headers for splitting\n",
        "                    md_text = self._convert_to_markdown_headers(doc.text)\n",
        "                    header_splits = header_splitter.split_text(md_text)\n",
        "\n",
        "                    # Further split large sections\n",
        "                    for split in header_splits:\n",
        "                        chunks = text_splitter.split_text(split.page_content)\n",
        "                        texts.extend(chunks)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown chunk strategy: {chunk_strategy}\")\n",
        "\n",
        "            self.chunk_cache[cache_key] = {\n",
        "                'texts': texts,\n",
        "                'strategy': chunk_strategy\n",
        "            }\n",
        "\n",
        "        return self.chunk_cache[cache_key]\n",
        "\n",
        "\n",
        "    def _convert_to_markdown_headers(self, text: str) -> str:\n",
        "        \"\"\"Convert document section titles to markdown headers\"\"\"\n",
        "        import re\n",
        "\n",
        "        # Common patterns for section headers in documents\n",
        "        patterns = [\n",
        "            (r'^(?:ITEM|Section)\\s+\\d+[.:]\\s*(.+)$', '# '),  # Main sections like \"ITEM 1: BUSINESS\"\n",
        "            (r'^\\d+\\.\\d+\\s+(.+)$', '## '),  # Subsections like \"1.1 Overview\"\n",
        "            (r'^\\([a-z]\\)\\s+(.+)$', '### '),  # Sub-subsections like \"(a) Description\"\n",
        "        ]\n",
        "\n",
        "        lines = text.split('\\n')\n",
        "        markdown_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            converted = False\n",
        "\n",
        "            for pattern, header_mark in patterns:\n",
        "                if re.match(pattern, line, re.IGNORECASE):\n",
        "                    markdown_lines.append(f\"{header_mark}{line}\")\n",
        "                    converted = True\n",
        "                    break\n",
        "\n",
        "            if not converted:\n",
        "                markdown_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(markdown_lines)\n",
        "\n",
        "\n",
        "    def run_cosine_search(self, query: str, threshold: int, chunk_strategy: str = \"semantic\", k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Run cosine similarity search with memory optimization\"\"\"\n",
        "        if self.embedding_model is None:\n",
        "            self.initialize_embedding_model()\n",
        "\n",
        "        cache_key = f\"{chunk_strategy}_{threshold}\"\n",
        "        if cache_key not in self.embedding_cache:\n",
        "            texts = self.chunk_cache[cache_key]['texts']\n",
        "\n",
        "            # Generate embeddings in batches\n",
        "            batch_size = 32\n",
        "            embeddings = []\n",
        "\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch_texts = texts[i:i + batch_size]\n",
        "                batch_embeddings = [self.embedding_model.get_text_embedding(text)\n",
        "                                  for text in batch_texts]\n",
        "                embeddings.extend(batch_embeddings)\n",
        "\n",
        "                if i % (batch_size * 4) == 0:\n",
        "                    gc.collect()\n",
        "\n",
        "            embeddings_array = np.array(embeddings).astype('float32')\n",
        "            normalized_embeddings = embeddings_array / np.linalg.norm(embeddings_array, axis=1)[:, np.newaxis]\n",
        "\n",
        "            dimension = embeddings_array.shape[1]\n",
        "            cosine_index = faiss.IndexFlatIP(dimension)\n",
        "            cosine_index.add(normalized_embeddings)\n",
        "\n",
        "            self.embedding_cache[cache_key] = {\n",
        "                'embeddings': embeddings_array,\n",
        "                'cosine_index': cosine_index\n",
        "            }\n",
        "\n",
        "        query_vector = self.embedding_model.get_text_embedding(query)\n",
        "        query_vector = np.array([query_vector]).astype('float32')\n",
        "        query_normalized = query_vector / np.linalg.norm(query_vector)\n",
        "\n",
        "        distances, indices = self.embedding_cache[cache_key]['cosine_index'].search(\n",
        "            query_normalized.reshape(1, -1).astype('float32'), k\n",
        "        )\n",
        "\n",
        "        return [\n",
        "            {\n",
        "                'text': self.chunk_cache[cache_key]['texts'][idx],\n",
        "                'distance': float(score),\n",
        "                'strategy': chunk_strategy\n",
        "            }\n",
        "            for score, idx in zip(distances[0], indices[0])\n",
        "        ]\n",
        "\n",
        "    def generate_response(self, query: str, context_rag: list, model: Dict) -> dict:\n",
        "        \"\"\"Generate response using provided context\"\"\"\n",
        "        try:\n",
        "            context_texts = [doc['text'] for doc in context_rag]\n",
        "            if not context_texts:\n",
        "                return {\"response_text\": \"No relevant context found.\", \"sources\": [], \"strategy\": context_rag[0]['strategy'] if context_rag else None}\n",
        "\n",
        "            context = \"\\n\\n\".join(context_texts)\n",
        "\n",
        "            prompt = PromptTemplate(template=\"\"\"\n",
        "            Instructions:\n",
        "\n",
        "            You are a helpful assistant who answers questions from context that has been provided to you.\n",
        "            Given the context information, provide a direct and concise answer to the question: {query}\n",
        "\n",
        "            Focus only on information present in the context. If you don't know the answer, say \"I don't know.\"\n",
        "            You must format your response as a JSON string object, starting with the word \"LLM_Response:\"\n",
        "\n",
        "            Your answer to {query} will be a JSON string object that starts with \"LLM_Response:\" as shown below:\n",
        "\n",
        "            LLM_Response:\n",
        "            {{\n",
        "                \"response_text\": \"Your detailed answer here\",\n",
        "                \"sources\": [\n",
        "                    \"Copy and paste here the exact text segments from the context that you used to generate your answer. Include all relevant segments, verbatim.\"\n",
        "                ]\n",
        "            }}\n",
        "\n",
        "            Important: In your response, the \"sources\" field must contain the exact text passages from the provided context that you used to formulate your answer. Copy these passages word-for-word.\n",
        "\n",
        "            Do not include a hypothetical example in your answer, only include your final answer after \"LLM_Response:\"\n",
        "\n",
        "            The context information that you will use for your answer is below:\n",
        "\n",
        "            ---------------\n",
        "            {context}\n",
        "            ---------------\n",
        "            \"\"\")\n",
        "\n",
        "            model_type = model['type']\n",
        "\n",
        "            llm = model['llm']\n",
        "\n",
        "            # Create the chain using the | operator\n",
        "            chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "            # Use invoke() with the input dictionary\n",
        "            response = chain.invoke({\n",
        "                \"query\": query,\n",
        "                \"context\": context\n",
        "               })\n",
        "\n",
        "            # Extract the JSON part\n",
        "            response_text = response.split(\"LLM_Response:\")[-1].strip()\n",
        "\n",
        "            # Try to parse as JSON\n",
        "            try:\n",
        "                if '{' in response_text and '}' in response_text:\n",
        "                    json_str = response_text[response_text.find('{'):response_text.rfind('}')+1]\n",
        "                    parsed_response = json.loads(json_str)\n",
        "                    return {\n",
        "                        \"response_text\": parsed_response.get(\"response_text\", response_text),\n",
        "                        \"sources\": parsed_response.get(\"sources\", []),\n",
        "                        \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                    }\n",
        "                else:\n",
        "                    return {\n",
        "                        \"response_text\": response_text,\n",
        "                        \"sources\": [],\n",
        "                        \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                    }\n",
        "            except json.JSONDecodeError:\n",
        "                return {\n",
        "                    \"response_text\": response_text,\n",
        "                    \"sources\": [],\n",
        "                    \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {str(e)}\")\n",
        "            return {\"response_text\": \"An error occurred while generating the response.\", \"sources\": []}\n",
        "\n",
        "\n",
        "# Global RAG pipeline instance\n",
        "_GLOBAL_RAG_PIPELINE = None"
      ],
      "metadata": {
        "id": "YY5rnivk-bAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment Class"
      ],
      "metadata": {
        "id": "05gTul4pIW6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MemoryOptimizedExperimentConfig:\n",
        "    def __init__(self,\n",
        "                 models: List[Dict],\n",
        "                 thresholds: List[int],\n",
        "                 questions: List[str],\n",
        "                 temperature: float,\n",
        "                 chunk_strategies: List[str]):\n",
        "        self.models = models\n",
        "        self.thresholds = thresholds\n",
        "        self.questions = questions\n",
        "        self.temperature = temperature\n",
        "        self.chunk_strategies = chunk_strategies\n",
        "\n",
        "        global _GLOBAL_RAG_PIPELINE\n",
        "        if _GLOBAL_RAG_PIPELINE is None:\n",
        "            print(\"Initializing global RAG pipeline\")\n",
        "            _GLOBAL_RAG_PIPELINE = RAGPipeline()\n",
        "        else:\n",
        "            print(\"Using existing global RAG pipeline\")\n",
        "        self.rag_pipeline = _GLOBAL_RAG_PIPELINE\n",
        "\n",
        "        self.current_model = None\n",
        "        self.current_model_name = None\n",
        "\n",
        "    @contextmanager\n",
        "    def load_model(self, model_config: Dict):\n",
        "        \"\"\"Context manager for lazy loading and proper cleanup of models\"\"\"\n",
        "        try:\n",
        "            model_name = model_config[\"name\"]\n",
        "            model_type = model_config[\"type\"]\n",
        "\n",
        "            # Clear any existing model\n",
        "            self.cleanup_current_model()\n",
        "\n",
        "            if model_type == \"mistral_api\":\n",
        "                mistral_api_key = userdata.get('MISTRAL_API_KEY')\n",
        "                self.current_model = {\n",
        "                    'llm': ChatMistralAI(\n",
        "                        model=model_name,\n",
        "                        temperature=self.temperature,\n",
        "                        api_key=mistral_api_key\n",
        "                    ),\n",
        "                    'type': 'mistral_api'\n",
        "                }\n",
        "            else:  # huggingface_quantized\n",
        "                print(f\"Loading quantized model: {model_name}\")\n",
        "\n",
        "                # Empty CUDA cache before loading new model\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                tokenizer = AutoTokenizer.from_pretrained(\n",
        "                    pretrained_model_name_or_path=model_config[\"tokenizer\"],\n",
        "                    trust_remote_code=True,\n",
        "                    use_fast=True,\n",
        "                    padding_side=\"left\"\n",
        "                )\n",
        "\n",
        "                model = AutoModelForCausalLM.from_pretrained(\n",
        "                    pretrained_model_name_or_path=model_name,\n",
        "                    device_map=\"auto\",\n",
        "                    trust_remote_code=True,\n",
        "                    torch_dtype=torch.float16,\n",
        "                    use_cache=True,\n",
        "                    low_cpu_mem_usage=True,\n",
        "                )\n",
        "\n",
        "                pipe = pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_new_tokens=512,\n",
        "                    temperature=self.temperature,\n",
        "                    top_p=0.95,\n",
        "                    top_k=50,\n",
        "                    do_sample=True,\n",
        "                    device_map=\"auto\"\n",
        "                )\n",
        "\n",
        "                self.current_model = {\n",
        "                    'llm': HuggingFacePipeline(pipeline=pipe),\n",
        "                    'type': 'huggingface_quantized',\n",
        "                    'model': model,  # Keep reference for cleanup\n",
        "                    'pipe': pipe     # Keep reference for cleanup\n",
        "                }\n",
        "\n",
        "            self.current_model_name = model_name\n",
        "            yield self.current_model\n",
        "\n",
        "        finally:\n",
        "            # Cleanup will happen in cleanup_current_model()\n",
        "            pass\n",
        "\n",
        "    def cleanup_current_model(self):\n",
        "        \"\"\"Clean up the current model and free memory\"\"\"\n",
        "        if self.current_model is not None:\n",
        "            if self.current_model['type'] == 'huggingface_quantized':\n",
        "                # Delete model components explicitly\n",
        "                del self.current_model['llm']\n",
        "                del self.current_model['model']\n",
        "                del self.current_model['pipe']\n",
        "\n",
        "                # Clear CUDA cache\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                # Run garbage collection\n",
        "                gc.collect()\n",
        "\n",
        "            self.current_model = None\n",
        "            self.current_model_name = None\n",
        "\n",
        "    def run_experiment(self):\n",
        "        \"\"\"Run experiments with optimized memory management\"\"\"\n",
        "        results = {\n",
        "            \"metadata\": {\n",
        "                \"timestamp\": time.strftime(\"%Y%m%d-%H%M%S\"),\n",
        "                \"models_tested\": [model[\"name\"] for model in self.models],\n",
        "                \"thresholds_tested\": self.thresholds,\n",
        "                \"chunk_strategies_tested\": self.chunk_strategies,\n",
        "                \"temperature\": self.temperature\n",
        "            },\n",
        "            \"results\": []\n",
        "        }\n",
        "\n",
        "        # Process each strategy\n",
        "        for strategy in self.chunk_strategies:\n",
        "            print(f\"\\nProcessing strategy: {strategy}\")\n",
        "\n",
        "            # Only iterate through thresholds for semantic chunking\n",
        "            if strategy == \"semantic\":\n",
        "                thresholds_to_test = self.thresholds\n",
        "            else:\n",
        "                # Use a single default threshold for non-semantic strategies\n",
        "                # This could be an average chunk size for paragraph/header splitting\n",
        "                thresholds_to_test = [1000]  # Default chunk size for non-semantic strategies\n",
        "\n",
        "            for threshold in thresholds_to_test:\n",
        "                if strategy == \"semantic\":\n",
        "                    print(f\"Processing threshold: {threshold}\")\n",
        "\n",
        "                self.rag_pipeline.create_chunks(documents, threshold, strategy)\n",
        "\n",
        "                # Process each model\n",
        "                for model_config in self.models:\n",
        "                    model_name = model_config[\"name\"]\n",
        "                    print(f\"\\nTesting model: {model_name}\")\n",
        "\n",
        "                    with self.load_model(model_config) as model:\n",
        "                        for question in self.questions:\n",
        "                            print(f\"Processing question: {question}\")\n",
        "\n",
        "                            context = self.rag_pipeline.run_cosine_search(\n",
        "                                query=question,\n",
        "                                threshold=threshold,\n",
        "                                chunk_strategy=strategy\n",
        "                            )\n",
        "\n",
        "                            answer = self.rag_pipeline.generate_response(\n",
        "                                query=question,\n",
        "                                context_rag=context,\n",
        "                                model=model\n",
        "                            )\n",
        "\n",
        "                            results[\"results\"].append({\n",
        "                                \"model\": model_name,\n",
        "                                \"threshold\": threshold if strategy == \"semantic\" else None,\n",
        "                                \"chunk_strategy\": strategy,\n",
        "                                \"question\": question,\n",
        "                                \"response\": answer\n",
        "                            })\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "tCzG7OE0IiDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluator Class"
      ],
      "metadata": {
        "id": "EpjD-Qz54mfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "import tiktoken\n",
        "import textwrap\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "class ExperimentEvaluator:\n",
        "    def __init__(self, api_key: str):\n",
        "        self.client = openai.OpenAI(api_key=api_key)\n",
        "        self.encoder = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "\n",
        "    def _get_baseline_answers(self, questions: List[str], source_doc: str) -> Dict[str, str]:\n",
        "        \"\"\"Get GPT-4o's own answers to the questions as baseline\"\"\"\n",
        "        baseline_prompt = f\"\"\"Source Document:\n",
        "        {source_doc}\n",
        "\n",
        "        Using only the information from the source document above, answer these questions.\n",
        "        Format your response as a valid JSON object with questions as keys and answers as values.\n",
        "        Keep answers concise and factual.\n",
        "\n",
        "        Questions to answer:\n",
        "        {json.dumps(questions, indent=2)}\n",
        "\n",
        "        Response format example:\n",
        "        {{\n",
        "            \"Question 1\": \"Answer 1\",\n",
        "            \"Question 2\": \"Answer 2\"\n",
        "        }}\"\"\"\n",
        "\n",
        "        try:\n",
        "            print(\"\\n--- Getting Baseline Answers ---\")\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides JSON-formatted answers based on source documents.\"},\n",
        "                    {\"role\": \"user\", \"content\": baseline_prompt}\n",
        "                ],\n",
        "                temperature=0.1\n",
        "            )\n",
        "\n",
        "            content = response.choices[0].message.content\n",
        "            print(\"Baseline response received:\", content[:200] + \"...\")\n",
        "\n",
        "            if '{' in content and '}' in content:\n",
        "                json_str = content[content.find('{'):content.rfind('}')+1]\n",
        "                return json.loads(json_str)\n",
        "            else:\n",
        "                print(\"Warning: No JSON structure found in GPT-4o's baseline response\")\n",
        "                return {\"error\": \"No JSON structure found\", \"questions\": questions}\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Warning: Could not parse GPT-4o's baseline answers as JSON: {str(e)}\")\n",
        "            return {\"error\": \"JSON parsing failed\", \"questions\": questions}\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error getting baseline answers: {str(e)}\")\n",
        "            return {\"error\": str(e), \"questions\": questions}\n",
        "\n",
        "    def evaluate_experiments(self, experiment_results: Dict, source_doc: str) -> Dict:\n",
        "        \"\"\"Evaluate experiment results using GPT-4o\"\"\"\n",
        "        try:\n",
        "            print(\"\\n=== Starting Evaluation Process ===\")\n",
        "            questions = list(set(result[\"question\"] for result in experiment_results[\"results\"]))\n",
        "            print(f\"Number of unique questions to evaluate: {len(questions)}\")\n",
        "\n",
        "            # Get unique model/strategy/(threshold) combinations\n",
        "            model_strategy_combinations = set(\n",
        "                (result[\"model\"],\n",
        "                result[\"chunk_strategy\"],\n",
        "                result[\"threshold\"] if result[\"chunk_strategy\"] == \"semantic\" else None)\n",
        "                for result in experiment_results[\"results\"]\n",
        "            )\n",
        "            print(f\"Number of model/strategy combinations: {len(model_strategy_combinations)}\")\n",
        "\n",
        "            # Get baseline answers for comparison\n",
        "            baseline_answers = self._get_baseline_answers(questions, source_doc)\n",
        "            print(\"Baseline answers received\")\n",
        "\n",
        "            # Evaluate each combination separately\n",
        "            all_evaluations = []\n",
        "\n",
        "            for model, strategy, threshold in model_strategy_combinations:\n",
        "                print(f\"\\nEvaluating model: {model}, strategy: {strategy}\" +\n",
        "                      (f\", threshold: {threshold}\" if strategy == \"semantic\" else \"\"))\n",
        "\n",
        "                relevant_results = [r for r in experiment_results[\"results\"]\n",
        "                                  if r[\"model\"] == model and\n",
        "                                    r[\"chunk_strategy\"] == strategy and\n",
        "                                    (r[\"threshold\"] == threshold if strategy == \"semantic\" else True)]\n",
        "\n",
        "                for result in relevant_results:\n",
        "                    # First, construct the threshold part of the JSON template separately\n",
        "                    threshold_json_part = f'\"threshold\": {result[\"threshold\"]},' if result[\"chunk_strategy\"] == \"semantic\" else \"\"\n",
        "\n",
        "                    evaluation_prompt = f\"\"\"Evaluate this specific response:\n",
        "\n",
        "            Question: {result[\"question\"]}\n",
        "            Baseline Answer: {baseline_answers.get(result[\"question\"], \"No baseline available\")}\n",
        "            Model: {result[\"model\"]}\n",
        "            Chunking Strategy: {result[\"chunk_strategy\"]}\n",
        "            {f'Threshold: {result[\"threshold\"]}' if result[\"chunk_strategy\"] == \"semantic\" else ''}\n",
        "            Response: {json.dumps(result[\"response\"], indent=2)}\n",
        "\n",
        "            Context about chunking strategies:\n",
        "            - Semantic chunking uses thresholds to determine chunk boundaries based on semantic similarity\n",
        "            - Paragraph chunking splits text at paragraph breaks (no threshold needed)\n",
        "            - Header chunking splits text at section headers (no threshold needed)\n",
        "\n",
        "            Score the response on these criteria (0-100):\n",
        "            - Accuracy: How well does it match the baseline/source\n",
        "            - Conciseness: Clear, direct answer without extra information\n",
        "            - Source Attribution: Uses relevant source text as evidence\n",
        "            - Reasonableness: Answer is properly contextualized\n",
        "\n",
        "            Provide your evaluation in this exact JSON format:\n",
        "            {{\n",
        "                \"model\": \"{result[\"model\"]}\",\n",
        "                \"chunk_strategy\": \"{result[\"chunk_strategy\"]}\",\n",
        "                {threshold_json_part}\n",
        "                \"question\": \"{result[\"question\"]}\",\n",
        "                \"scores\": {{\n",
        "                    \"accuracy\": <score>,\n",
        "                    \"conciseness\": <score>,\n",
        "                    \"source_attribution\": <score>,\n",
        "                    \"reasonableness\": <score>\n",
        "                }},\n",
        "                \"composite_score\": <average of scores>,\n",
        "                \"explanation\": \"detailed explanation\"\n",
        "            }}\"\"\"\n",
        "\n",
        "                    try:\n",
        "                        response = self.client.chat.completions.create(\n",
        "                            model=\"gpt-4o\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"You are an expert at evaluating LLM responses for accuracy and quality.\"},\n",
        "                                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
        "                            ],\n",
        "                            temperature=0.7,\n",
        "                            max_tokens=1000\n",
        "                        )\n",
        "\n",
        "                        content = response.choices[0].message.content\n",
        "                        print(f\"\\nEvaluating {model}/{strategy}\" +\n",
        "                              (f\"/{threshold}\" if strategy == \"semantic\" else \"\") +\n",
        "                              f\"/{result['question']}\")\n",
        "                        print(\"Raw response:\", content[:200] + \"...\")\n",
        "\n",
        "                        if '{' in content and '}' in content:\n",
        "                            json_str = content[content.find('{'):content.rfind('}')+1]\n",
        "                            evaluation = json.loads(json_str)\n",
        "                            all_evaluations.append(evaluation)\n",
        "                        else:\n",
        "                            print(f\"No JSON found in response for {model}/{strategy}/{result['question']}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error evaluating {model}/{strategy}/{result['question']}: {str(e)}\")\n",
        "\n",
        "            # Create final evaluation structure with updated metadata\n",
        "            final_evaluation = {\n",
        "                \"metadata\": {\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"model_used\": \"gpt-4o\",\n",
        "                    \"num_combinations_evaluated\": len(model_strategy_combinations),\n",
        "                    \"num_questions_evaluated\": len(questions),\n",
        "                    \"evaluation_status\": \"success\" if all_evaluations else \"failed\",\n",
        "                    \"chunking_strategies_used\": list(set(result[\"chunk_strategy\"] for result in experiment_results[\"results\"]))\n",
        "                },\n",
        "                \"evaluations\": all_evaluations,\n",
        "                \"summary\": self._generate_summary(all_evaluations)\n",
        "            }\n",
        "\n",
        "            return final_evaluation\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nCritical error in evaluation process: {str(e)}\")\n",
        "            return self._create_default_evaluation(experiment_results)\n",
        "\n",
        "    def _generate_summary(self, evaluations: List[Dict]) -> Dict:\n",
        "        \"\"\"Generate summary statistics from evaluations with chunking strategy awareness\"\"\"\n",
        "        if not evaluations:\n",
        "            return {\n",
        "                \"overall_performance\": \"No evaluations available\",\n",
        "                \"optimal_configuration\": \"Not available\",\n",
        "                \"performance_analysis\": \"Evaluation process failed\"\n",
        "            }\n",
        "\n",
        "        # Calculate average scores by model/strategy combination\n",
        "        strategy_scores = {}\n",
        "        for eval in evaluations:\n",
        "            key = (eval[\"model\"], eval[\"chunk_strategy\"])\n",
        "            if \"threshold\" in eval:  # Only for semantic chunking\n",
        "                key = (eval[\"model\"], eval[\"chunk_strategy\"], eval[\"threshold\"])\n",
        "\n",
        "            if key not in strategy_scores:\n",
        "                strategy_scores[key] = {\n",
        "                    \"count\": 0,\n",
        "                    \"total_accuracy\": 0,\n",
        "                    \"total_conciseness\": 0,\n",
        "                    \"total_source_attribution\": 0,\n",
        "                    \"total_reasonableness\": 0,\n",
        "                    \"total_composite\": 0\n",
        "                }\n",
        "\n",
        "            scores = strategy_scores[key]\n",
        "            scores[\"count\"] += 1\n",
        "            scores[\"total_accuracy\"] += eval[\"scores\"][\"accuracy\"]\n",
        "            scores[\"total_conciseness\"] += eval[\"scores\"][\"conciseness\"]\n",
        "            scores[\"total_source_attribution\"] += eval[\"scores\"][\"source_attribution\"]\n",
        "            scores[\"total_reasonableness\"] += eval[\"scores\"][\"reasonableness\"]\n",
        "            scores[\"total_composite\"] += eval[\"composite_score\"]\n",
        "\n",
        "        # Find best performing configuration\n",
        "        best_score = 0\n",
        "        best_config = None\n",
        "\n",
        "        strategy_analysis = {}\n",
        "        for key, scores in strategy_scores.items():\n",
        "            avg_composite = scores[\"total_composite\"] / scores[\"count\"]\n",
        "            if len(key) == 3:  # Semantic chunking with threshold\n",
        "                model, strategy, threshold = key\n",
        "                config_str = f\"{model} with {strategy} chunking (threshold: {threshold})\"\n",
        "            else:  # Other chunking strategies\n",
        "                model, strategy = key\n",
        "                config_str = f\"{model} with {strategy} chunking\"\n",
        "\n",
        "            strategy_analysis[config_str] = avg_composite\n",
        "\n",
        "            if avg_composite > best_score:\n",
        "                best_score = avg_composite\n",
        "                best_config = config_str\n",
        "\n",
        "        return {\n",
        "            \"overall_performance\": f\"Average composite score across all evaluations: {sum(e['composite_score'] for e in evaluations)/len(evaluations):.2f}/100\",\n",
        "            \"optimal_configuration\": f\"Best performance: {best_config} (score: {best_score:.2f}/100)\",\n",
        "            \"performance_analysis\": {\n",
        "                \"configurations_tested\": len(strategy_scores),\n",
        "                \"total_evaluations\": len(evaluations),\n",
        "                \"strategy_performance\": strategy_analysis\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _create_default_evaluation(self, experiment_results: Dict) -> Dict:\n",
        "        \"\"\"Create a default evaluation structure when parsing fails\"\"\"\n",
        "        print(\"\\n--- Creating Default Evaluation Due to Failure ---\")\n",
        "        default_eval = {\n",
        "            \"metadata\": {\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"model_used\": \"gpt-4o\",\n",
        "                \"num_permutations_evaluated\": len(experiment_results[\"results\"]),\n",
        "                \"num_questions_evaluated\": len(set(r[\"question\"] for r in experiment_results[\"results\"])),\n",
        "                \"evaluation_status\": \"failed\"\n",
        "            },\n",
        "            \"evaluations\": [],\n",
        "            \"summary\": {\n",
        "                \"overall_performance\": \"Evaluation failed - using default structure\",\n",
        "                \"optimal_permutation\": \"Not available\",\n",
        "                \"performance_analysis\": \"Evaluation process encountered errors\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for result in experiment_results[\"results\"]:\n",
        "            default_eval[\"evaluations\"].append({\n",
        "                \"model\": result[\"model\"],\n",
        "                \"threshold\": result[\"threshold\"],\n",
        "                \"question\": result[\"question\"],\n",
        "                \"scores\": {\n",
        "                    \"accuracy\": 0,\n",
        "                    \"conciseness\": 0,\n",
        "                    \"source_attribution\": 0,\n",
        "                    \"reasonableness\": 0\n",
        "                },\n",
        "                \"composite_score\": 0,\n",
        "                \"explanation\": \"Evaluation failed - default scores assigned\"\n",
        "            })\n",
        "\n",
        "        print(\"Created default evaluation with\", len(default_eval[\"evaluations\"]), \"empty evaluations\")\n",
        "        return default_eval\n",
        "\n",
        "    def format_and_save_results(self, experiment_results: Dict, evaluation_results: Dict, save_dir: str):\n",
        "        \"\"\"Format and save both experiment and evaluation results\"\"\"\n",
        "        print(\"\\n=== Starting Results Formatting ===\")\n",
        "        try:\n",
        "            print(\"Input evaluation_results keys:\", list(evaluation_results.keys()))\n",
        "\n",
        "            if not isinstance(evaluation_results, dict):\n",
        "                print(\"Warning: evaluation_results is not a dictionary\")\n",
        "                evaluation_results = self._create_default_evaluation(experiment_results)\n",
        "\n",
        "            if \"metadata\" not in evaluation_results:\n",
        "                print(\"Warning: metadata missing from evaluation_results\")\n",
        "                evaluation_results = self._create_default_evaluation(experiment_results)\n",
        "\n",
        "            # Format experiment results\n",
        "            formatted_experiment = {\n",
        "                \"metadata\": experiment_results.get(\"metadata\", {}),\n",
        "                \"results\": []\n",
        "            }\n",
        "\n",
        "            # Group results by model and strategy/threshold\n",
        "            for result in experiment_results[\"results\"]:\n",
        "                formatted_result = {\n",
        "                    \"model\": result[\"model\"],\n",
        "                    \"chunk_strategy\": result[\"chunk_strategy\"],\n",
        "                    \"threshold\": result[\"threshold\"],\n",
        "                    \"question\": result[\"question\"],\n",
        "                    \"response\": {\n",
        "                        \"answer\": result[\"response\"].get(\"response_text\", \"\"),\n",
        "                        \"sources\": result[\"response\"].get(\"sources\", [])\n",
        "                    }\n",
        "                }\n",
        "                formatted_experiment[\"results\"].append(formatted_result)\n",
        "\n",
        "            # Format evaluation results with aggregated scores\n",
        "            formatted_evaluation = {\n",
        "                \"metadata\": evaluation_results[\"metadata\"],\n",
        "                \"model_evaluations\": {},\n",
        "                \"overall_summary\": evaluation_results.get(\"summary\", {})\n",
        "            }\n",
        "\n",
        "            # Process evaluations if they exist\n",
        "            if \"evaluations\" in evaluation_results:\n",
        "                for eval in evaluation_results[\"evaluations\"]:\n",
        "                    model_name = eval[\"model\"]\n",
        "                    chunk_strategy = eval.get(\"chunk_strategy\", \"semantic\")  # default to semantic for backward compatibility\n",
        "                    threshold = eval.get(\"threshold\")\n",
        "\n",
        "                    if model_name not in formatted_evaluation[\"model_evaluations\"]:\n",
        "                        formatted_evaluation[\"model_evaluations\"][model_name] = {\n",
        "                            \"strategies\": {}\n",
        "                        }\n",
        "\n",
        "                    strategy_key = f\"{chunk_strategy}\"\n",
        "                    if strategy_key not in formatted_evaluation[\"model_evaluations\"][model_name][\"strategies\"]:\n",
        "                        formatted_evaluation[\"model_evaluations\"][model_name][\"strategies\"][strategy_key] = {\n",
        "                            \"thresholds\": {} if chunk_strategy == \"semantic\" else {\"default\": {\"questions\": []}}\n",
        "                        }\n",
        "\n",
        "                    if chunk_strategy == \"semantic\":\n",
        "                        threshold_key = str(threshold)\n",
        "                        if threshold_key not in formatted_evaluation[\"model_evaluations\"][model_name][\"strategies\"][strategy_key][\"thresholds\"]:\n",
        "                            formatted_evaluation[\"model_evaluations\"][model_name][\"strategies\"][strategy_key][\"thresholds\"][threshold_key] = {\n",
        "                                \"questions\": [],\n",
        "                                \"average_scores\": {\n",
        "                                    \"accuracy\": 0,\n",
        "                                    \"conciseness\": 0,\n",
        "                                    \"source_attribution\": 0,\n",
        "                                    \"reasonableness\": 0,\n",
        "                                    \"composite\": 0\n",
        "                                }\n",
        "                            }\n",
        "                        target_dict = formatted_evaluation[\"model_evaluations\"][model_name][\"strategies\"][strategy_key][\"thresholds\"][threshold_key]\n",
        "                    else:\n",
        "                        target_dict = formatted_evaluation[\"model_evaluations\"][model_name][\"strategies\"][strategy_key][\"thresholds\"][\"default\"]\n",
        "\n",
        "                    # Add question evaluation\n",
        "                    target_dict[\"questions\"].append({\n",
        "                        \"question\": eval[\"question\"],\n",
        "                        \"scores\": eval[\"scores\"],\n",
        "                        \"composite_score\": eval.get(\"composite_score\", 0),\n",
        "                        \"explanation\": eval.get(\"explanation\", \"\")\n",
        "                    })\n",
        "\n",
        "                    # Update average scores\n",
        "                    if \"average_scores\" not in target_dict:\n",
        "                        target_dict[\"average_scores\"] = {\n",
        "                            \"accuracy\": 0,\n",
        "                            \"conciseness\": 0,\n",
        "                            \"source_attribution\": 0,\n",
        "                            \"reasonableness\": 0,\n",
        "                            \"composite\": 0\n",
        "                        }\n",
        "\n",
        "                    questions = target_dict[\"questions\"]\n",
        "                    avg_scores = target_dict[\"average_scores\"]\n",
        "\n",
        "                    avg_scores[\"accuracy\"] = sum(q[\"scores\"][\"accuracy\"] for q in questions) / len(questions)\n",
        "                    avg_scores[\"conciseness\"] = sum(q[\"scores\"][\"conciseness\"] for q in questions) / len(questions)\n",
        "                    avg_scores[\"source_attribution\"] = sum(q[\"scores\"][\"source_attribution\"] for q in questions) / len(questions)\n",
        "                    avg_scores[\"reasonableness\"] = sum(q[\"scores\"][\"reasonableness\"] for q in questions) / len(questions)\n",
        "                    avg_scores[\"composite\"] = sum(q[\"composite_score\"] for q in questions) / len(questions)\n",
        "\n",
        "            # Save formatted results\n",
        "            timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "            experiment_file = f\"{save_dir}/experiment_results_{timestamp}.json\"\n",
        "            evaluation_file = f\"{save_dir}/evaluation_results_{timestamp}.json\"\n",
        "\n",
        "            with open(experiment_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(formatted_experiment, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            with open(evaluation_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(formatted_evaluation, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            print(\"\\n=== Results Formatting Complete ===\")\n",
        "            return formatted_experiment, formatted_evaluation\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError in formatting and saving results: {str(e)}\")\n",
        "            default_eval = self._create_default_evaluation(experiment_results)\n",
        "            return experiment_results, default_eval\n",
        "\n",
        "\n",
        "    def evaluate_and_format(self, experiment_results: Dict, source_doc: str) -> Dict:\n",
        "        \"\"\"Convenience method for evaluation and formatting\"\"\"\n",
        "        print(\"\\n=== Starting evaluate_and_format ===\")\n",
        "        print(\"Step 1: Running evaluation\")\n",
        "        evaluation = self.evaluate_experiments(experiment_results, source_doc)\n",
        "\n",
        "        print(\"\\nStep 2: Running display_results\")\n",
        "        self.display_results(evaluation)\n",
        "\n",
        "        print(\"\\nStep 3: Returning evaluation\")\n",
        "        return evaluation\n",
        "\n",
        "\n",
        "    def display_results(self, evaluation_results: Dict = None):\n",
        "        \"\"\"Format and display evaluation results\"\"\"\n",
        "        try:\n",
        "            results = evaluation_results\n",
        "\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"MODEL EVALUATION RESULTS\")\n",
        "            print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "            if \"evaluations\" in results:\n",
        "                print(\"DETAILED MODEL PERFORMANCE\")\n",
        "                print(\"-\"*80)\n",
        "                current_model = None\n",
        "                current_strategy = None\n",
        "                current_threshold = None\n",
        "\n",
        "                # Sort evaluations by model, strategy, threshold, then question\n",
        "                sorted_evaluations = sorted(\n",
        "                    results[\"evaluations\"],\n",
        "                    key=lambda x: (\n",
        "                        x[\"model\"],\n",
        "                        x.get(\"chunk_strategy\", \"semantic\"),\n",
        "                        str(x.get(\"threshold\", \"default\")),\n",
        "                        x[\"question\"]\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                for eval in sorted_evaluations:\n",
        "                    # Print model header if it's a new model\n",
        "                    if eval[\"model\"] != current_model:\n",
        "                        current_model = eval[\"model\"]\n",
        "                        print(f\"\\nModel: {current_model}\")\n",
        "                        current_strategy = None\n",
        "                        current_threshold = None\n",
        "\n",
        "                    # Print strategy header if it's a new strategy\n",
        "                    chunk_strategy = eval.get(\"chunk_strategy\", \"semantic\")\n",
        "                    if chunk_strategy != current_strategy:\n",
        "                        current_strategy = chunk_strategy\n",
        "                        print(f\"\\nChunking Strategy: {current_strategy}\")\n",
        "                        current_threshold = None\n",
        "\n",
        "                    # Print threshold header if it's semantic strategy and new threshold\n",
        "                    if chunk_strategy == \"semantic\":\n",
        "                        threshold = eval.get(\"threshold\")\n",
        "                        if threshold != current_threshold:\n",
        "                            current_threshold = threshold\n",
        "                            print(f\"\\nThreshold: {threshold}\")\n",
        "\n",
        "                    print(\"─\"*40)\n",
        "\n",
        "                    # Print evaluation details\n",
        "                    print(f\"\\nQuestion: {eval['question']}\")\n",
        "                    print(f\"Accuracy Score:          {eval['scores']['accuracy']:>3}/100\")\n",
        "                    print(f\"Conciseness Score:       {eval['scores']['conciseness']:>3}/100\")\n",
        "                    print(f\"Source Attribution:      {eval['scores']['source_attribution']:>3}/100\")\n",
        "                    print(f\"Reasonableness Score:    {eval['scores']['reasonableness']:>3}/100\")\n",
        "                    print(f\"Final Composite Score:   {eval['composite_score']:>3}/100\")\n",
        "                    print(\"\\nExplanation:\")\n",
        "                    print(textwrap.fill(eval['explanation'], width=80))\n",
        "\n",
        "                # Print summary section\n",
        "                print(\"\\n\" + \"=\"*80)\n",
        "                print(\"OVERALL ANALYSIS\")\n",
        "                print(\"=\"*80)\n",
        "\n",
        "                if \"summary\" in results:\n",
        "                    print(\"\\nPerformance Summary:\")\n",
        "                    print(\"-\"*80)\n",
        "                    print(textwrap.fill(results[\"summary\"][\"overall_performance\"], width=80))\n",
        "\n",
        "                    print(\"\\nOptimal Configuration:\")\n",
        "                    print(\"-\"*80)\n",
        "                    print(textwrap.fill(results[\"summary\"][\"optimal_permutation\"], width=80))\n",
        "\n",
        "                    print(\"\\nPerformance Analysis:\")\n",
        "                    print(\"-\"*80)\n",
        "                    print(textwrap.fill(results[\"summary\"][\"performance_analysis\"], width=80))\n",
        "\n",
        "                # Print metadata\n",
        "                if \"metadata\" in results:\n",
        "                    print(\"\\n\" + \"=\"*80)\n",
        "                    print(\"METADATA\")\n",
        "                    print(\"=\"*80)\n",
        "                    print(f\"Timestamp:           {results['metadata']['timestamp']}\")\n",
        "                    print(f\"Model Used:          {results['metadata']['model_used']}\")\n",
        "                    print(f\"Permutations:        {results['metadata']['num_permutations_evaluated']}\")\n",
        "                    print(f\"Questions Evaluated: {results['metadata']['num_questions_evaluated']}\")\n",
        "                    print(f\"Evaluation Status:   {results['metadata']['evaluation_status']}\")\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "                    print(\"Error parsing JSON results:\", e)\n",
        "        except KeyError as e:\n",
        "                    print(\"Error accessing result data:\", e)\n",
        "        except Exception as e:\n",
        "                    print(f\"Error displaying results: {str(e)}\")"
      ],
      "metadata": {
        "id": "1rAK93yw4qCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "koQ5ZObJC2ek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qdI5iaXYsun",
        "outputId": "a1cd0389-534c-491a-84ba-c9cca5442d76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing RAG piple and associated document chunks found. Preserving cached chunks...\n",
            "Using existing global RAG pipeline\n",
            "Starting experiment with configurations:\n",
            "Global temperature: 0.3\n",
            "Models: ['open-mistral-nemo']\n",
            "Thresholds: [85, 95]\n",
            "Chunk strategies: ['semantic', 'paragraph', 'header']\n",
            "Number of questions: 5\n",
            "\n",
            "Processing strategy: semantic\n",
            "Processing threshold: 85\n",
            "Creating new chunks for strategy semantic with threshold 85\n",
            "\n",
            "Testing model: open-mistral-nemo\n",
            "Processing question: What were cloud revenues in Q2 2024?\n",
            "Processing question: What were the main drivers of revenue growth in Q2?\n",
            "Processing question: How much did YouTube ad revenues grow in Q2 in APAC?\n",
            "Processing question: Can you summarize recent key antitrust matters?\n",
            "Processing question: What were YouTube ad revenues in Q2?\n",
            "Processing threshold: 95\n",
            "\n",
            "Testing model: open-mistral-nemo\n",
            "Processing question: What were cloud revenues in Q2 2024?\n",
            "Processing question: What were the main drivers of revenue growth in Q2?\n",
            "Processing question: How much did YouTube ad revenues grow in Q2 in APAC?\n",
            "Processing question: Can you summarize recent key antitrust matters?\n",
            "Processing question: What were YouTube ad revenues in Q2?\n",
            "\n",
            "Processing strategy: paragraph\n",
            "Creating new chunks for strategy paragraph with threshold 1000\n",
            "\n",
            "Testing model: open-mistral-nemo\n",
            "Processing question: What were cloud revenues in Q2 2024?\n",
            "Processing question: What were the main drivers of revenue growth in Q2?\n",
            "Processing question: How much did YouTube ad revenues grow in Q2 in APAC?\n",
            "Processing question: Can you summarize recent key antitrust matters?\n",
            "Processing question: What were YouTube ad revenues in Q2?\n",
            "\n",
            "Processing strategy: header\n",
            "Creating new chunks for strategy header with threshold 1000\n",
            "\n",
            "Testing model: open-mistral-nemo\n",
            "Processing question: What were cloud revenues in Q2 2024?\n",
            "Processing question: What were the main drivers of revenue growth in Q2?\n",
            "Processing question: How much did YouTube ad revenues grow in Q2 in APAC?\n",
            "Processing question: Can you summarize recent key antitrust matters?\n",
            "Processing question: What were YouTube ad revenues in Q2?\n",
            "\n",
            "Initializing GPT-4 evaluation...\n",
            "\n",
            "=== Starting evaluate_and_format ===\n",
            "Step 1: Running evaluation\n",
            "\n",
            "=== Starting Evaluation Process ===\n",
            "Number of unique questions to evaluate: 5\n",
            "Number of model/strategy combinations: 4\n",
            "\n",
            "--- Getting Baseline Answers ---\n",
            "Baseline response received: ```json\n",
            "{\n",
            "    \"How much did YouTube ad revenues grow in Q2 in APAC?\": \"Information not available in the document.\",\n",
            "    \"What were YouTube ad revenues in Q2?\": \"Information not available in the docume...\n",
            "Baseline answers received\n",
            "\n",
            "Evaluating model: open-mistral-nemo, strategy: header\n",
            "\n",
            "Evaluating open-mistral-nemo/header/What were cloud revenues in Q2 2024?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"header\",\n",
            "    \n",
            "    \"question\": \"What were cloud revenues in Q2 2024?\",\n",
            "    \"scores\": {\n",
            "        \"accuracy\": 90,\n",
            "        \"conciseness\": ...\n",
            "\n",
            "Evaluating open-mistral-nemo/header/What were the main drivers of revenue growth in Q2?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"header\",\n",
            "    \n",
            "    \"question\": \"What were the main drivers of revenue growth in Q2?\",\n",
            "    \"scores\": {\n",
            "        \"accuracy\": 95,\n",
            "        ...\n",
            "\n",
            "Evaluating open-mistral-nemo/header/How much did YouTube ad revenues grow in Q2 in APAC?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"header\",\n",
            "    \n",
            "    \"question\": \"How much did YouTube ad revenues grow in Q2 in APAC?\",\n",
            "    \"scores\": {\n",
            "        \"accuracy\": 40,\n",
            "       ...\n",
            "\n",
            "Evaluating open-mistral-nemo/header/Can you summarize recent key antitrust matters?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"header\",\n",
            "    \n",
            "    \"question\": \"Can you summarize recent key antitrust matters?\",\n",
            "    \"scores\": {\n",
            "        \"accuracy\": 85,\n",
            "        \"con...\n",
            "\n",
            "Evaluating open-mistral-nemo/header/What were YouTube ad revenues in Q2?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"header\",\n",
            "    \n",
            "    \"question\": \"What were YouTube ad revenues in Q2?\",\n",
            "    \"scores\": {\n",
            "        \"accuracy\": 100,\n",
            "        \"conciseness\":...\n",
            "\n",
            "Evaluating model: open-mistral-nemo, strategy: semantic, threshold: 95\n",
            "\n",
            "Evaluating open-mistral-nemo/semantic/95/What were cloud revenues in Q2 2024?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"semantic\",\n",
            "    \"threshold\": 95,\n",
            "    \"question\": \"What were cloud revenues in Q2 2024?\",\n",
            "    \"scores\": {\n",
            "        \"accuracy\": 100,\n",
            "    ...\n",
            "\n",
            "Evaluating open-mistral-nemo/semantic/95/What were the main drivers of revenue growth in Q2?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"semantic\",\n",
            "    \"threshold\": 95,\n",
            "    \"question\": \"What were the main drivers of revenue growth in Q2?\",\n",
            "    \"scores\": {\n",
            "        \"accur...\n",
            "\n",
            "Evaluating open-mistral-nemo/semantic/95/How much did YouTube ad revenues grow in Q2 in APAC?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"semantic\",\n",
            "    \"threshold\": 95,\n",
            "    \"question\": \"How much did YouTube ad revenues grow in Q2 in APAC?\",\n",
            "    \"scores\": {\n",
            "        \"accu...\n",
            "\n",
            "Evaluating open-mistral-nemo/semantic/95/Can you summarize recent key antitrust matters?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"semantic\",\n",
            "    \"threshold\": 95,\n",
            "    \"question\": \"Can you summarize recent key antitrust matters?\",\n",
            "    \"scores\": {\n",
            "        \"accuracy\"...\n",
            "\n",
            "Evaluating open-mistral-nemo/semantic/95/What were YouTube ad revenues in Q2?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"semantic\",\n",
            "    \"threshold\": 95,\n",
            "    \"question\": \"What were YouTube ad revenues in Q2?\",\n",
            "    \"scores\": {\n",
            "        \"accuracy\": 60,\n",
            "     ...\n",
            "\n",
            "Evaluating model: open-mistral-nemo, strategy: semantic, threshold: 85\n",
            "\n",
            "Evaluating open-mistral-nemo/semantic/85/What were cloud revenues in Q2 2024?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"semantic\",\n",
            "    \"threshold\": 85,\n",
            "    \"question\": \"What were cloud revenues in Q2 2024?\",\n",
            "    \"scores\": {\n",
            "        \"accuracy\": 90,\n",
            "     ...\n",
            "\n",
            "Evaluating open-mistral-nemo/semantic/85/What were the main drivers of revenue growth in Q2?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"semantic\",\n",
            "    \"threshold\": 85,\n",
            "    \"question\": \"What were the main drivers of revenue growth in Q2?\",\n",
            "    \"scores\": {\n",
            "        \"accur...\n",
            "\n",
            "Evaluating open-mistral-nemo/semantic/85/How much did YouTube ad revenues grow in Q2 in APAC?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"semantic\",\n",
            "    \"threshold\": 85,\n",
            "    \"question\": \"How much did YouTube ad revenues grow in Q2 in APAC?\",\n",
            "    \"scores\": {\n",
            "        \"accu...\n",
            "\n",
            "Evaluating open-mistral-nemo/semantic/85/Can you summarize recent key antitrust matters?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"semantic\",\n",
            "    \"threshold\": 85,\n",
            "    \"question\": \"Can you summarize recent key antitrust matters?\",\n",
            "    \"scores\": {\n",
            "        \"accuracy\"...\n",
            "\n",
            "Evaluating open-mistral-nemo/semantic/85/What were YouTube ad revenues in Q2?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"semantic\",\n",
            "    \"threshold\": 85,\n",
            "    \"question\": \"What were YouTube ad revenues in Q2?\",\n",
            "    \"scores\": {\n",
            "        \"accuracy\": 60,\n",
            "     ...\n",
            "\n",
            "Evaluating model: open-mistral-nemo, strategy: paragraph\n",
            "\n",
            "Evaluating open-mistral-nemo/paragraph/What were cloud revenues in Q2 2024?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"paragraph\",\n",
            "    \n",
            "    \"question\": \"What were cloud revenues in Q2 2024?\",\n",
            "    \"scores\": {\n",
            "        \"accuracy\": 95,\n",
            "        \"conciseness...\n",
            "\n",
            "Evaluating open-mistral-nemo/paragraph/What were the main drivers of revenue growth in Q2?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"paragraph\",\n",
            "    \n",
            "    \"question\": \"What were the main drivers of revenue growth in Q2?\",\n",
            "    \"scores\": {\n",
            "        \"accuracy\": 95,\n",
            "     ...\n",
            "\n",
            "Evaluating open-mistral-nemo/paragraph/How much did YouTube ad revenues grow in Q2 in APAC?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"paragraph\",\n",
            "    \n",
            "    \"question\": \"How much did YouTube ad revenues grow in Q2 in APAC?\",\n",
            "    \"scores\": {\n",
            "        \"accuracy\": 30,\n",
            "    ...\n",
            "\n",
            "Evaluating open-mistral-nemo/paragraph/Can you summarize recent key antitrust matters?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"paragraph\",\n",
            "    \n",
            "    \"question\": \"Can you summarize recent key antitrust matters?\",\n",
            "    \"scores\": {\n",
            "        \"accuracy\": 80,\n",
            "        \"...\n",
            "\n",
            "Evaluating open-mistral-nemo/paragraph/What were YouTube ad revenues in Q2?\n",
            "Raw response: ```json\n",
            "{\n",
            "    \"model\": \"open-mistral-nemo\",\n",
            "    \"chunk_strategy\": \"paragraph\",\n",
            "    \n",
            "    \"question\": \"What were YouTube ad revenues in Q2?\",\n",
            "    \"scores\": {\n",
            "        \"accuracy\": 90,\n",
            "        \"conciseness...\n",
            "\n",
            "Step 2: Running display_results\n",
            "\n",
            "================================================================================\n",
            "MODEL EVALUATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "DETAILED MODEL PERFORMANCE\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Model: open-mistral-nemo\n",
            "\n",
            "Chunking Strategy: header\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: Can you summarize recent key antitrust matters?\n",
            "Accuracy Score:           85/100\n",
            "Conciseness Score:        90/100\n",
            "Source Attribution:       95/100\n",
            "Reasonableness Score:     90/100\n",
            "Final Composite Score:    90/100\n",
            "\n",
            "Explanation:\n",
            "The response accurately summarizes recent key antitrust matters involving\n",
            "Google, including fines and lawsuits in both Europe and the U.S. The response is\n",
            "concise and does not contain unnecessary information, staying focused on the\n",
            "requested summary. It uses the provided sources effectively to support the\n",
            "details mentioned, such as the EC's fine and U.S. lawsuits. The answer is\n",
            "properly contextualized, offering specific examples and outcomes where\n",
            "available, while indicating that some decisions are still pending, which adds to\n",
            "the reasonableness. However, the accuracy score is slightly reduced as the\n",
            "baseline suggests information might not be completely current, yet the response\n",
            "claims specific dates for events in 2023 and expected outcomes in 2024, which\n",
            "could be speculative without current proof.\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: How much did YouTube ad revenues grow in Q2 in APAC?\n",
            "Accuracy Score:           40/100\n",
            "Conciseness Score:        80/100\n",
            "Source Attribution:       50/100\n",
            "Reasonableness Score:     30/100\n",
            "Final Composite Score:    50/100\n",
            "\n",
            "Explanation:\n",
            "The response states that YouTube ad revenues grew by $998 million in Q2 in APAC,\n",
            "but the source mentions this figure without specifying the APAC region. The\n",
            "source text only provides information about the overall growth in YouTube ad\n",
            "revenues for the specified periods, not broken down by region. Therefore, the\n",
            "accuracy score is low due to the incorrect attribution of the figure to APAC.\n",
            "Conciseness is higher because the response directly answers the question without\n",
            "extraneous information. However, source attribution is poor as the provided\n",
            "source does not support the regional breakdown claimed in the answer.\n",
            "Reasonableness is low because the response misinterprets the scope of the data,\n",
            "failing to contextualize it appropriately regarding the region specified in the\n",
            "question.\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: What were YouTube ad revenues in Q2?\n",
            "Accuracy Score:          100/100\n",
            "Conciseness Score:       100/100\n",
            "Source Attribution:      100/100\n",
            "Reasonableness Score:    100/100\n",
            "Final Composite Score:   100/100\n",
            "\n",
            "Explanation:\n",
            "The response accurately states that YouTube ad revenues in Q2 were $8,663\n",
            "million, which directly matches the information provided in the source. The\n",
            "answer is concise, providing only the necessary information without any\n",
            "additional details. The source attribution is appropriate as it directly\n",
            "references the data used for the answer. The response is reasonable as it is\n",
            "contextualized correctly, addressing the question directly without needing\n",
            "further context.\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: What were cloud revenues in Q2 2024?\n",
            "Accuracy Score:           90/100\n",
            "Conciseness Score:       100/100\n",
            "Source Attribution:        0/100\n",
            "Reasonableness Score:     80/100\n",
            "Final Composite Score:   67.5/100\n",
            "\n",
            "Explanation:\n",
            "The response 'I don't know.' is accurate in the sense that it admits a lack of\n",
            "specific information, which aligns with the baseline answer indicating the\n",
            "information is unavailable. Thus, it scores high in accuracy. The response is\n",
            "concise, providing a direct answer without unnecessary details. However, it\n",
            "scores 0 in source attribution because it does not reference any specific source\n",
            "text or evidence. In terms of reasonableness, the answer is somewhat reasonable\n",
            "as it acknowledges the lack of information, but it could have been more\n",
            "contextualized by indicating the absence of data in the available documents,\n",
            "which would have provided a clearer rationale for the lack of an answer, hence a\n",
            "score of 80.\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: What were the main drivers of revenue growth in Q2?\n",
            "Accuracy Score:           95/100\n",
            "Conciseness Score:        90/100\n",
            "Source Attribution:      100/100\n",
            "Reasonableness Score:     95/100\n",
            "Final Composite Score:    95/100\n",
            "\n",
            "Explanation:\n",
            "The response accurately identifies the main drivers of revenue growth in Q2 as\n",
            "an increase in Google Services revenues and Google Cloud revenues. It matches\n",
            "the information provided in the source text almost verbatim, thus earning a high\n",
            "accuracy score. The response is concise and directly answers the question\n",
            "without unnecessary information, although it could have slightly simplified the\n",
            "numbers for clarity, which is why it received a score of 90 for conciseness. The\n",
            "source attribution is perfect as the response directly reflects the source text\n",
            "provided. The reasonableness is also high as the answer is properly\n",
            "contextualized and aligns well with the question asked.\n",
            "\n",
            "Chunking Strategy: paragraph\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: Can you summarize recent key antitrust matters?\n",
            "Accuracy Score:           80/100\n",
            "Conciseness Score:        85/100\n",
            "Source Attribution:       75/100\n",
            "Reasonableness Score:     70/100\n",
            "Final Composite Score:   77.5/100\n",
            "\n",
            "Explanation:\n",
            "The response accurately describes several key antitrust matters involving\n",
            "Google, including the €2.4 billion fine in 2017 and ongoing investigations into\n",
            "various aspects of Google's business practices. However, there is a discrepancy\n",
            "with the December 2023 California jury verdict, as it appears speculative given\n",
            "the current date constraints. The response is concise, focusing on significant\n",
            "antitrust issues without excessive detail. Source attribution is moderately\n",
            "strong, drawing from multiple relevant sources, though the future date issue\n",
            "casts doubt on the reliability of the information. Reasonableness is slightly\n",
            "lower due to the inclusion of the December 2023 verdict, which affects the\n",
            "overall contextual accuracy.\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: How much did YouTube ad revenues grow in Q2 in APAC?\n",
            "Accuracy Score:           30/100\n",
            "Conciseness Score:        80/100\n",
            "Source Attribution:       50/100\n",
            "Reasonableness Score:     40/100\n",
            "Final Composite Score:    50/100\n",
            "\n",
            "Explanation:\n",
            "The response claims a specific growth of $998 million for YouTube ad revenues in\n",
            "Q2 in APAC, however, the provided source does not specify this geographic\n",
            "breakdown. The source only mentions the overall increase in revenues for YouTube\n",
            "ads during the specified periods but does not attribute this increase\n",
            "specifically to APAC. Hence, the accuracy suffers due to the unsupported\n",
            "geographic specification. The response is concise, directly answering the\n",
            "question without superfluous information. Source attribution is moderate because\n",
            "the presented source gives a figure but lacks the regional detail, which is\n",
            "crucial in this context. The reasonableness is also low because the response\n",
            "assumes a level of detail (APAC-specific growth) that is not found in the source\n",
            "material.\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: What were YouTube ad revenues in Q2?\n",
            "Accuracy Score:           90/100\n",
            "Conciseness Score:       100/100\n",
            "Source Attribution:       80/100\n",
            "Reasonableness Score:     85/100\n",
            "Final Composite Score:   88.75/100\n",
            "\n",
            "Explanation:\n",
            "The response accurately provides the YouTube ad revenues for Q2 as $8,663\n",
            "million, which matches the source text. It is concise and directly answers the\n",
            "question without unnecessary information. However, the source attribution score\n",
            "is lower because the source text provided is not clearly formatted or explained,\n",
            "making it difficult to fully verify the extracted information's context. Despite\n",
            "this, the answer remains reasonable and contextually appropriate.\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: What were cloud revenues in Q2 2024?\n",
            "Accuracy Score:           95/100\n",
            "Conciseness Score:       100/100\n",
            "Source Attribution:       90/100\n",
            "Reasonableness Score:     85/100\n",
            "Final Composite Score:   92.5/100\n",
            "\n",
            "Explanation:\n",
            "The response accurately states that Google Cloud revenues in Q2 2024 were\n",
            "$15,485 million, which matches the information provided in the source. It is\n",
            "concise, providing a direct answer without unnecessary details. Source\n",
            "attribution is slightly lower because while the source contains the revenue\n",
            "figure, the format in which it is presented ('Google Cloud  8,031  10,347\n",
            "15,485  19,921') is somewhat ambiguous and not explicitly linked to Q2 2024 in\n",
            "the response, requiring some inference. The reasonableness score is slightly\n",
            "lower because the response should ideally offer some context, such as explicitly\n",
            "stating that the figure is from Google Cloud, to ensure clarity and prevent\n",
            "misinterpretation.\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: What were the main drivers of revenue growth in Q2?\n",
            "Accuracy Score:           95/100\n",
            "Conciseness Score:        90/100\n",
            "Source Attribution:      100/100\n",
            "Reasonableness Score:     90/100\n",
            "Final Composite Score:   93.75/100\n",
            "\n",
            "Explanation:\n",
            "The response accurately identifies the main drivers of revenue growth in Q2 as\n",
            "increases in Google Services and Google Cloud revenues, matching the source\n",
            "text. It is concise and directly answers the question without unnecessary\n",
            "information. The source attribution is perfect, as the response directly\n",
            "corresponds to the provided source text. The reasonableness is high because the\n",
            "response is well-contextualized, although it could slightly benefit from more\n",
            "context about the overall revenue growth percentage mentioned in the source.\n",
            "\n",
            "Chunking Strategy: semantic\n",
            "\n",
            "Threshold: 85\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: Can you summarize recent key antitrust matters?\n",
            "Accuracy Score:           90/100\n",
            "Conciseness Score:        85/100\n",
            "Source Attribution:       95/100\n",
            "Reasonableness Score:     90/100\n",
            "Final Composite Score:    90/100\n",
            "\n",
            "Explanation:\n",
            "The response accurately summarizes the key antitrust matters and matches the\n",
            "provided sources. The events mentioned are correctly attributed to the\n",
            "respective investigations and legal actions. The response is concise, listing\n",
            "the events without unnecessary information, though a slightly more streamlined\n",
            "format could improve clarity. Source attribution is strong, with each point\n",
            "backed by a specific source. The response is reasonable and contextualized,\n",
            "giving a clear overview of the antitrust matters without deviating from the\n",
            "topic.\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: How much did YouTube ad revenues grow in Q2 in APAC?\n",
            "Accuracy Score:           60/100\n",
            "Conciseness Score:        80/100\n",
            "Source Attribution:       50/100\n",
            "Reasonableness Score:     50/100\n",
            "Final Composite Score:    60/100\n",
            "\n",
            "Explanation:\n",
            "The response claims a specific figure of $2.4 billion for the growth of YouTube\n",
            "ad revenues in Q2 2024 in APAC, but the source text does not specify the\n",
            "geographical region (APAC). The increase of $2.4 billion is mentioned for a six-\n",
            "month period, not specifically for Q2 alone, and it does not attribute this\n",
            "increase to the APAC region. Therefore, the accuracy is low as the response\n",
            "misinterprets the data. Conciseness is relatively high as the answer is direct,\n",
            "but it lacks correct context. Source attribution is low because the provided\n",
            "source does not directly support the claim made in the response. The\n",
            "reasonableness is also low as the response does not properly contextualize the\n",
            "data with respect to the question about Q2 growth specifically in APAC.\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: What were YouTube ad revenues in Q2?\n",
            "Accuracy Score:           60/100\n",
            "Conciseness Score:        90/100\n",
            "Source Attribution:       40/100\n",
            "Reasonableness Score:     50/100\n",
            "Final Composite Score:    60/100\n",
            "\n",
            "Explanation:\n",
            "The accuracy score is low because the exact figure provided ($8,663 million) is\n",
            "not directly supported by the evidence given in the source. The source only\n",
            "mentions an increase of $998 million, not the total revenue. Conciseness is high\n",
            "because the response is direct and does not include unnecessary information.\n",
            "Source attribution is low because the response does not correctly use the given\n",
            "source to substantiate the specific revenue figure stated. Reasonableness is\n",
            "moderate as the response attempts to provide a specific figure, but without\n",
            "proper justification or context from the source, the answer lacks full\n",
            "reliability.\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: What were cloud revenues in Q2 2024?\n",
            "Accuracy Score:           90/100\n",
            "Conciseness Score:        95/100\n",
            "Source Attribution:       80/100\n",
            "Reasonableness Score:     85/100\n",
            "Final Composite Score:   87.5/100\n",
            "\n",
            "Explanation:\n",
            "The response accurately reports Google Cloud revenues for Q2 2024 as $10,347\n",
            "million, which seems consistent with the provided source. However, the source\n",
            "attribution score is slightly lower because the source provided does not\n",
            "explicitly mention Q2, only a yearly comparison between 2023 and 2024, which\n",
            "could imply but does not clearly specify the quarterly revenue. The response is\n",
            "concise and provides a direct answer without additional unnecessary information.\n",
            "It is reasonable but lacks explicit context indicating it covers Q2\n",
            "specifically, relying on an assumption that the figure applies to Q2.\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: What were the main drivers of revenue growth in Q2?\n",
            "Accuracy Score:           90/100\n",
            "Conciseness Score:        95/100\n",
            "Source Attribution:       85/100\n",
            "Reasonableness Score:     90/100\n",
            "Final Composite Score:    90/100\n",
            "\n",
            "Explanation:\n",
            "The response accurately identifies the main driver of revenue growth in Q2, as\n",
            "indicated by the provided source. The answer is concise, focusing directly on\n",
            "the increase in subscription revenues and the growth in YouTube subscribers,\n",
            "without unnecessary information. The source attribution is relevant, as it\n",
            "directly supports the claim made in the response. The reasonableness score\n",
            "reflects that the answer is well contextualized, clearly linking the growth in\n",
            "subscribers to the overall revenue growth. The composite score is an average of\n",
            "the individual scores, reflecting a strong overall performance.\n",
            "\n",
            "Threshold: 95\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: Can you summarize recent key antitrust matters?\n",
            "Accuracy Score:           90/100\n",
            "Conciseness Score:        80/100\n",
            "Source Attribution:       85/100\n",
            "Reasonableness Score:     90/100\n",
            "Final Composite Score:   86.25/100\n",
            "\n",
            "Explanation:\n",
            "The response accurately reflects key antitrust matters concerning Google,\n",
            "mentioning fines and lawsuits in various regions, which aligns with the source\n",
            "material. However, it includes information from 2017 and 2018, which may not be\n",
            "considered 'recent' in 2023, slightly impacting the accuracy. The response is\n",
            "mostly concise, though it could be more direct by focusing solely on the most\n",
            "current matters. Source attribution is strong, with relevant details drawn from\n",
            "the provided source. The response is reasonable, with appropriate context, but\n",
            "the inclusion of older events lessens the focus on recent developments.\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: How much did YouTube ad revenues grow in Q2 in APAC?\n",
            "Accuracy Score:           50/100\n",
            "Conciseness Score:        90/100\n",
            "Source Attribution:       70/100\n",
            "Reasonableness Score:     50/100\n",
            "Final Composite Score:    65/100\n",
            "\n",
            "Explanation:\n",
            "The response states that YouTube ad revenues grew by $998 million in Q2 in APAC.\n",
            "However, the source provided indicates an overall increase of $998 million, but\n",
            "does not specify that this growth is isolated to APAC. Therefore, the accuracy\n",
            "score is low because the response may misrepresent the geographic specificity of\n",
            "the data. The response is concise, providing a direct answer without extraneous\n",
            "information. For source attribution, the response does attempt to use a source\n",
            "but misinterprets the data provided. The reasonableness score is low because the\n",
            "context does not support the claim that this revenue growth is specifically for\n",
            "APAC.\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: What were YouTube ad revenues in Q2?\n",
            "Accuracy Score:           60/100\n",
            "Conciseness Score:        90/100\n",
            "Source Attribution:       50/100\n",
            "Reasonableness Score:     70/100\n",
            "Final Composite Score:   67.5/100\n",
            "\n",
            "Explanation:\n",
            "The response states that YouTube ad revenues in Q2 were $8,663 million. However,\n",
            "the sources provided only mention the increase in revenues ($998 million from Q2\n",
            "2023 to Q2 2024) and do not provide the actual revenue figure for Q2. Therefore,\n",
            "the accuracy is moderate because the response doesn't directly match the\n",
            "baseline or the provided sources. Conciseness is high as the response is clear\n",
            "and direct. Source attribution is low due to lack of direct evidence for the\n",
            "figure quoted. Reasonableness is moderate because while the response is\n",
            "plausible, it lacks proper context and verification from the sources.\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: What were cloud revenues in Q2 2024?\n",
            "Accuracy Score:          100/100\n",
            "Conciseness Score:       100/100\n",
            "Source Attribution:      100/100\n",
            "Reasonableness Score:    100/100\n",
            "Final Composite Score:   100/100\n",
            "\n",
            "Explanation:\n",
            "The response accurately states that Google Cloud revenues in Q2 2024 were\n",
            "$10.347 billion, which matches the source provided. The answer is concise,\n",
            "providing the exact figure without unnecessary details. The source attribution\n",
            "is perfect as it directly cites the relevant part of the document. The answer is\n",
            "reasonable and appropriately contextualized, given that it directly responds to\n",
            "the question with verified data from the source.\n",
            "────────────────────────────────────────\n",
            "\n",
            "Question: What were the main drivers of revenue growth in Q2?\n",
            "Accuracy Score:           95/100\n",
            "Conciseness Score:        95/100\n",
            "Source Attribution:      100/100\n",
            "Reasonableness Score:     95/100\n",
            "Final Composite Score:   96.25/100\n",
            "\n",
            "Explanation:\n",
            "The response accurately identifies the main drivers of revenue growth in Q2 as\n",
            "an increase in Google Services and Google Cloud revenues, matching the\n",
            "information in the provided source. It is concise, providing only the necessary\n",
            "details without extraneous information. The source attribution is perfect as it\n",
            "directly supports the answer with a relevant excerpt. The reasonableness is high\n",
            "because the response is well-contextualized and clearly addresses the question\n",
            "asked.\n",
            "\n",
            "================================================================================\n",
            "OVERALL ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Performance Summary:\n",
            "--------------------------------------------------------------------------------\n",
            "Average composite score across all evaluations: 80.38/100\n",
            "\n",
            "Optimal Configuration:\n",
            "--------------------------------------------------------------------------------\n",
            "Error accessing result data: 'optimal_permutation'\n",
            "\n",
            "Step 3: Returning evaluation\n",
            "\n",
            "=== Starting Results Formatting ===\n",
            "Input evaluation_results keys: ['metadata', 'evaluations', 'summary']\n",
            "\n",
            "=== Results Formatting Complete ===\n",
            "\n",
            "Experiment and evaluation completed successfully\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    global _GLOBAL_RAG_PIPELINE\n",
        "    if _GLOBAL_RAG_PIPELINE is not None:\n",
        "        print(\"Existing RAG piple and associated document chunks found. Preserving cached chunks...\")\n",
        "\n",
        "    # Single temperature setting for all models\n",
        "    GLOBAL_TEMPERATURE = 0.3\n",
        "\n",
        "    config = MemoryOptimizedExperimentConfig(\n",
        "        models=MODEL_CONFIGS[\"models\"],\n",
        "        thresholds=MODEL_CONFIGS[\"thresholds\"],\n",
        "        questions=QUESTION_CONFIGS[\"questions\"],\n",
        "        temperature=GLOBAL_TEMPERATURE,\n",
        "        chunk_strategies=MODEL_CONFIGS[\"chunk_strategies\"]\n",
        "    )\n",
        "\n",
        "    print(\"Starting experiment with configurations:\")\n",
        "    print(f\"Global temperature: {GLOBAL_TEMPERATURE}\")\n",
        "    print(f\"Models: {[model['name'] for model in config.models]}\")\n",
        "    print(f\"Thresholds: {config.thresholds}\")\n",
        "    print(f\"Chunk strategies: {config.chunk_strategies}\")\n",
        "    print(f\"Number of questions: {len(config.questions)}\")\n",
        "\n",
        "    # Run the experiment\n",
        "    results = config.run_experiment()\n",
        "\n",
        "    # Get source document text from the global documents variable\n",
        "    source_doc = documents[0].text  # documents is loaded at the start of this script\n",
        "\n",
        "    # Initialize the evaluator\n",
        "    print(\"\\nInitializing GPT-4 evaluation...\")\n",
        "    openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "    evaluator = ExperimentEvaluator(openai_api_key)\n",
        "\n",
        "    # Run evaluation and get intermediate formatted results\n",
        "    evaluation = evaluator.evaluate_and_format(results, source_doc)\n",
        "\n",
        "    # Format and save final results\n",
        "    formatted_results, formatted_evaluation = evaluator.format_and_save_results(\n",
        "        results,\n",
        "        evaluation,\n",
        "        FILE_CONFIGS['save_directory']\n",
        "    )\n",
        "\n",
        "    print(\"\\nExperiment and evaluation completed successfully\")\n",
        "    return formatted_results, formatted_evaluation\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results, evaluation = main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vClfo1_AR5rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yxCYWFPZs--Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}