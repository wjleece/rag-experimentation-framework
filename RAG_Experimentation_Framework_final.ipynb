{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMy7GngmJ4ZSS2/aWH8BL5I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjleece/rag-experimentation-framework/blob/main/RAG_Experimentation_Framework_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you use this code, please cite:\n",
        "\n",
        "{\n",
        "  title = {RAG Experimentation Framework},\n",
        "\n",
        "  author = {Bill Leece},\n",
        "\n",
        "  year = {2024}\n",
        "}"
      ],
      "metadata": {
        "id": "wZ0kV_UtQn5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "_lHNBLR-92Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers --quiet\n",
        "#!pip install -U optimum --quiet\n",
        "!pip install -U accelerate  --quiet\n",
        "#!pip install -U bitsandbytes  --quiet\n",
        "!pip install -U torch --quiet\n",
        "!pip install -U sentencepiece --quiet\n",
        "!pip install -U llama-index --quiet\n",
        "!pip install -U llama-index-llms-mistralai --quiet\n",
        "!pip install -U llama-index-embeddings-mistralai --quiet\n",
        "!pip install -U llama-index-llms-langchain --quiet\n",
        "!pip install -U langchain --quiet\n",
        "!pip install -U langchain-community --quiet\n",
        "!pip install -U langchain-mistralai --quiet\n",
        "!pip install -U langchain_huggingface --quiet\n",
        "!pip install -U faiss-gpu --quiet"
      ],
      "metadata": {
        "id": "4g_Vs7wgZW-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29deb831-c701-4095-ee9d-5883af1102e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.0/189.0 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.6/254.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "import transformers\n",
        "import torch\n",
        "import gc\n",
        "import openai\n",
        "import json\n",
        "import tiktoken\n",
        "import textwrap\n",
        "import time\n",
        "from google.colab import drive, userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "from llama_index.core import SimpleDirectoryReader, Settings\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "import time\n",
        "from typing import List, Dict, Tuple\n",
        "from contextlib import contextmanager\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any"
      ],
      "metadata": {
        "id": "Ao7eaSfq-TKs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get('MISTRAL_API_KEY')\n",
        "api_key = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "YvGHY024-OXK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' #Use GPUs when possible"
      ],
      "metadata": {
        "id": "mxAHV7T_-Xlh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment Configurations"
      ],
      "metadata": {
        "id": "jkqEV8M_HUKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup configurations\n",
        "MODEL_CONFIGS = {\n",
        "    \"models\": [\n",
        "    #    {\n",
        "    #        \"name\": \"open-mixtral-8x7b\",\n",
        "    #        \"type\": \"mistral_api\",\n",
        "    #        \"tokenizer\": None,  # Not needed for API models\n",
        "    #    },\n",
        "         {\n",
        "            \"name\": \"open-mistral-nemo\",\n",
        "            \"type\": \"mistral_api\",\n",
        "            \"tokenizer\": None,  # Not needed for API models\n",
        "         },\n",
        "   #     {\n",
        "   #         \"name\": \"ministral-8b-latest\",\n",
        "   #         \"type\": \"mistral_api\",\n",
        "   #         \"tokenizer\": None,  # Not needed for API models\n",
        "   #     },\n",
        "   #   {\n",
        "   #         \"name\": \"wjleece/quantized-mistral-7b\",\n",
        "   #         \"type\": \"huggingface_quantized\",\n",
        "   #         \"tokenizer\": \"mistralai/Mixtral-8x7B-v0.1\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "   #          \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "   #             \"load_in_4bit\": True,\n",
        "   #             \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "   #             \"bnb_4bit_quant_type\": \"nf4\",\n",
        "   #             \"bnb_4bit_use_double_quant\": False\n",
        "   #         }\n",
        "   #     },\n",
        "   #   {\n",
        "   #           \"name\": \"wjleece/quantized-mistral-nemo-12b\",\n",
        "   #           \"type\": \"huggingface_quantized\",\n",
        "   #           \"tokenizer\": \"mistralai/Mistral-Nemo-Instruct-2407\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "   #           \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "   #               \"load_in_4bit\": True,\n",
        "   #               \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "   #               \"bnb_4bit_quant_type\": \"nf4\",\n",
        "   #               \"bnb_4bit_use_double_quant\": False\n",
        "   #          }\n",
        "   #       },\n",
        "     #  {\n",
        "     #         \"name\": \"wjleece/quantized-mistral-8b\",\n",
        "     #         \"type\": \"huggingface_quantized\",\n",
        "     #         \"tokenizer\": \"mistralai/Ministral-8B-Instruct-2410\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "     #         \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "     #             \"load_in_4bit\": True,\n",
        "     #             \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "     #             \"bnb_4bit_quant_type\": \"nf4\",\n",
        "     #             \"bnb_4bit_use_double_quant\": False\n",
        "     #         }\n",
        "     #     }\n",
        "       ]\n",
        "}\n",
        "\n",
        "\n",
        "CHUNKING_CONFIGS = {\n",
        "    \"strategies\": [\"semantic\", \"paragraph\", \"header\"], #results will be saved in this order, with thesholds applicable for semantic only (if it is included)\n",
        "    \"thresholds\": [85, 95], #semantic threshold, only applicable for semantic chunking\n",
        "    \"max_chunk_size\": 2048, #only used in paragraph and header chunking\n",
        "    \"chunk_overlap\": 100, #only used in paragraph and header chunking\n",
        "    \"min_chunk_size\": 35 #if a chunk is only 35 characters - about 5 words - just ignore it\n",
        "}\n",
        "\n",
        "QUESTION_CONFIGS = {\n",
        "    \"questions\": [\n",
        "        \"What were cloud revenues in Q2 2024?\",\n",
        "       # \"What were the main drivers of revenue growth in Q2?\",\n",
        "       # \"How much did YouTube ad revenues grow in Q2 in APAC?\",\n",
        "       # \"Can you summarize recent key antitrust matters?\",\n",
        "       # \"Compare the revenue growth across all geographic regions and explain the main factors for each region.\",\n",
        "       # \"Summarize all mentioned risk factors related to international operations.\",\n",
        "       # \"What were the major changes in operating expenses across all categories and their stated reasons?\"\n",
        "    ] #These quetsions should relate to the RAG document --> these are your 'business use cases'\n",
        "}\n",
        "\n",
        "FILE_CONFIGS = {\n",
        "    \"save_directory\": '/content/drive/My Drive/AI/Model_Analysis'\n",
        "}"
      ],
      "metadata": {
        "id": "YDjgk_JhHWkj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load RAG Document"
      ],
      "metadata": {
        "id": "e_wxgOGc95sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "documents = SimpleDirectoryReader(input_files=[\"/content/drive/My Drive/AI/Datasets/Google-10-q/goog-10-q-q2-2024.pdf\"]).load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS4Lemk09v9Y",
        "outputId": "af779fa4-3e89-487f-e7a1-33d793770e64"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RAG Pipeline Class"
      ],
      "metadata": {
        "id": "MgLxma5M-bZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global singleton instance\n",
        "_GLOBAL_RAG_PIPELINE = None\n",
        "\n",
        "class RAGPipeline:\n",
        "    def __init__(self):\n",
        "        self.chunk_cache = {}\n",
        "        self.embedding_cache = {}\n",
        "        self.embedding_model = None\n",
        "\n",
        "    @classmethod\n",
        "    def get_instance(cls):\n",
        "        \"\"\"Get or create singleton instance\"\"\"\n",
        "        global _GLOBAL_RAG_PIPELINE\n",
        "        if _GLOBAL_RAG_PIPELINE is None:\n",
        "            _GLOBAL_RAG_PIPELINE = cls()\n",
        "        return _GLOBAL_RAG_PIPELINE\n",
        "\n",
        "\n",
        "    def initialize_embedding_model(self):\n",
        "        \"\"\"Initialize the embedding model if not already initialized\"\"\"\n",
        "        if self.embedding_model is None:\n",
        "            mistral_api_key = userdata.get('MISTRAL_API_KEY')\n",
        "            self.embedding_model = MistralAIEmbedding(\n",
        "                model_name=\"mistral-embed\",\n",
        "                api_key=mistral_api_key\n",
        "            )\n",
        "        return self.embedding_model\n",
        "\n",
        "    def convert_to_markdown_headers(self, text):\n",
        "        \"\"\"Convert document section titles to markdown headers\"\"\"\n",
        "        import re\n",
        "\n",
        "        patterns = [\n",
        "            (r'^(?:ITEM|Section)\\s+\\d+[.:]\\s*(.+)$', '# '),\n",
        "            (r'^\\d+\\.\\d+\\s+(.+)$', '## '),\n",
        "            (r'^\\([a-z]\\)\\s+(.+)$', '### ')\n",
        "        ]\n",
        "\n",
        "        lines = text.split('\\n')\n",
        "        markdown_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            converted = False\n",
        "\n",
        "            for pattern, header_mark in patterns:\n",
        "                if re.match(pattern, line, re.IGNORECASE):\n",
        "                    markdown_lines.append(f\"{header_mark}{line}\")\n",
        "                    converted = True\n",
        "                    break\n",
        "\n",
        "            if not converted:\n",
        "                markdown_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(markdown_lines)\n",
        "\n",
        "\n",
        "    def create_chunks(self, documents: List, threshold: int, chunk_strategy: str = \"semantic\") -> Dict:\n",
        "        \"\"\"Create or retrieve chunks based on specified strategy\"\"\"\n",
        "\n",
        "        MAX_CHUNK_SIZE = CHUNKING_CONFIGS['max_chunk_size']\n",
        "        CHUNK_OVERLAP = CHUNKING_CONFIGS['chunk_overlap']\n",
        "        MIN_CHUNK_SIZE = CHUNKING_CONFIGS['min_chunk_size']\n",
        "\n",
        "        if chunk_strategy == \"semantic\":\n",
        "            cache_key = f\"{chunk_strategy}_{threshold}\"\n",
        "        else:\n",
        "            cache_key = f\"{chunk_strategy}_{MAX_CHUNK_SIZE}\"\n",
        "\n",
        "        if cache_key not in self.chunk_cache:\n",
        "            print(\"\\n=== CHUNK CREATION DEBUG ===\")\n",
        "            print(f\"Strategy: {chunk_strategy}\")\n",
        "            print(f\"Cache key: {cache_key}\")\n",
        "            if chunk_strategy == \"semantic\":\n",
        "                print(f\"Using semantic threshold: {threshold}\")\n",
        "            else:\n",
        "                print(f\"Using max chunk size: {MAX_CHUNK_SIZE} characters with {CHUNK_OVERLAP} character overlap\")\n",
        "\n",
        "            if len(self.chunk_cache) > 2:\n",
        "                oldest_key = min(self.chunk_cache.keys())\n",
        "                if oldest_key != cache_key:\n",
        "                    del self.chunk_cache[oldest_key]\n",
        "                    if oldest_key in self.embedding_cache:\n",
        "                        del self.embedding_cache[oldest_key]\n",
        "                    gc.collect()\n",
        "\n",
        "            if chunk_strategy == \"semantic\":\n",
        "                if self.embedding_model is None:\n",
        "                    self.initialize_embedding_model()\n",
        "\n",
        "                splitter = SemanticSplitterNodeParser(\n",
        "                    buffer_size=1,\n",
        "                    breakpoint_percentile_threshold=threshold,\n",
        "                    embed_model=self.embedding_model\n",
        "                )\n",
        "                nodes = splitter.get_nodes_from_documents(documents)\n",
        "                texts = [node.text for node in nodes]\n",
        "\n",
        "            elif chunk_strategy == \"paragraph\":\n",
        "                text_splitter = RecursiveCharacterTextSplitter(\n",
        "                    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "                    chunk_size=MAX_CHUNK_SIZE,\n",
        "                    chunk_overlap=CHUNK_OVERLAP,\n",
        "                    length_function=len\n",
        "                )\n",
        "                texts = []\n",
        "                for doc in documents:\n",
        "                    chunks = text_splitter.split_text(doc.text)\n",
        "                    texts.extend(chunks)\n",
        "\n",
        "            elif chunk_strategy == \"header\":\n",
        "                headers_to_split_on = [\n",
        "                    (\"#\", \"Header 1\"),\n",
        "                    (\"##\", \"Header 2\"),\n",
        "                    (\"###\", \"Header 3\"),\n",
        "                ]\n",
        "\n",
        "                header_splitter = MarkdownHeaderTextSplitter(\n",
        "                    headers_to_split_on=headers_to_split_on\n",
        "                )\n",
        "\n",
        "                text_splitter = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size=MAX_CHUNK_SIZE,\n",
        "                    chunk_overlap=CHUNK_OVERLAP,\n",
        "                    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "                )\n",
        "\n",
        "                texts = []\n",
        "                for doc in documents:\n",
        "                    md_text = self.convert_to_markdown_headers(doc.text)\n",
        "                    header_splits = header_splitter.split_text(md_text)\n",
        "\n",
        "                    for split in header_splits:\n",
        "                        if len(split.page_content) > MAX_CHUNK_SIZE:\n",
        "                            chunks = text_splitter.split_text(split.page_content)\n",
        "                            texts.extend(chunks)\n",
        "                        else:\n",
        "                            texts.append(split.page_content)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown chunk strategy: {chunk_strategy}\")\n",
        "\n",
        "            # Filter out chunks that are too small\n",
        "            texts = [t for t in texts if len(t.strip()) >= MIN_CHUNK_SIZE]\n",
        "\n",
        "            if texts:\n",
        "                chunk_lengths = [len(t) for t in texts]\n",
        "                chunk_stats = {\n",
        "                    'num_chunks': len(texts),\n",
        "                    'avg_chunk_size': sum(chunk_lengths)/len(texts),\n",
        "                    'min_chunk_size': min(chunk_lengths),\n",
        "                    'max_chunk_size': max(chunk_lengths)\n",
        "                }\n",
        "\n",
        "                self.chunk_cache[cache_key] = {\n",
        "                    'texts': texts,\n",
        "                    'strategy': chunk_strategy,\n",
        "                    'chunk_stats': chunk_stats\n",
        "                }\n",
        "\n",
        "        return self.chunk_cache[cache_key]\n",
        "\n",
        "    def run_cosine_search(self, query: str, threshold: int, chunk_strategy: str = \"semantic\", k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Run cosine similarity search with memory optimization and debugging\"\"\"\n",
        "        print(\"\\n=== COSINE SEARCH DEBUG ===\")\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Strategy: {chunk_strategy}\")\n",
        "        print(f\"Threshold: {threshold}\")\n",
        "        print(f\"Requested k: {k}\")\n",
        "\n",
        "        if self.embedding_model is None:\n",
        "            self.initialize_embedding_model()\n",
        "\n",
        "        FIXED_CHUNK_SIZE = 1024\n",
        "\n",
        "        if chunk_strategy == \"semantic\":\n",
        "            cache_key = f\"{chunk_strategy}_{threshold}\"\n",
        "        else:\n",
        "            cache_key = f\"{chunk_strategy}_{FIXED_CHUNK_SIZE}\"\n",
        "\n",
        "        print(f\"Cache key: {cache_key}\")\n",
        "\n",
        "        if cache_key not in self.embedding_cache:\n",
        "            try:\n",
        "                texts = self.chunk_cache[cache_key]['texts']\n",
        "                print(f\"Creating embeddings for {len(texts)} chunks\")\n",
        "            except KeyError:\n",
        "                print(f\"Warning: No chunks found for strategy {chunk_strategy}\")\n",
        "                return []\n",
        "\n",
        "            batch_size = 32\n",
        "            embeddings = []\n",
        "\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch_texts = texts[i:i + batch_size]\n",
        "                batch_embeddings = [self.embedding_model.get_text_embedding(text)\n",
        "                                  for text in batch_texts]\n",
        "                embeddings.extend(batch_embeddings)\n",
        "\n",
        "                if i % (batch_size * 4) == 0:\n",
        "                    gc.collect()\n",
        "\n",
        "            embeddings_array = np.array(embeddings).astype('float32')\n",
        "            normalized_embeddings = embeddings_array / np.linalg.norm(embeddings_array, axis=1)[:, np.newaxis]\n",
        "\n",
        "            dimension = embeddings_array.shape[1]\n",
        "            cosine_index = faiss.IndexFlatIP(dimension)\n",
        "            cosine_index.add(normalized_embeddings)\n",
        "\n",
        "            self.embedding_cache[cache_key] = {\n",
        "                'embeddings': embeddings_array,\n",
        "                'cosine_index': cosine_index\n",
        "            }\n",
        "\n",
        "        query_vector = self.embedding_model.get_text_embedding(query)\n",
        "        query_vector = np.array([query_vector]).astype('float32')\n",
        "        query_normalized = query_vector / np.linalg.norm(query_vector)\n",
        "\n",
        "        distances, indices = self.embedding_cache[cache_key]['cosine_index'].search(\n",
        "            query_normalized.reshape(1, -1).astype('float32'), k\n",
        "        )\n",
        "\n",
        "        return [\n",
        "            {\n",
        "                'text': self.chunk_cache[cache_key]['texts'][idx],\n",
        "                'distance': float(score),\n",
        "                'strategy': chunk_strategy\n",
        "            }\n",
        "            for score, idx in zip(distances[0], indices[0])\n",
        "        ]\n",
        "\n",
        "    def generate_response(self, query: str, context_rag: list, model: Dict) -> dict:\n",
        "        \"\"\"Generate response using provided context\"\"\"\n",
        "        try:\n",
        "            context_texts = [doc['text'] for doc in context_rag]\n",
        "            if not context_texts:\n",
        "                return {\"response_text\": \"No relevant context found.\", \"sources\": [], \"strategy\": context_rag[0]['strategy'] if context_rag else None}\n",
        "\n",
        "            context = \"\\n\\n\".join(context_texts)\n",
        "\n",
        "            prompt = PromptTemplate(template=\"\"\"\n",
        "            Instructions:\n",
        "\n",
        "            You are a helpful assistant who answers questions from context that has been provided to you.\n",
        "            Given the context information, provide a direct and concise answer to the question: {query}\n",
        "\n",
        "            Focus only on information present in the context. If you don't know the answer, say \"I don't know.\"\n",
        "            You must format your response as a JSON string object, starting with the word \"LLM_Response:\"\n",
        "\n",
        "            Your answer to {query} will be a JSON string object that starts with \"LLM_Response:\" as shown below:\n",
        "\n",
        "            LLM_Response:\n",
        "            {{\n",
        "                \"response_text\": \"Your detailed answer here\",\n",
        "                \"sources\": [\n",
        "                    \"Copy and paste here the exact text segments from the context that you used to generate your answer. Include all relevant segments, verbatim.\"\n",
        "                ]\n",
        "            }}\n",
        "\n",
        "            Important: In your response, the \"sources\" field must contain the exact text passages from the provided context that you used to formulate your answer. Copy these passages word-for-word.\n",
        "\n",
        "            Do not include a hypothetical example in your answer, only include your final answer after \"LLM_Response:\"\n",
        "\n",
        "            The context information that you will use for your answer is below:\n",
        "\n",
        "            ---------------\n",
        "            {context}\n",
        "            ---------------\n",
        "            \"\"\")\n",
        "\n",
        "            model_type = model['type']\n",
        "            llm = model['llm']\n",
        "\n",
        "            chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "            response = chain.invoke({\n",
        "                \"query\": query,\n",
        "                \"context\": context\n",
        "               })\n",
        "\n",
        "            response_text = response.split(\"LLM_Response:\")[-1].strip()\n",
        "\n",
        "            try:\n",
        "                if '{' in response_text and '}' in response_text:\n",
        "                    json_str = response_text[response_text.find('{'):response_text.rfind('}')+1]\n",
        "                    parsed_response = json.loads(json_str)\n",
        "                    return {\n",
        "                        \"response_text\": parsed_response.get(\"response_text\", response_text),\n",
        "                        \"sources\": parsed_response.get(\"sources\", []),\n",
        "                        \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                    }\n",
        "                else:\n",
        "                    return {\n",
        "                        \"response_text\": response_text,\n",
        "                        \"sources\": [],\n",
        "                        \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                    }\n",
        "            except json.JSONDecodeError:\n",
        "                return {\n",
        "                    \"response_text\": response_text,\n",
        "                    \"sources\": [],\n",
        "                    \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {str(e)}\")\n",
        "            return {\"response_text\": \"An error occurred while generating the response.\", \"sources\": []}"
      ],
      "metadata": {
        "id": "YY5rnivk-bAh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ModelConfig Class"
      ],
      "metadata": {
        "id": "ljqi1Qg8j9F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelConfig:\n",
        "    \"\"\"Handles model configuration and management\"\"\"\n",
        "    def __init__(self,\n",
        "                 models: List[Dict],\n",
        "                 temperature: float = 0.3):\n",
        "        self.models = models\n",
        "        self.temperature = temperature\n",
        "        self.current_model = None\n",
        "        self.current_model_name = None\n",
        "\n",
        "\n",
        "    @contextmanager\n",
        "    def load_model(self, model_config: Dict):\n",
        "        \"\"\"Context manager for lazy loading and proper cleanup of models\"\"\"\n",
        "        try:\n",
        "            model_name = model_config[\"name\"]\n",
        "            model_type = model_config[\"type\"]\n",
        "\n",
        "            # Clear any existing model\n",
        "            self.cleanup_current_model()\n",
        "\n",
        "            if model_type == \"mistral_api\":\n",
        "                mistral_api_key = userdata.get('MISTRAL_API_KEY')\n",
        "                self.current_model = {\n",
        "                    'llm': ChatMistralAI(\n",
        "                        model=model_name,\n",
        "                        temperature=self.temperature,\n",
        "                        api_key=mistral_api_key\n",
        "                    ),\n",
        "                    'type': 'mistral_api'\n",
        "                }\n",
        "            else:  # huggingface_quantized\n",
        "                print(f\"Loading quantized model: {model_name}\")\n",
        "\n",
        "                # Empty CUDA cache before loading new model\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                tokenizer = AutoTokenizer.from_pretrained(\n",
        "                    pretrained_model_name_or_path=model_config[\"tokenizer\"],\n",
        "                    trust_remote_code=True,\n",
        "                    use_fast=True,\n",
        "                    padding_side=\"left\"\n",
        "                )\n",
        "\n",
        "                model = AutoModelForCausalLM.from_pretrained(\n",
        "                    pretrained_model_name_or_path=model_name,\n",
        "                    device_map=\"auto\",\n",
        "                    trust_remote_code=True,\n",
        "                    torch_dtype=torch.float16,\n",
        "                    use_cache=True,\n",
        "                    low_cpu_mem_usage=True,\n",
        "                )\n",
        "\n",
        "                pipe = pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_new_tokens=512,\n",
        "                    temperature=self.temperature,\n",
        "                    top_p=0.95,\n",
        "                    top_k=50,\n",
        "                    do_sample=True,\n",
        "                    device_map=\"auto\"\n",
        "                )\n",
        "\n",
        "                self.current_model = {\n",
        "                    'llm': HuggingFacePipeline(pipeline=pipe),\n",
        "                    'type': 'huggingface_quantized',\n",
        "                    'model': model,  # Keep reference for cleanup\n",
        "                    'pipe': pipe     # Keep reference for cleanup\n",
        "                }\n",
        "\n",
        "            self.current_model_name = model_name\n",
        "            yield self.current_model\n",
        "\n",
        "        finally:\n",
        "            # Cleanup will happen in cleanup_current_model()\n",
        "            pass\n",
        "\n",
        "    def cleanup_current_model(self):\n",
        "        \"\"\"Clean up the current model and free memory\"\"\"\n",
        "        if self.current_model is not None:\n",
        "            if self.current_model['type'] == 'huggingface_quantized':\n",
        "                # Delete model components explicitly\n",
        "                del self.current_model['llm']\n",
        "                del self.current_model['model']\n",
        "                del self.current_model['pipe']\n",
        "\n",
        "                # Clear CUDA cache\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                # Run garbage collection\n",
        "                gc.collect()\n",
        "\n",
        "            self.current_model = None\n",
        "            self.current_model_name = None"
      ],
      "metadata": {
        "id": "tCzG7OE0IiDT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ExperimentRunner Class"
      ],
      "metadata": {
        "id": "05gTul4pIW6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperimentRunner:\n",
        "    \"\"\"Handles experiment execution\"\"\"\n",
        "    def __init__(self,\n",
        "                 model_config: ModelConfig,\n",
        "                 thresholds: List[int],\n",
        "                 questions: List[str],\n",
        "                 chunk_strategies: List[str],\n",
        "                 rag_pipeline: RAGPipeline = None):\n",
        "        self.model_config = model_config\n",
        "        self.thresholds = thresholds\n",
        "        self.questions = questions\n",
        "        self.chunk_strategies = chunk_strategies\n",
        "\n",
        "        # Use existing RAG pipeline or create new one\n",
        "        global _GLOBAL_RAG_PIPELINE\n",
        "        if rag_pipeline:\n",
        "            self.rag_pipeline = rag_pipeline\n",
        "        elif _GLOBAL_RAG_PIPELINE:\n",
        "            self.rag_pipeline = _GLOBAL_RAG_PIPELINE\n",
        "        else:\n",
        "            print(\"Initializing new RAG pipeline\")\n",
        "            _GLOBAL_RAG_PIPELINE = RAGPipeline()\n",
        "            self.rag_pipeline = _GLOBAL_RAG_PIPELINE\n",
        "\n",
        "    def run_experiments(self) -> Dict:\n",
        "        results = {\n",
        "            \"metadata\": {\n",
        "                \"timestamp\": time.strftime(\"%Y%m%d-%H%M%S\"),\n",
        "                \"models_tested\": [model[\"name\"] for model in self.model_config.models],\n",
        "                \"thresholds_tested\": self.thresholds,\n",
        "                \"chunk_strategies_tested\": self.chunk_strategies,\n",
        "                \"temperature\": self.model_config.temperature\n",
        "            },\n",
        "            \"results\": []\n",
        "        }\n",
        "\n",
        "        for strategy in self.chunk_strategies:\n",
        "            if strategy == \"semantic\":\n",
        "                thresholds_to_test = self.thresholds\n",
        "            else:\n",
        "                thresholds_to_test = [None]\n",
        "\n",
        "            for threshold in thresholds_to_test:\n",
        "                actual_threshold = threshold if strategy == \"semantic\" else 0\n",
        "\n",
        "                # Get chunks and their stats\n",
        "                chunks_data = self.rag_pipeline.create_chunks(\n",
        "                    documents,\n",
        "                    threshold=actual_threshold,\n",
        "                    chunk_strategy=strategy\n",
        "                )\n",
        "\n",
        "                # Store chunk stats in a format that will persist through the pipeline\n",
        "                chunk_stats = {\n",
        "                    \"strategy\": strategy,\n",
        "                    \"threshold\": threshold,\n",
        "                    \"stats\": chunks_data[\"chunk_stats\"]\n",
        "                }\n",
        "\n",
        "                for model_config in self.model_config.models:\n",
        "                    model_name = model_config[\"name\"]\n",
        "                    print(f\"\\nTesting model: {model_name}\")\n",
        "\n",
        "                    with self.model_config.load_model(model_config) as model:\n",
        "                        for question in self.questions:\n",
        "                            print(f\"Processing question: {question}\")\n",
        "\n",
        "                            context = self.rag_pipeline.run_cosine_search(\n",
        "                                query=question,\n",
        "                                threshold=threshold,\n",
        "                                chunk_strategy=strategy\n",
        "                            )\n",
        "\n",
        "                            answer = self.rag_pipeline.generate_response(\n",
        "                                query=question,\n",
        "                                context_rag=context,\n",
        "                                model=model\n",
        "                            )\n",
        "\n",
        "                            # Include chunk stats in results\n",
        "                            results[\"results\"].append({\n",
        "                                \"model\": model_name,\n",
        "                                \"threshold\": threshold if strategy == \"semantic\" else None,\n",
        "                                \"chunk_strategy\": strategy,\n",
        "                                \"question\": question,\n",
        "                                \"response\": answer,\n",
        "                                \"chunk_stats\": chunk_stats[\"stats\"]  # Include the stats here\n",
        "                            })\n",
        "\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "8hFyd9G1kC8M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluator Class"
      ],
      "metadata": {
        "id": "EpjD-Qz54mfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperimentEvaluator:\n",
        "    \"\"\"Handles pure evaluation logic\"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.client = openai.OpenAI(api_key=api_key)\n",
        "        self.encoder = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "\n",
        "    def _get_baseline_answers(self, questions: List[str], source_docs: List) -> Dict[str, str]:\n",
        "        \"\"\"Get GPT-4o's own answers to the questions as baseline\"\"\"\n",
        "        print(\"\\n=== DEBUG: _get_baseline_answers ===\")\n",
        "        print(f\"Questions received: {questions}\")\n",
        "        print(f\"Number of document parts: {len(source_docs)}\")\n",
        "\n",
        "        # Concatenate all document parts\n",
        "        full_document = \"\\n\\n\".join([doc.text for doc in source_docs])\n",
        "        print(f\"\\nFull document length: {len(full_document)} characters\")\n",
        "\n",
        "        # Print sample from document\n",
        "        print(\"\\nSampling from document:\")\n",
        "        print(\"Start:\", full_document[:200], \"...\")\n",
        "        print(\"Middle:\", full_document[len(full_document)//2:len(full_document)//2 + 200], \"...\")\n",
        "        print(\"End:\", full_document[-200:], \"...\")\n",
        "\n",
        "        baseline_prompt = f\"\"\"Source Document:\n",
        "        {full_document}\n",
        "\n",
        "        Using ONLY the information from the source document above, answer these questions.\n",
        "        - If the exact information is found, provide it with specific numbers\n",
        "        - If information is not found, explicitly state that\n",
        "        - If there are metrics, make sure to include appropriate units\n",
        "\n",
        "        Format your response as a valid JSON object with questions as keys and answers as values.\n",
        "        Keep answers concise and factual.\n",
        "\n",
        "        Questions to answer:\n",
        "        {json.dumps(questions, indent=2)}\"\"\"\n",
        "\n",
        "        try:\n",
        "            print(\"\\n--- Getting Baseline Answers ---\")\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides JSON-formatted answers based on source documents.\"},\n",
        "                    {\"role\": \"user\", \"content\": baseline_prompt}\n",
        "                ],\n",
        "                temperature=0.1\n",
        "            )\n",
        "\n",
        "            content = response.choices[0].message.content\n",
        "            print(\"\\nRaw GPT-4 Response:\")\n",
        "            print(content)\n",
        "\n",
        "            if '{' in content and '}' in content:\n",
        "                json_str = content[content.find('{'):content.rfind('}')+1]\n",
        "                baseline_answers = json.loads(json_str)\n",
        "                print(\"\\nParsed Baseline Answers:\")\n",
        "                print(baseline_answers)\n",
        "                return baseline_answers\n",
        "            print(\"\\nWarning: No JSON structure found in response\")\n",
        "            return {\"error\": \"No JSON structure found\", \"questions\": questions}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError in _get_baseline_answers: {str(e)}\")\n",
        "            return {\"error\": str(e), \"questions\": questions}\n",
        "\n",
        "    def evaluate_experiments(self, experiment_results: Dict, *, source_docs: List) -> Dict:  # Updated signature\n",
        "        \"\"\"Core evaluation logic\"\"\"\n",
        "        try:\n",
        "            print(\"\\n=== DEBUG: evaluate_experiments ===\")\n",
        "            print(\"Getting questions...\")\n",
        "            questions = list(set(result[\"question\"] for result in experiment_results[\"results\"]))\n",
        "            print(f\"Questions extracted: {questions}\")\n",
        "\n",
        "            print(\"\\nGetting baseline answers...\")\n",
        "            baseline_answers = self._get_baseline_answers(questions, source_docs)  # Pass source_docs\n",
        "            print(f\"Baseline answers received: {baseline_answers}\")\n",
        "\n",
        "            model_strategy_combinations = set(\n",
        "                (result[\"model\"],\n",
        "                result[\"chunk_strategy\"],\n",
        "                result[\"threshold\"] if result[\"chunk_strategy\"] == \"semantic\" else None)\n",
        "                for result in experiment_results[\"results\"]\n",
        "            )\n",
        "\n",
        "            all_evaluations = []\n",
        "\n",
        "            for model, strategy, threshold in model_strategy_combinations:\n",
        "                relevant_results = [r for r in experiment_results[\"results\"]\n",
        "                                  if r[\"model\"] == model and\n",
        "                                     r[\"chunk_strategy\"] == strategy and\n",
        "                                     (r[\"threshold\"] == threshold if strategy == \"semantic\" else True)]\n",
        "\n",
        "                for result in relevant_results:\n",
        "                    print(f\"\\nEvaluating response for: {result['question']}\")\n",
        "                    baseline = baseline_answers.get(result[\"question\"], \"No baseline available\")\n",
        "                    print(f\"Using baseline answer: {baseline}\")\n",
        "\n",
        "                    evaluation = self._evaluate_single_response(result, baseline)\n",
        "                    all_evaluations.append(evaluation)\n",
        "\n",
        "            return {\n",
        "                \"metadata\": {\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"model_used\": \"gpt-4o\",\n",
        "                    \"num_combinations_evaluated\": len(model_strategy_combinations),\n",
        "                    \"num_questions_evaluated\": len(questions),\n",
        "                    \"evaluation_status\": \"success\"\n",
        "                },\n",
        "                \"evaluations\": all_evaluations,\n",
        "                \"summary\": self._generate_summary(all_evaluations)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nCritical error in evaluate_experiments: {str(e)}\")\n",
        "            return self._create_default_evaluation(experiment_results)\n",
        "\n",
        "    def _evaluate_single_response(self, result: Dict, baseline: str) -> Dict:\n",
        "        \"\"\"Evaluate a single response against both baseline and source data\"\"\"\n",
        "        evaluation_prompt = f\"\"\"Compare and evaluate this response:\n",
        "\n",
        "        Question: {result[\"question\"]}\n",
        "\n",
        "        Baseline Answer from GPT-4: {baseline}\n",
        "        Source Data: {json.dumps(result.get(\"response\", {}).get(\"sources\", []), indent=2)}\n",
        "        Model Response: {json.dumps(result.get(\"response\", {}).get(\"response_text\", \"\"), indent=2)}\n",
        "\n",
        "        Score the response on these criteria (0-100):\n",
        "        - Source Accuracy: How well the response matches the actual source data provided\n",
        "        - Source Attribution: Whether appropriate source text is cited as evidence\n",
        "        - Conciseness: Clear, direct answer without extra information\n",
        "\n",
        "        Provide your evaluation in this exact JSON format:\n",
        "        {{\n",
        "            \"model\": \"{result[\"model\"]}\",\n",
        "            \"chunk_strategy\": \"{result[\"chunk_strategy\"]}\",\n",
        "            \"threshold\": {result[\"threshold\"] if result[\"chunk_strategy\"] == \"semantic\" else \"null\"},\n",
        "            \"question\": \"{result[\"question\"]}\",\n",
        "            \"baseline_answer\": \"{baseline}\",\n",
        "            \"model_response\": {json.dumps(result.get(\"response\", {}), indent=2)},\n",
        "            \"scores\": {{\n",
        "                \"source_accuracy\": <score>,\n",
        "                \"source_attribution\": <score>,\n",
        "                \"conciseness\": <score>\n",
        "            }},\n",
        "            \"composite_score\": <average of scores>,\n",
        "            \"detailed_analysis\": {{\n",
        "                \"accuracy_analysis\": \"explanation focusing on match with source data, including comparison with baseline\",\n",
        "                \"attribution_analysis\": \"explanation of source usage\",\n",
        "                \"conciseness_analysis\": \"explanation of clarity and directness\"\n",
        "            }}\n",
        "        }}\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert at evaluating response accuracy against both baseline answers and source data.\"},\n",
        "                    {\"role\": \"user\", \"content\": evaluation_prompt}\n",
        "                ],\n",
        "                temperature=0.7,\n",
        "                max_tokens=1000\n",
        "            )\n",
        "\n",
        "            content = response.choices[0].message.content\n",
        "            if '{' in content and '}' in content:\n",
        "                json_str = content[content.find('{'):content.rfind('}')+1]\n",
        "                return json.loads(json_str)\n",
        "            return self._create_default_single_evaluation(result, baseline)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating response: {str(e)}\")\n",
        "            return self._create_default_single_evaluation(result, baseline)\n",
        "\n",
        "    def _create_default_single_evaluation(self, result: Dict, baseline: str) -> Dict:\n",
        "        \"\"\"Create a default evaluation for a single response when evaluation fails\"\"\"\n",
        "        return {\n",
        "            \"model\": result[\"model\"],\n",
        "            \"chunk_strategy\": result[\"chunk_strategy\"],\n",
        "            \"threshold\": result[\"threshold\"] if result[\"chunk_strategy\"] == \"semantic\" else None,\n",
        "            \"question\": result[\"question\"],\n",
        "            \"baseline_answer\": baseline,\n",
        "            \"model_response\": result.get(\"response\", {}),\n",
        "            \"scores\": {\n",
        "                \"source_accuracy\": 0,\n",
        "                \"source_attribution\": 0,\n",
        "                \"conciseness\": 0\n",
        "            },\n",
        "            \"composite_score\": 0,\n",
        "            \"detailed_analysis\": {\n",
        "                \"accuracy_analysis\": \"Evaluation failed\",\n",
        "                \"attribution_analysis\": \"Evaluation failed\",\n",
        "                \"conciseness_analysis\": \"Evaluation failed\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _generate_summary(self, evaluations: List[Dict]) -> Dict:\n",
        "        \"\"\"Generate summary statistics from evaluations\"\"\"\n",
        "        if not evaluations:\n",
        "            return {\n",
        "                \"overall_performance\": \"No evaluations available\",\n",
        "                \"optimal_permutation\": \"Not available\",\n",
        "                \"performance_analysis\": \"Evaluation process failed\"\n",
        "            }\n",
        "\n",
        "        strategy_scores = {}\n",
        "        for eval in evaluations:\n",
        "            key = (eval[\"model\"], eval[\"chunk_strategy\"])\n",
        "            if \"threshold\" in eval and eval[\"chunk_strategy\"] == \"semantic\":\n",
        "                key = (eval[\"model\"], eval[\"chunk_strategy\"], eval[\"threshold\"])\n",
        "\n",
        "            if key not in strategy_scores:\n",
        "                strategy_scores[key] = {\n",
        "                    \"count\": 0,\n",
        "                    \"total_composite\": 0\n",
        "                }\n",
        "\n",
        "            scores = strategy_scores[key]\n",
        "            scores[\"count\"] += 1\n",
        "            scores[\"total_composite\"] += eval[\"composite_score\"]\n",
        "\n",
        "        best_score = 0\n",
        "        best_config = None\n",
        "        strategy_analysis = {}\n",
        "\n",
        "        for key, scores in strategy_scores.items():\n",
        "            avg_composite = scores[\"total_composite\"] / scores[\"count\"]\n",
        "\n",
        "            if len(key) == 3:  # Semantic chunking with threshold\n",
        "                model, strategy, threshold = key\n",
        "                config_str = f\"{model} with {strategy} chunking (threshold: {threshold})\"\n",
        "            else:  # Other chunking strategies\n",
        "                model, strategy = key\n",
        "                config_str = f\"{model} with {strategy} chunking\"\n",
        "\n",
        "            strategy_analysis[config_str] = avg_composite\n",
        "\n",
        "            if avg_composite > best_score:\n",
        "                best_score = avg_composite\n",
        "                best_config = config_str\n",
        "\n",
        "        return {\n",
        "            \"overall_performance\": f\"Average composite score across all evaluations: {sum(e['composite_score'] for e in evaluations)/len(evaluations):.2f}/100\",\n",
        "            \"optimal_permutation\": f\"Best performance: {best_config} (score: {best_score:.2f}/100)\",\n",
        "            \"performance_analysis\": strategy_analysis\n",
        "        }\n",
        "\n",
        "    def _create_default_evaluation(self, experiment_results: Dict) -> Dict:\n",
        "        \"\"\"Create a default evaluation result when the evaluation process fails\"\"\"\n",
        "        return {\n",
        "            \"metadata\": {\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"model_used\": \"gpt-4o\",\n",
        "                \"num_combinations_evaluated\": 0,\n",
        "                \"num_questions_evaluated\": 0,\n",
        "                \"evaluation_status\": \"failed\"\n",
        "            },\n",
        "            \"evaluations\": [\n",
        "                self._create_default_single_evaluation(result, \"Evaluation failed\")\n",
        "                for result in experiment_results[\"results\"]\n",
        "            ],\n",
        "            \"summary\": {\n",
        "                \"overall_performance\": \"Evaluation failed\",\n",
        "                \"optimal_permutation\": \"Not available\",\n",
        "                \"performance_analysis\": \"Evaluation process failed\"\n",
        "            }\n",
        "        }"
      ],
      "metadata": {
        "id": "1rAK93yw4qCx"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results Manager Class"
      ],
      "metadata": {
        "id": "lJmy7VbumFk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResultsManager:\n",
        "    \"\"\"Handles formatting, saving, and displaying evaluation results\"\"\"\n",
        "    def __init__(self, save_directory: str):\n",
        "        self.save_directory = save_directory\n",
        "        os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "    def format_results(self, experiment_results: Dict, evaluation_results: Dict) -> Tuple[Dict, Dict]:\n",
        "        \"\"\"Format experiment and evaluation results into structured output\"\"\"\n",
        "        print(\"\\n=== Starting Results Formatting ===\")\n",
        "\n",
        "        # Format experiment results\n",
        "        formatted_experiment = {\n",
        "            \"metadata\": experiment_results.get(\"metadata\", {}),\n",
        "            \"results\": [{\n",
        "                \"model\": result[\"model\"],\n",
        "                \"chunk_strategy\": result[\"chunk_strategy\"],\n",
        "                \"threshold\": result[\"threshold\"],\n",
        "                \"question\": result[\"question\"],\n",
        "                \"response\": {\n",
        "                    \"answer\": result[\"response\"].get(\"response_text\", \"\"),\n",
        "                    \"sources\": result[\"response\"].get(\"sources\", [])\n",
        "                }\n",
        "            } for result in experiment_results[\"results\"]]\n",
        "        }\n",
        "\n",
        "        # Format evaluation results with baseline answer\n",
        "        formatted_evaluation = {\n",
        "            \"metadata\": evaluation_results[\"metadata\"],\n",
        "            \"evaluations\": [{\n",
        "                \"model\": eval.get(\"model\"),\n",
        "                \"chunk_strategy\": eval.get(\"chunk_strategy\"),\n",
        "                \"threshold\": eval.get(\"threshold\"),\n",
        "                \"question\": eval.get(\"question\"),\n",
        "                \"baseline_answer\": eval.get(\"baseline_answer\", \"No baseline available\"),  # Include baseline answer\n",
        "                \"model_response\": eval.get(\"model_response\", {}),\n",
        "                \"scores\": eval.get(\"scores\", {}),\n",
        "                \"composite_score\": eval.get(\"composite_score\"),\n",
        "                \"detailed_analysis\": eval.get(\"detailed_analysis\", {})\n",
        "            } for eval in evaluation_results.get(\"evaluations\", [])],\n",
        "            \"overall_summary\": evaluation_results.get(\"summary\", {})\n",
        "        }\n",
        "\n",
        "        return formatted_experiment, formatted_evaluation\n",
        "\n",
        "    def save_results(self, formatted_experiment: Dict, formatted_evaluation: Dict) -> Tuple[str, str]:\n",
        "        \"\"\"Save formatted results to JSON files\"\"\"\n",
        "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "        experiment_file = f\"{self.save_directory}/experiment_results_{timestamp}.json\"\n",
        "        evaluation_file = f\"{self.save_directory}/evaluation_results_{timestamp}.json\"\n",
        "\n",
        "        for filepath, data in [\n",
        "            (experiment_file, formatted_experiment),\n",
        "            (evaluation_file, formatted_evaluation)\n",
        "        ]:\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return experiment_file, evaluation_file\n",
        "\n",
        "    def display_results(self, evaluation_results: Dict):\n",
        "        \"\"\"Display evaluation results in a clear, formatted manner\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EVALUATION RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Display metadata\n",
        "        metadata = evaluation_results.get(\"metadata\", {})\n",
        "        print(\"\\nMETADATA:\")\n",
        "        print(\"-\"*80)\n",
        "        print(f\"Timestamp:           {metadata.get('timestamp', 'Not available')}\")\n",
        "        print(f\"Model Used:          {metadata.get('model_used', 'Not available')}\")\n",
        "        print(f\"Combinations:        {metadata.get('num_combinations_evaluated', 'Not available')}\")\n",
        "        print(f\"Questions:           {metadata.get('num_questions_evaluated', 'Not available')}\")\n",
        "        print(f\"Evaluation Status:   {metadata.get('evaluation_status', 'Not available')}\")\n",
        "\n",
        "        # Display evaluations\n",
        "        evaluations = evaluation_results.get(\"evaluations\", [])\n",
        "        if evaluations:\n",
        "            print(\"\\nDETAILED EVALUATIONS:\")\n",
        "            print(\"-\"*80)\n",
        "            for eval in evaluations:\n",
        "                print(f\"\\nQuestion: {eval.get('question', 'No question provided')}\")\n",
        "                print(f\"Model: {eval.get('model', 'No model specified')}\")\n",
        "                print(f\"Strategy: {eval.get('chunk_strategy', 'No strategy specified')}\")\n",
        "                if eval.get('threshold'):\n",
        "                    print(f\"Threshold: {eval.get('threshold')}\")\n",
        "\n",
        "                # Display baseline answer\n",
        "                print(\"\\nBaseline Answer:\")\n",
        "                baseline = eval.get('baseline_answer', 'No baseline answer available')\n",
        "                print(textwrap.fill(str(baseline), width=80))\n",
        "\n",
        "                print(\"\\nModel Response:\")\n",
        "                response = eval.get('model_response', {})\n",
        "                response_text = response.get('response_text', 'No response available')\n",
        "                if response_text:\n",
        "                    print(textwrap.fill(str(response_text), width=80))\n",
        "                else:\n",
        "                    print(\"No response available\")\n",
        "\n",
        "                print(\"\\nSource Data:\")\n",
        "                sources = response.get('sources', [])\n",
        "                if sources:\n",
        "                    for source in sources:\n",
        "                        if source:  # Check if source is not empty\n",
        "                            print(textwrap.fill(str(source), width=80))\n",
        "                else:\n",
        "                    print(\"No source data available\")\n",
        "\n",
        "                print(\"\\nScores:\")\n",
        "                scores = eval.get('scores', {})\n",
        "                for metric, score in scores.items():\n",
        "                    print(f\"- {metric.replace('_', ' ').capitalize()}: {score}/100\")\n",
        "                print(f\"Composite Score: {eval.get('composite_score', 0)}/100\")\n",
        "\n",
        "                print(\"\\nDetailed Analysis:\")\n",
        "                analysis = eval.get('detailed_analysis', {})\n",
        "                for aspect, details in analysis.items():\n",
        "                    if details:  # Check if details is not empty\n",
        "                        print(f\"\\n{aspect.replace('_', ' ').capitalize()}:\")\n",
        "                        print(textwrap.fill(str(details), width=80))\n",
        "\n",
        "        # Display summary\n",
        "        summary = evaluation_results.get(\"overall_summary\", {})\n",
        "        if summary:\n",
        "            print(\"\\nOVERALL SUMMARY:\")\n",
        "            print(\"-\"*80)\n",
        "\n",
        "            if \"overall_performance\" in summary:\n",
        "                print(\"\\nOverall Performance:\")\n",
        "                print(textwrap.fill(str(summary[\"overall_performance\"]), width=80))\n",
        "\n",
        "            if \"optimal_permutation\" in summary:\n",
        "                print(\"\\nOptimal Configuration:\")\n",
        "                print(textwrap.fill(str(summary[\"optimal_permutation\"]), width=80))\n",
        "\n",
        "            if \"performance_analysis\" in summary:\n",
        "                print(\"\\nPerformance Analysis:\")\n",
        "                analysis = summary[\"performance_analysis\"]\n",
        "                if isinstance(analysis, dict):\n",
        "                    for config, score in analysis.items():\n",
        "                        print(f\"{config}: {score:.2f}\")\n",
        "                else:\n",
        "                    print(textwrap.fill(str(analysis), width=80))"
      ],
      "metadata": {
        "id": "vnMb5d8cmKQU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "koQ5ZObJC2ek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qdI5iaXYsun",
        "outputId": "6efe386e-0de8-4655-8a32-a3af8c36b005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting experiment with configurations:\n",
            "Models: ['open-mistral-nemo']\n",
            "Thresholds: [85, 95]\n",
            "Chunk strategies: ['semantic', 'paragraph', 'header']\n",
            "Number of questions: 1\n",
            "\n",
            "Testing model: open-mistral-nemo\n",
            "Processing question: What were cloud revenues in Q2 2024?\n",
            "\n",
            "=== COSINE SEARCH DEBUG ===\n",
            "Query: What were cloud revenues in Q2 2024?\n",
            "Strategy: semantic\n",
            "Threshold: 85\n",
            "Requested k: 5\n",
            "Cache key: semantic_85\n",
            "\n",
            "Testing model: open-mistral-nemo\n",
            "Processing question: What were cloud revenues in Q2 2024?\n",
            "\n",
            "=== COSINE SEARCH DEBUG ===\n",
            "Query: What were cloud revenues in Q2 2024?\n",
            "Strategy: semantic\n",
            "Threshold: 95\n",
            "Requested k: 5\n",
            "Cache key: semantic_95\n",
            "\n",
            "=== CHUNK CREATION DEBUG ===\n",
            "Strategy: paragraph\n",
            "Cache key: paragraph_2048\n",
            "Using max chunk size: 2048 characters with 100 character overlap\n",
            "\n",
            "Testing model: open-mistral-nemo\n",
            "Processing question: What were cloud revenues in Q2 2024?\n",
            "\n",
            "=== COSINE SEARCH DEBUG ===\n",
            "Query: What were cloud revenues in Q2 2024?\n",
            "Strategy: paragraph\n",
            "Threshold: None\n",
            "Requested k: 5\n",
            "Cache key: paragraph_1024\n",
            "Warning: No chunks found for strategy paragraph\n",
            "\n",
            "=== CHUNK CREATION DEBUG ===\n",
            "Strategy: header\n",
            "Cache key: header_2048\n",
            "Using max chunk size: 2048 characters with 100 character overlap\n",
            "\n",
            "Testing model: open-mistral-nemo\n",
            "Processing question: What were cloud revenues in Q2 2024?\n",
            "\n",
            "=== COSINE SEARCH DEBUG ===\n",
            "Query: What were cloud revenues in Q2 2024?\n",
            "Strategy: header\n",
            "Threshold: None\n",
            "Requested k: 5\n",
            "Cache key: header_1024\n",
            "Warning: No chunks found for strategy header\n",
            "\n",
            "Initializing GPT-4o evaluation...\n",
            "\n",
            "=== DEBUG: evaluate_experiments ===\n",
            "Getting questions...\n",
            "Questions extracted: ['What were cloud revenues in Q2 2024?']\n",
            "\n",
            "Getting baseline answers...\n",
            "\n",
            "=== DEBUG: _get_baseline_answers ===\n",
            "Questions received: ['What were cloud revenues in Q2 2024?']\n",
            "Number of document parts: 52\n",
            "\n",
            "Full document length: 147898 characters\n",
            "\n",
            "Sampling from document:\n",
            "Start: UNITED STATES\n",
            "SECURITIES AND EXCHANGE COMMISSION\n",
            "Washington, D.C. 20549\n",
            "________________________________________________________________________________________\n",
            "FORM 10-Q \n",
            "____________________________ ...\n",
            "Middle: llocation of \n",
            "undistributed earnings  8,600  1,275  8,493  18,368  10,046  1,484  9,623  21,153 \n",
            "Net income $ 8,600 $ 1,275 $ 8,493 $ 18,368 $ 11,219 $ 1,657 $ 10,743 $ 23,619 \n",
            "Denominator\n",
            "Number of s ...\n",
            "End: ment Officer; Chief Financial Officer\n",
            "ALPHABET INC.\n",
            "July 23, 2024 By: /s/    AMIE THUENER O'TOOLE        \n",
            "Amie Thuener O'Toole\n",
            "Vice President, Corporate Controller and Principal Accounting \n",
            "Officer\n",
            "52 ...\n",
            "\n",
            "--- Getting Baseline Answers ---\n",
            "\n",
            "Raw GPT-4 Response:\n",
            "```json\n",
            "{\n",
            "  \"What were cloud revenues in Q2 2024?\": \"$10,347 million\"\n",
            "}\n",
            "```\n",
            "\n",
            "Parsed Baseline Answers:\n",
            "{'What were cloud revenues in Q2 2024?': '$10,347 million'}\n",
            "Baseline answers received: {'What were cloud revenues in Q2 2024?': '$10,347 million'}\n",
            "\n",
            "Evaluating response for: What were cloud revenues in Q2 2024?\n",
            "Using baseline answer: $10,347 million\n",
            "\n",
            "Evaluating response for: What were cloud revenues in Q2 2024?\n",
            "Using baseline answer: $10,347 million\n",
            "\n",
            "Evaluating response for: What were cloud revenues in Q2 2024?\n",
            "Using baseline answer: $10,347 million\n",
            "\n",
            "Evaluating response for: What were cloud revenues in Q2 2024?\n",
            "Using baseline answer: $10,347 million\n",
            "\n",
            "=== Starting Results Formatting ===\n",
            "\n",
            "================================================================================\n",
            "EVALUATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "METADATA:\n",
            "--------------------------------------------------------------------------------\n",
            "Timestamp:           2024-11-16T16:36:02.476515\n",
            "Model Used:          gpt-4o\n",
            "Combinations:        4\n",
            "Questions:           1\n",
            "Evaluation Status:   success\n",
            "\n",
            "DETAILED EVALUATIONS:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Question: What were cloud revenues in Q2 2024?\n",
            "Model: open-mistral-nemo\n",
            "Strategy: semantic\n",
            "Threshold: 95\n",
            "\n",
            "Baseline Answer:\n",
            "$10,347 million\n",
            "\n",
            "Model Response:\n",
            "Google Cloud revenues in Q2 2024 were $10.3 billion.\n",
            "\n",
            "Source Data:\n",
            "Revenues by type (in millions): Three Months Ended Six Months Ended June 30,\n",
            "June 30, 2023 2024 2023 2024 Google Cloud  8,031  10,347  15,485  19,921\n",
            "\n",
            "Scores:\n",
            "- Source accuracy: 95/100\n",
            "- Source attribution: 100/100\n",
            "- Conciseness: 100/100\n",
            "Composite Score: 98.33/100\n",
            "\n",
            "Detailed Analysis:\n",
            "\n",
            "Accuracy analysis:\n",
            "The model response accurately converts the source data of $10,347 million into\n",
            "$10.3 billion, which is consistent with the baseline answer. The conversion is\n",
            "correct, but a slight rounding occurs in the conversion to billion.\n",
            "\n",
            "Attribution analysis:\n",
            "The response includes an appropriate source text that supports the answer\n",
            "provided. The source data is clearly cited and relevant.\n",
            "\n",
            "Conciseness analysis:\n",
            "The response is clear and direct, providing the necessary answer without any\n",
            "extraneous information.\n",
            "\n",
            "Question: What were cloud revenues in Q2 2024?\n",
            "Model: open-mistral-nemo\n",
            "Strategy: paragraph\n",
            "\n",
            "Baseline Answer:\n",
            "$10,347 million\n",
            "\n",
            "Model Response:\n",
            "No relevant context found.\n",
            "\n",
            "Source Data:\n",
            "No source data available\n",
            "\n",
            "Scores:\n",
            "- Source accuracy: 0/100\n",
            "- Source attribution: 0/100\n",
            "- Conciseness: 80/100\n",
            "Composite Score: 26.67/100\n",
            "\n",
            "Detailed Analysis:\n",
            "\n",
            "Accuracy analysis:\n",
            "The model response did not provide an answer to the question or match the\n",
            "baseline answer of $10,347 million, as no source data was available to verify.\n",
            "Therefore, the source accuracy score is 0.\n",
            "\n",
            "Attribution analysis:\n",
            "Since the model response did not provide any source for its answer, the source\n",
            "attribution score is 0. No source text was cited as evidence.\n",
            "\n",
            "Conciseness analysis:\n",
            "The response is concise as it directly states that no relevant context was found\n",
            "without additional unnecessary information. However, since it doesn't provide an\n",
            "answer, the conciseness score is slightly reduced.\n",
            "\n",
            "Question: What were cloud revenues in Q2 2024?\n",
            "Model: open-mistral-nemo\n",
            "Strategy: semantic\n",
            "Threshold: 85\n",
            "\n",
            "Baseline Answer:\n",
            "$10,347 million\n",
            "\n",
            "Model Response:\n",
            "Google Cloud revenues in Q2 2024 were $10,347 million.\n",
            "\n",
            "Source Data:\n",
            "Google Cloud revenues were $8,031 million in Q2 2023 and $10,347 million in Q2\n",
            "2024, as per the 'Disaggregated Revenues' table.\n",
            "\n",
            "Scores:\n",
            "- Source accuracy: 100/100\n",
            "- Source attribution: 100/100\n",
            "- Conciseness: 100/100\n",
            "Composite Score: 100/100\n",
            "\n",
            "Detailed Analysis:\n",
            "\n",
            "Accuracy analysis:\n",
            "The response accurately reflects the source data, stating the exact revenue\n",
            "figure for Q2 2024 as $10,347 million, consistent with both the provided source\n",
            "data and the baseline answer.\n",
            "\n",
            "Attribution analysis:\n",
            "The response appropriately cites the source data, ensuring the source text is\n",
            "referenced as evidence for the revenue figure provided.\n",
            "\n",
            "Conciseness analysis:\n",
            "The response is clear and direct, providing the necessary information without\n",
            "any extraneous details.\n",
            "\n",
            "Question: What were cloud revenues in Q2 2024?\n",
            "Model: open-mistral-nemo\n",
            "Strategy: header\n",
            "\n",
            "Baseline Answer:\n",
            "$10,347 million\n",
            "\n",
            "Model Response:\n",
            "No relevant context found.\n",
            "\n",
            "Source Data:\n",
            "No source data available\n",
            "\n",
            "Scores:\n",
            "- Source accuracy: 0/100\n",
            "- Source attribution: 0/100\n",
            "- Conciseness: 100/100\n",
            "Composite Score: 33.33/100\n",
            "\n",
            "Detailed Analysis:\n",
            "\n",
            "Accuracy analysis:\n",
            "The model response states 'No relevant context found', which does not provide a\n",
            "numerical answer for the cloud revenues in Q2 2024. Since no source data was\n",
            "provided, it's impossible to verify against the actual numbers. The baseline\n",
            "answer of $10,347 million cannot be confirmed or denied due to lack of source\n",
            "data.\n",
            "\n",
            "Attribution analysis:\n",
            "The model response does not cite any sources, as it states that no relevant\n",
            "context was found. Thus, it scores zero in source attribution because it neither\n",
            "provides nor relies on any source text.\n",
            "\n",
            "Conciseness analysis:\n",
            "The response is concise as it directly states that no relevant context was found\n",
            "without adding unnecessary information. This meets the criteria for conciseness\n",
            "and thus scores a full 100.\n",
            "\n",
            "OVERALL SUMMARY:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Overall Performance:\n",
            "Average composite score across all evaluations: 64.58/100\n",
            "\n",
            "Optimal Configuration:\n",
            "Best performance: open-mistral-nemo with semantic chunking (threshold: 85)\n",
            "(score: 100.00/100)\n",
            "\n",
            "Performance Analysis:\n",
            "open-mistral-nemo with semantic chunking (threshold: 95): 98.33\n",
            "open-mistral-nemo with paragraph chunking: 26.67\n",
            "open-mistral-nemo with semantic chunking (threshold: 85): 100.00\n",
            "open-mistral-nemo with header chunking: 33.33\n",
            "\n",
            "Experiment complete!\n",
            "Results saved to:\n",
            "  Experiment results: /content/drive/My Drive/AI/Model_Analysis/experiment_results_20241116-163602.json\n",
            "  Evaluation results: /content/drive/My Drive/AI/Model_Analysis/evaluation_results_20241116-163602.json\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Initialize configurations\n",
        "    model_config = ModelConfig(\n",
        "        models=MODEL_CONFIGS[\"models\"],\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    # Initialize experiment runner\n",
        "    experiment_runner = ExperimentRunner(\n",
        "        model_config=model_config,\n",
        "        thresholds=CHUNKING_CONFIGS[\"thresholds\"],\n",
        "        questions=QUESTION_CONFIGS[\"questions\"],\n",
        "        chunk_strategies=CHUNKING_CONFIGS[\"strategies\"]\n",
        "    )\n",
        "\n",
        "    print(\"Starting experiment with configurations:\")\n",
        "    print(f\"Models: {[model['name'] for model in model_config.models]}\")\n",
        "    print(f\"Thresholds: {CHUNKING_CONFIGS['thresholds']}\")\n",
        "    print(f\"Chunk strategies: {CHUNKING_CONFIGS['strategies']}\")\n",
        "    print(f\"Number of questions: {len(QUESTION_CONFIGS['questions'])}\")\n",
        "\n",
        "    # Run experiments\n",
        "    experiment_results = experiment_runner.run_experiments()\n",
        "\n",
        "   # Initialize evaluator with full documents list\n",
        "    print(\"\\nInitializing GPT-4o evaluation...\")\n",
        "    evaluator = ExperimentEvaluator(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "    # Run evaluation with full documents list\n",
        "    evaluation_results = evaluator.evaluate_experiments(\n",
        "        experiment_results=experiment_results,\n",
        "        source_docs=documents  # Pass the full documents list\n",
        "    )\n",
        "\n",
        "    # Initialize results manager\n",
        "    results_manager = ResultsManager(save_directory=FILE_CONFIGS['save_directory'])\n",
        "\n",
        "    # Format results\n",
        "    formatted_experiment, formatted_evaluation = results_manager.format_results(\n",
        "        experiment_results=experiment_results,\n",
        "        evaluation_results=evaluation_results\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    experiment_file, evaluation_file = results_manager.save_results(\n",
        "        formatted_experiment=formatted_experiment,\n",
        "        formatted_evaluation=formatted_evaluation\n",
        "    )\n",
        "\n",
        "    # Display results\n",
        "    results_manager.display_results(evaluation_results=formatted_evaluation)\n",
        "\n",
        "    print(\"\\nExperiment complete!\")\n",
        "    print(f\"Results saved to:\")\n",
        "    print(f\"  Experiment results: {experiment_file}\")\n",
        "    print(f\"  Evaluation results: {evaluation_file}\")\n",
        "\n",
        "    return formatted_experiment, formatted_evaluation\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results, evaluation = main()"
      ]
    }
  ]
}