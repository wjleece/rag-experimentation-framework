{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyM5ot3Tud2XHUTBs4R1TNLD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjleece/rag-experimentation-framework/blob/main/RAG_Experimentation_Framework_final_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you use this code, please cite:\n",
        "\n",
        "{\n",
        "  title = {RAG Experimentation Framework},\n",
        "\n",
        "  author = {Bill Leece},\n",
        "\n",
        "  year = {2024}\n",
        "}"
      ],
      "metadata": {
        "id": "wZ0kV_UtQn5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "_lHNBLR-92Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers --quiet\n",
        "#!pip install -U optimum --quiet\n",
        "!pip install -U accelerate  --quiet\n",
        "#!pip install -U bitsandbytes  --quiet\n",
        "!pip install -U torch --quiet\n",
        "!pip install -U sentencepiece --quiet\n",
        "!pip install -U llama-index --quiet\n",
        "!pip install -U llama-index-llms-mistralai --quiet\n",
        "!pip install -U llama-index-embeddings-mistralai --quiet\n",
        "!pip install -U llama-index-llms-langchain --quiet\n",
        "!pip install -U langchain --quiet\n",
        "!pip install -U langchain-community --quiet\n",
        "!pip install -U langchain-mistralai --quiet\n",
        "!pip install -U langchain_huggingface --quiet\n",
        "!pip install -U faiss-gpu --quiet"
      ],
      "metadata": {
        "id": "4g_Vs7wgZW-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f710389c-752d-4339-a0f6-c3be779ab790"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.17 requires torch<2.5,>=1.10, but you have torch 2.5.1 which is incompatible.\n",
            "torchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.5.1 which is incompatible.\n",
            "torchvision 0.19.1+cu121 requires torch==2.4.1, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.0/189.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.6/254.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.0/307.0 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.7/268.7 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "import transformers\n",
        "import torch\n",
        "import gc\n",
        "from google.colab import drive, userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "from llama_index.core import SimpleDirectoryReader, Settings\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "import time\n",
        "from typing import List, Dict, Tuple\n",
        "from contextlib import contextmanager\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "Ao7eaSfq-TKs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get('MISTRAL_API_KEY')"
      ],
      "metadata": {
        "id": "YvGHY024-OXK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' #Use GPUs when possible"
      ],
      "metadata": {
        "id": "mxAHV7T_-Xlh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfUKkdCs-wQJ",
        "outputId": "ee5b1e8c-e08d-4463-889f-f1d6f85cdf46"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 14 14:55:17 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   50C    P8              14W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment Configurations"
      ],
      "metadata": {
        "id": "jkqEV8M_HUKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup configurations\n",
        "MODEL_CONFIGS = {\n",
        "    \"models\": [\n",
        "    #    {\n",
        "    #        \"name\": \"open-mixtral-8x7b\",\n",
        "    #        \"type\": \"mistral_api\",\n",
        "    #        \"tokenizer\": None,  # Not needed for API models\n",
        "    #    },\n",
        "         {\n",
        "            \"name\": \"open-mistral-nemo\",\n",
        "            \"type\": \"mistral_api\",\n",
        "            \"tokenizer\": None,  # Not needed for API models\n",
        "         },\n",
        "   #     {\n",
        "   #         \"name\": \"ministral-8b-latest\",\n",
        "   #         \"type\": \"mistral_api\",\n",
        "   #         \"tokenizer\": None,  # Not needed for API models\n",
        "   #     },\n",
        "   #   {\n",
        "   #         \"name\": \"wjleece/quantized-mistral-7b\",\n",
        "   #         \"type\": \"huggingface_quantized\",\n",
        "   #         \"tokenizer\": \"mistralai/Mixtral-8x7B-v0.1\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "   #          \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "   #             \"load_in_4bit\": True,\n",
        "   #             \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "   #             \"bnb_4bit_quant_type\": \"nf4\",\n",
        "   #             \"bnb_4bit_use_double_quant\": False\n",
        "   #         }\n",
        "   #     },\n",
        "   #   {\n",
        "   #           \"name\": \"wjleece/quantized-mistral-nemo-12b\",\n",
        "   #           \"type\": \"huggingface_quantized\",\n",
        "   #           \"tokenizer\": \"mistralai/Mistral-Nemo-Instruct-2407\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "   #           \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "   #               \"load_in_4bit\": True,\n",
        "   #               \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "   #               \"bnb_4bit_quant_type\": \"nf4\",\n",
        "   #               \"bnb_4bit_use_double_quant\": False\n",
        "   #          }\n",
        "   #       },\n",
        "     #  {\n",
        "     #         \"name\": \"wjleece/quantized-mistral-8b\",\n",
        "     #         \"type\": \"huggingface_quantized\",\n",
        "     #         \"tokenizer\": \"mistralai/Ministral-8B-Instruct-2410\",  # The same tokenizer that works on the base model will work on the quantized model - there is no 'quantized tokenizer'\n",
        "     #         \"quantization_config\": {                    #Quantization config left here as a reference, but not used in the code (as we're using an already quantized model from HuggingFace)\n",
        "     #             \"load_in_4bit\": True,\n",
        "     #             \"bnb_4bit_compute_dtype\": \"float16\",\n",
        "     #             \"bnb_4bit_quant_type\": \"nf4\",\n",
        "     #             \"bnb_4bit_use_double_quant\": False\n",
        "     #         }\n",
        "     #     }\n",
        "       ],\n",
        "    \"chunk_strategies\": [\"semantic\", \"paragraph\", \"header\"], #can also try \"paragraph\", \"header\" strategies - I got some surprising results here at times\n",
        "    \"thresholds\": [85, 95], #RAG semantic chunking thresholds (higher thresholds --> fewer RAG chunks created)\n",
        "}\n",
        "\"\"\n",
        "QUESTION_CONFIGS = {\n",
        "    \"questions\": [\n",
        "        \"What were cloud revenues in Q2 2024?\",\n",
        "        \"What were the main drivers of revenue growth in Q2?\",\n",
        "        \"How much did YouTube ad revenues grow in Q2 in APAC?\",\n",
        "        \"Can you summarize recent key antitrust matters?\",\n",
        "        \"Compare the revenue growth across all geographic regions and explain the main factors for each region.\",\n",
        "        \"Summarize all mentioned risk factors related to international operations.\",\n",
        "        \"What were the major changes in operating expenses across all categories and their stated reasons?\"\n",
        "    ] #These quetsions should relate to the RAG document --> these are your 'business use cases'\n",
        "}\n",
        "\n",
        "FILE_CONFIGS = {\n",
        "    \"save_directory\": '/content/drive/My Drive/AI/Model_Analysis'\n",
        "}"
      ],
      "metadata": {
        "id": "YDjgk_JhHWkj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load RAG Document"
      ],
      "metadata": {
        "id": "e_wxgOGc95sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "documents = SimpleDirectoryReader(input_files=[\"/content/drive/My Drive/AI/Datasets/Google-10-q/goog-10-q-q2-2024.pdf\"]).load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS4Lemk09v9Y",
        "outputId": "bdcf3f79-e50c-4c46-dd68-bb6a3ac57f51"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RAG Pipeline Class"
      ],
      "metadata": {
        "id": "MgLxma5M-bZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global singleton instance\n",
        "_GLOBAL_RAG_PIPELINE = None\n",
        "\n",
        "class RAGPipeline:\n",
        "    def __init__(self):\n",
        "        self.chunk_cache = {}\n",
        "        self.embedding_cache = {}\n",
        "        self.embedding_model = None\n",
        "\n",
        "    @classmethod\n",
        "    def get_instance(cls):\n",
        "        \"\"\"Get or create singleton instance\"\"\"\n",
        "        global _GLOBAL_RAG_PIPELINE\n",
        "        if _GLOBAL_RAG_PIPELINE is None:\n",
        "            _GLOBAL_RAG_PIPELINE = cls()\n",
        "        return _GLOBAL_RAG_PIPELINE\n",
        "\n",
        "\n",
        "    def initialize_embedding_model(self):\n",
        "        \"\"\"Initialize the embedding model if not already initialized\"\"\"\n",
        "        if self.embedding_model is None:\n",
        "            mistral_api_key = userdata.get('MISTRAL_API_KEY')\n",
        "            self.embedding_model = MistralAIEmbedding(\n",
        "                model_name=\"mistral-embed\",\n",
        "                api_key=mistral_api_key\n",
        "            )\n",
        "        return self.embedding_model\n",
        "\n",
        "    def convert_to_markdown_headers(self, text):\n",
        "        \"\"\"Convert document section titles to markdown headers\"\"\"\n",
        "        import re\n",
        "\n",
        "        patterns = [\n",
        "            (r'^(?:ITEM|Section)\\s+\\d+[.:]\\s*(.+)$', '# '),\n",
        "            (r'^\\d+\\.\\d+\\s+(.+)$', '## '),\n",
        "            (r'^\\([a-z]\\)\\s+(.+)$', '### ')\n",
        "        ]\n",
        "\n",
        "        lines = text.split('\\n')\n",
        "        markdown_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            converted = False\n",
        "\n",
        "            for pattern, header_mark in patterns:\n",
        "                if re.match(pattern, line, re.IGNORECASE):\n",
        "                    markdown_lines.append(f\"{header_mark}{line}\")\n",
        "                    converted = True\n",
        "                    break\n",
        "\n",
        "            if not converted:\n",
        "                markdown_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(markdown_lines)\n",
        "\n",
        "    def create_chunks(self, documents: List, threshold: int, chunk_strategy: str = \"semantic\") -> Dict:\n",
        "        \"\"\"Create or retrieve chunks based on specified strategy\"\"\"\n",
        "        from langchain.text_splitter import (\n",
        "            RecursiveCharacterTextSplitter,\n",
        "            MarkdownHeaderTextSplitter\n",
        "        )\n",
        "\n",
        "        FIXED_CHUNK_SIZE = 1024\n",
        "        CHUNK_OVERLAP = 100\n",
        "\n",
        "        if chunk_strategy == \"semantic\":\n",
        "            cache_key = f\"{chunk_strategy}_{threshold}\"\n",
        "        else:\n",
        "            cache_key = f\"{chunk_strategy}_{FIXED_CHUNK_SIZE}\"\n",
        "\n",
        "        if cache_key not in self.chunk_cache:\n",
        "            print(\"\\n=== CHUNK CREATION DEBUG ===\")\n",
        "            print(f\"Strategy: {chunk_strategy}\")\n",
        "            print(f\"Cache key: {cache_key}\")\n",
        "            if chunk_strategy == \"semantic\":\n",
        "                print(f\"Using semantic threshold: {threshold}\")\n",
        "            else:\n",
        "                print(f\"Using fixed chunk size: {FIXED_CHUNK_SIZE} characters with {CHUNK_OVERLAP} character overlap\")\n",
        "\n",
        "            if len(self.chunk_cache) > 2:\n",
        "                oldest_key = min(self.chunk_cache.keys())\n",
        "                if oldest_key != cache_key:\n",
        "                    del self.chunk_cache[oldest_key]\n",
        "                    if oldest_key in self.embedding_cache:\n",
        "                        del self.embedding_cache[oldest_key]\n",
        "                    gc.collect()\n",
        "\n",
        "            if chunk_strategy == \"semantic\":\n",
        "                if self.embedding_model is None:\n",
        "                    self.initialize_embedding_model()\n",
        "\n",
        "                splitter = SemanticSplitterNodeParser(\n",
        "                    buffer_size=1,\n",
        "                    breakpoint_percentile_threshold=threshold,\n",
        "                    embed_model=self.embedding_model\n",
        "                )\n",
        "                nodes = splitter.get_nodes_from_documents(documents)\n",
        "                texts = [node.text for node in nodes]\n",
        "\n",
        "            elif chunk_strategy == \"paragraph\":\n",
        "                text_splitter = RecursiveCharacterTextSplitter(\n",
        "                    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "                    chunk_size=FIXED_CHUNK_SIZE,\n",
        "                    chunk_overlap=CHUNK_OVERLAP,\n",
        "                    length_function=len\n",
        "                )\n",
        "                texts = []\n",
        "                for doc in documents:\n",
        "                    chunks = text_splitter.split_text(doc.text)\n",
        "                    texts.extend(chunks)\n",
        "\n",
        "            elif chunk_strategy == \"header\":\n",
        "                headers_to_split_on = [\n",
        "                    (\"#\", \"Header 1\"),\n",
        "                    (\"##\", \"Header 2\"),\n",
        "                    (\"###\", \"Header 3\"),\n",
        "                ]\n",
        "\n",
        "                header_splitter = MarkdownHeaderTextSplitter(\n",
        "                    headers_to_split_on=headers_to_split_on\n",
        "                )\n",
        "\n",
        "                text_splitter = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size=FIXED_CHUNK_SIZE,\n",
        "                    chunk_overlap=CHUNK_OVERLAP,\n",
        "                    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "                )\n",
        "\n",
        "                texts = []\n",
        "                for doc in documents:\n",
        "                    md_text = self.convert_to_markdown_headers(doc.text)\n",
        "                    header_splits = header_splitter.split_text(md_text)\n",
        "\n",
        "                    for split in header_splits:\n",
        "                        chunks = text_splitter.split_text(split.page_content)\n",
        "                        texts.extend(chunks)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown chunk strategy: {chunk_strategy}\")\n",
        "\n",
        "            if texts:\n",
        "                chunk_lengths = [len(t) for t in texts]\n",
        "                print(f\"Created {len(texts)} chunks\")\n",
        "                print(f\"Average chunk size: {sum(chunk_lengths)/len(texts):.0f} characters\")\n",
        "                print(f\"Smallest chunk: {min(chunk_lengths)} characters\")\n",
        "                print(f\"Largest chunk: {max(chunk_lengths)} characters\")\n",
        "\n",
        "                print(\"\\nFirst few chunks for inspection:\")\n",
        "                for i, text in enumerate(texts[:3]):\n",
        "                    print(f\"\\nChunk {i} (length {len(text)}):\")\n",
        "                    print(\"-\" * 80)\n",
        "                    print(text)\n",
        "                    print(\"-\" * 80)\n",
        "            else:\n",
        "                print(\"Warning: No chunks created!\")\n",
        "\n",
        "            self.chunk_cache[cache_key] = {\n",
        "                'texts': texts,\n",
        "                'strategy': chunk_strategy\n",
        "            }\n",
        "\n",
        "        return self.chunk_cache[cache_key]\n",
        "\n",
        "    def run_cosine_search(self, query: str, threshold: int, chunk_strategy: str = \"semantic\", k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Run cosine similarity search with memory optimization and debugging\"\"\"\n",
        "        print(\"\\n=== COSINE SEARCH DEBUG ===\")\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Strategy: {chunk_strategy}\")\n",
        "        print(f\"Threshold: {threshold}\")\n",
        "        print(f\"Requested k: {k}\")\n",
        "\n",
        "        if self.embedding_model is None:\n",
        "            self.initialize_embedding_model()\n",
        "\n",
        "        FIXED_CHUNK_SIZE = 1024\n",
        "\n",
        "        if chunk_strategy == \"semantic\":\n",
        "            cache_key = f\"{chunk_strategy}_{threshold}\"\n",
        "        else:\n",
        "            cache_key = f\"{chunk_strategy}_{FIXED_CHUNK_SIZE}\"\n",
        "\n",
        "        print(f\"Cache key: {cache_key}\")\n",
        "\n",
        "        if cache_key not in self.embedding_cache:\n",
        "            try:\n",
        "                texts = self.chunk_cache[cache_key]['texts']\n",
        "                print(f\"Creating embeddings for {len(texts)} chunks\")\n",
        "            except KeyError:\n",
        "                print(f\"Warning: No chunks found for strategy {chunk_strategy}\")\n",
        "                return []\n",
        "\n",
        "            batch_size = 32\n",
        "            embeddings = []\n",
        "\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch_texts = texts[i:i + batch_size]\n",
        "                batch_embeddings = [self.embedding_model.get_text_embedding(text)\n",
        "                                  for text in batch_texts]\n",
        "                embeddings.extend(batch_embeddings)\n",
        "\n",
        "                if i % (batch_size * 4) == 0:\n",
        "                    gc.collect()\n",
        "\n",
        "            embeddings_array = np.array(embeddings).astype('float32')\n",
        "            normalized_embeddings = embeddings_array / np.linalg.norm(embeddings_array, axis=1)[:, np.newaxis]\n",
        "\n",
        "            dimension = embeddings_array.shape[1]\n",
        "            cosine_index = faiss.IndexFlatIP(dimension)\n",
        "            cosine_index.add(normalized_embeddings)\n",
        "\n",
        "            self.embedding_cache[cache_key] = {\n",
        "                'embeddings': embeddings_array,\n",
        "                'cosine_index': cosine_index\n",
        "            }\n",
        "\n",
        "        query_vector = self.embedding_model.get_text_embedding(query)\n",
        "        query_vector = np.array([query_vector]).astype('float32')\n",
        "        query_normalized = query_vector / np.linalg.norm(query_vector)\n",
        "\n",
        "        distances, indices = self.embedding_cache[cache_key]['cosine_index'].search(\n",
        "            query_normalized.reshape(1, -1).astype('float32'), k\n",
        "        )\n",
        "\n",
        "        return [\n",
        "            {\n",
        "                'text': self.chunk_cache[cache_key]['texts'][idx],\n",
        "                'distance': float(score),\n",
        "                'strategy': chunk_strategy\n",
        "            }\n",
        "            for score, idx in zip(distances[0], indices[0])\n",
        "        ]\n",
        "\n",
        "    def generate_response(self, query: str, context_rag: list, model: Dict) -> dict:\n",
        "        \"\"\"Generate response using provided context\"\"\"\n",
        "        try:\n",
        "            context_texts = [doc['text'] for doc in context_rag]\n",
        "            if not context_texts:\n",
        "                return {\"response_text\": \"No relevant context found.\", \"sources\": [], \"strategy\": context_rag[0]['strategy'] if context_rag else None}\n",
        "\n",
        "            context = \"\\n\\n\".join(context_texts)\n",
        "\n",
        "            prompt = PromptTemplate(template=\"\"\"\n",
        "            Instructions:\n",
        "\n",
        "            You are a helpful assistant who answers questions from context that has been provided to you.\n",
        "            Given the context information, provide a direct and concise answer to the question: {query}\n",
        "\n",
        "            Focus only on information present in the context. If you don't know the answer, say \"I don't know.\"\n",
        "            You must format your response as a JSON string object, starting with the word \"LLM_Response:\"\n",
        "\n",
        "            Your answer to {query} will be a JSON string object that starts with \"LLM_Response:\" as shown below:\n",
        "\n",
        "            LLM_Response:\n",
        "            {{\n",
        "                \"response_text\": \"Your detailed answer here\",\n",
        "                \"sources\": [\n",
        "                    \"Copy and paste here the exact text segments from the context that you used to generate your answer. Include all relevant segments, verbatim.\"\n",
        "                ]\n",
        "            }}\n",
        "\n",
        "            Important: In your response, the \"sources\" field must contain the exact text passages from the provided context that you used to formulate your answer. Copy these passages word-for-word.\n",
        "\n",
        "            Do not include a hypothetical example in your answer, only include your final answer after \"LLM_Response:\"\n",
        "\n",
        "            The context information that you will use for your answer is below:\n",
        "\n",
        "            ---------------\n",
        "            {context}\n",
        "            ---------------\n",
        "            \"\"\")\n",
        "\n",
        "            model_type = model['type']\n",
        "            llm = model['llm']\n",
        "\n",
        "            chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "            response = chain.invoke({\n",
        "                \"query\": query,\n",
        "                \"context\": context\n",
        "               })\n",
        "\n",
        "            response_text = response.split(\"LLM_Response:\")[-1].strip()\n",
        "\n",
        "            try:\n",
        "                if '{' in response_text and '}' in response_text:\n",
        "                    json_str = response_text[response_text.find('{'):response_text.rfind('}')+1]\n",
        "                    parsed_response = json.loads(json_str)\n",
        "                    return {\n",
        "                        \"response_text\": parsed_response.get(\"response_text\", response_text),\n",
        "                        \"sources\": parsed_response.get(\"sources\", []),\n",
        "                        \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                    }\n",
        "                else:\n",
        "                    return {\n",
        "                        \"response_text\": response_text,\n",
        "                        \"sources\": [],\n",
        "                        \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                    }\n",
        "            except json.JSONDecodeError:\n",
        "                return {\n",
        "                    \"response_text\": response_text,\n",
        "                    \"sources\": [],\n",
        "                    \"strategy\": context_rag[0]['strategy'] if context_rag else None\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {str(e)}\")\n",
        "            return {\"response_text\": \"An error occurred while generating the response.\", \"sources\": []}"
      ],
      "metadata": {
        "id": "YY5rnivk-bAh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ModelConfig Class"
      ],
      "metadata": {
        "id": "ljqi1Qg8j9F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelConfig:\n",
        "    \"\"\"Handles model configuration and management\"\"\"\n",
        "    def __init__(self,\n",
        "                 models: List[Dict],\n",
        "                 temperature: float = 0.3):\n",
        "        self.models = models\n",
        "        self.temperature = temperature\n",
        "        self.current_model = None\n",
        "        self.current_model_name = None\n",
        "\n",
        "\n",
        "    @contextmanager\n",
        "    def load_model(self, model_config: Dict):\n",
        "        \"\"\"Context manager for lazy loading and proper cleanup of models\"\"\"\n",
        "        try:\n",
        "            model_name = model_config[\"name\"]\n",
        "            model_type = model_config[\"type\"]\n",
        "\n",
        "            # Clear any existing model\n",
        "            self.cleanup_current_model()\n",
        "\n",
        "            if model_type == \"mistral_api\":\n",
        "                mistral_api_key = userdata.get('MISTRAL_API_KEY')\n",
        "                self.current_model = {\n",
        "                    'llm': ChatMistralAI(\n",
        "                        model=model_name,\n",
        "                        temperature=self.temperature,\n",
        "                        api_key=mistral_api_key\n",
        "                    ),\n",
        "                    'type': 'mistral_api'\n",
        "                }\n",
        "            else:  # huggingface_quantized\n",
        "                print(f\"Loading quantized model: {model_name}\")\n",
        "\n",
        "                # Empty CUDA cache before loading new model\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                tokenizer = AutoTokenizer.from_pretrained(\n",
        "                    pretrained_model_name_or_path=model_config[\"tokenizer\"],\n",
        "                    trust_remote_code=True,\n",
        "                    use_fast=True,\n",
        "                    padding_side=\"left\"\n",
        "                )\n",
        "\n",
        "                model = AutoModelForCausalLM.from_pretrained(\n",
        "                    pretrained_model_name_or_path=model_name,\n",
        "                    device_map=\"auto\",\n",
        "                    trust_remote_code=True,\n",
        "                    torch_dtype=torch.float16,\n",
        "                    use_cache=True,\n",
        "                    low_cpu_mem_usage=True,\n",
        "                )\n",
        "\n",
        "                pipe = pipeline(\n",
        "                    \"text-generation\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_new_tokens=512,\n",
        "                    temperature=self.temperature,\n",
        "                    top_p=0.95,\n",
        "                    top_k=50,\n",
        "                    do_sample=True,\n",
        "                    device_map=\"auto\"\n",
        "                )\n",
        "\n",
        "                self.current_model = {\n",
        "                    'llm': HuggingFacePipeline(pipeline=pipe),\n",
        "                    'type': 'huggingface_quantized',\n",
        "                    'model': model,  # Keep reference for cleanup\n",
        "                    'pipe': pipe     # Keep reference for cleanup\n",
        "                }\n",
        "\n",
        "            self.current_model_name = model_name\n",
        "            yield self.current_model\n",
        "\n",
        "        finally:\n",
        "            # Cleanup will happen in cleanup_current_model()\n",
        "            pass\n",
        "\n",
        "    def cleanup_current_model(self):\n",
        "        \"\"\"Clean up the current model and free memory\"\"\"\n",
        "        if self.current_model is not None:\n",
        "            if self.current_model['type'] == 'huggingface_quantized':\n",
        "                # Delete model components explicitly\n",
        "                del self.current_model['llm']\n",
        "                del self.current_model['model']\n",
        "                del self.current_model['pipe']\n",
        "\n",
        "                # Clear CUDA cache\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                # Run garbage collection\n",
        "                gc.collect()\n",
        "\n",
        "            self.current_model = None\n",
        "            self.current_model_name = None"
      ],
      "metadata": {
        "id": "tCzG7OE0IiDT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ExperimentRunner Class"
      ],
      "metadata": {
        "id": "05gTul4pIW6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperimentRunner:\n",
        "    \"\"\"Handles experiment execution\"\"\"\n",
        "    def __init__(self,\n",
        "                 model_config: ModelConfig,\n",
        "                 thresholds: List[int],\n",
        "                 questions: List[str],\n",
        "                 chunk_strategies: List[str],\n",
        "                 rag_pipeline: RAGPipeline = None):\n",
        "        self.model_config = model_config\n",
        "        self.thresholds = thresholds\n",
        "        self.questions = questions\n",
        "        self.chunk_strategies = chunk_strategies\n",
        "\n",
        "        # Use existing RAG pipeline or create new one\n",
        "        global _GLOBAL_RAG_PIPELINE\n",
        "        if rag_pipeline:\n",
        "            self.rag_pipeline = rag_pipeline\n",
        "        elif _GLOBAL_RAG_PIPELINE:\n",
        "            self.rag_pipeline = _GLOBAL_RAG_PIPELINE\n",
        "        else:\n",
        "            print(\"Initializing new RAG pipeline\")\n",
        "            _GLOBAL_RAG_PIPELINE = RAGPipeline()\n",
        "            self.rag_pipeline = _GLOBAL_RAG_PIPELINE\n",
        "\n",
        "    def run_experiments(self) -> Dict:\n",
        "        \"\"\"Run experiments with optimized memory management\"\"\"\n",
        "        results = {\n",
        "            \"metadata\": {\n",
        "                \"timestamp\": time.strftime(\"%Y%m%d-%H%M%S\"),\n",
        "                \"models_tested\": [model[\"name\"] for model in self.model_config.models],\n",
        "                \"thresholds_tested\": self.thresholds,\n",
        "                \"chunk_strategies_tested\": self.chunk_strategies,\n",
        "                \"temperature\": self.model_config.temperature\n",
        "            },\n",
        "            \"results\": []\n",
        "        }\n",
        "\n",
        "        # Process each strategy\n",
        "        for strategy in self.chunk_strategies:\n",
        "            print(f\"\\nProcessing strategy: {strategy}\")\n",
        "\n",
        "            # Only iterate through thresholds for semantic chunking\n",
        "            if strategy == \"semantic\":\n",
        "                thresholds_to_test = self.thresholds\n",
        "                print(f\"Testing semantic thresholds: {thresholds_to_test}\")\n",
        "            else:\n",
        "                # Use None for non-semantic strategies as they use fixed chunk sizes\n",
        "                thresholds_to_test = [None]\n",
        "                print(f\"Using fixed chunk size for {strategy} strategy\")\n",
        "\n",
        "            for threshold in thresholds_to_test:\n",
        "                # Pass the threshold only for semantic strategy\n",
        "                actual_threshold = threshold if strategy == \"semantic\" else 0\n",
        "\n",
        "                self.rag_pipeline.create_chunks(\n",
        "                    documents,\n",
        "                    threshold=actual_threshold,\n",
        "                    chunk_strategy=strategy\n",
        "                )\n",
        "\n",
        "                # Process each model\n",
        "                for model_config in self.model_config.models:\n",
        "                    model_name = model_config[\"name\"]\n",
        "                    print(f\"\\nTesting model: {model_name}\")\n",
        "\n",
        "                    with self.model_config.load_model(model_config) as model:\n",
        "                        for question in self.questions:\n",
        "                            print(f\"Processing question: {question}\")\n",
        "\n",
        "                            context = self.rag_pipeline.run_cosine_search(\n",
        "                                query=question,\n",
        "                                threshold=threshold,\n",
        "                                chunk_strategy=strategy\n",
        "                            )\n",
        "\n",
        "                            answer = self.rag_pipeline.generate_response(\n",
        "                                query=question,\n",
        "                                context_rag=context,\n",
        "                                model=model\n",
        "                            )\n",
        "\n",
        "                            results[\"results\"].append({\n",
        "                                \"model\": model_name,\n",
        "                                \"threshold\": threshold if strategy == \"semantic\" else None,\n",
        "                                \"chunk_strategy\": strategy,\n",
        "                                \"question\": question,\n",
        "                                \"response\": answer\n",
        "                            })\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "8hFyd9G1kC8M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluator Class"
      ],
      "metadata": {
        "id": "EpjD-Qz54mfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "import tiktoken\n",
        "import textwrap\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "class ExperimentEvaluator:\n",
        "    \"\"\"Handles pure evaluation logic\"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.client = openai.OpenAI(api_key=api_key)\n",
        "        self.encoder = tiktoken.encoding_for_model(\"gpt-4o\")\n",
        "\n",
        "    def _get_baseline_answers(self, questions: List[str], source_doc: str) -> Dict[str, str]:\n",
        "        \"\"\"Get GPT-4o's own answers to the questions as baseline\"\"\"\n",
        "        baseline_prompt = f\"\"\"Source Document:\n",
        "        {source_doc}\n",
        "\n",
        "        Using only the information from the source document above, answer these questions.\n",
        "        Format your response as a valid JSON object with questions as keys and answers as values.\n",
        "        Keep answers concise and factual.\n",
        "\n",
        "        Questions to answer:\n",
        "        {json.dumps(questions, indent=2)}\"\"\"\n",
        "\n",
        "        try:\n",
        "            print(\"\\n--- Getting Baseline Answers ---\")\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides JSON-formatted answers based on source documents.\"},\n",
        "                    {\"role\": \"user\", \"content\": baseline_prompt}\n",
        "                ],\n",
        "                temperature=0.1\n",
        "            )\n",
        "\n",
        "            content = response.choices[0].message.content\n",
        "            if '{' in content and '}' in content:\n",
        "                json_str = content[content.find('{'):content.rfind('}')+1]\n",
        "                return json.loads(json_str)\n",
        "            return {\"error\": \"No JSON structure found\", \"questions\": questions}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error getting baseline answers: {str(e)}\")\n",
        "            return {\"error\": str(e), \"questions\": questions}\n",
        "\n",
        "    def evaluate_experiments(self, experiment_results: Dict, source_doc: str) -> Dict:\n",
        "        \"\"\"Core evaluation logic\"\"\"\n",
        "        try:\n",
        "            print(\"\\n=== Starting Evaluation Process ===\")\n",
        "            questions = list(set(result[\"question\"] for result in experiment_results[\"results\"]))\n",
        "            model_strategy_combinations = set(\n",
        "                (result[\"model\"],\n",
        "                result[\"chunk_strategy\"],\n",
        "                result[\"threshold\"] if result[\"chunk_strategy\"] == \"semantic\" else None)\n",
        "                for result in experiment_results[\"results\"]\n",
        "            )\n",
        "\n",
        "            baseline_answers = self._get_baseline_answers(questions, source_doc)\n",
        "            all_evaluations = []\n",
        "\n",
        "            for model, strategy, threshold in model_strategy_combinations:\n",
        "                relevant_results = [r for r in experiment_results[\"results\"]\n",
        "                                  if r[\"model\"] == model and\n",
        "                                     r[\"chunk_strategy\"] == strategy and\n",
        "                                     (r[\"threshold\"] == threshold if strategy == \"semantic\" else True)]\n",
        "\n",
        "                for result in relevant_results:\n",
        "                    evaluation = self._evaluate_single_response(\n",
        "                        result, baseline_answers.get(result[\"question\"], \"No baseline available\")\n",
        "                    )\n",
        "                    all_evaluations.append(evaluation)\n",
        "\n",
        "            return {\n",
        "                \"metadata\": {\n",
        "                    \"timestamp\": datetime.now().isoformat(),\n",
        "                    \"model_used\": \"gpt-4o\",\n",
        "                    \"num_combinations_evaluated\": len(model_strategy_combinations),\n",
        "                    \"num_questions_evaluated\": len(questions),\n",
        "                    \"evaluation_status\": \"success\"\n",
        "                },\n",
        "                \"evaluations\": all_evaluations,\n",
        "                \"summary\": self._generate_summary(all_evaluations)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nCritical error in evaluation process: {str(e)}\")\n",
        "            return self._create_default_evaluation(experiment_results)\n",
        "\n",
        "    def _evaluate_single_response(self, result: Dict, baseline: str) -> Dict:\n",
        "        \"\"\"Evaluate a single response\"\"\"\n",
        "        evaluation_prompt = self._construct_evaluation_prompt(result, baseline)\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert at evaluating LLM responses for accuracy and quality.\"},\n",
        "                    {\"role\": \"user\", \"content\": evaluation_prompt}\n",
        "                ],\n",
        "                temperature=0.7,\n",
        "                max_tokens=1000\n",
        "            )\n",
        "\n",
        "            content = response.choices[0].message.content\n",
        "            if '{' in content and '}' in content:\n",
        "                json_str = content[content.find('{'):content.rfind('}')+1]\n",
        "                return json.loads(json_str)\n",
        "            return self._create_default_single_evaluation(result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating response: {str(e)}\")\n",
        "            return self._create_default_single_evaluation(result)\n",
        "\n",
        "    def _construct_evaluation_prompt(self, result: Dict, baseline: str) -> str:\n",
        "        \"\"\"Construct the prompt for evaluating a single response\"\"\"\n",
        "        threshold_part = f'Threshold: {result[\"threshold\"]}' if result[\"chunk_strategy\"] == \"semantic\" else ''\n",
        "\n",
        "        return f\"\"\"Evaluate this specific response:\n",
        "\n",
        "        Question: {result[\"question\"]}\n",
        "        Baseline Answer: {baseline}\n",
        "        Model: {result[\"model\"]}\n",
        "        Chunking Strategy: {result[\"chunk_strategy\"]}\n",
        "        {threshold_part}\n",
        "        Response: {json.dumps(result[\"response\"], indent=2)}\n",
        "\n",
        "        Context about chunking strategies:\n",
        "        - Semantic chunking uses thresholds to determine chunk boundaries based on semantic similarity\n",
        "        - Paragraph chunking splits text at paragraph breaks (no threshold needed)\n",
        "        - Header chunking splits text at section headers (no threshold needed)\n",
        "\n",
        "        Score the response on these criteria (0-100):\n",
        "        - Accuracy: How well does it match the baseline/source\n",
        "        - Conciseness: Clear, direct answer without extra information\n",
        "        - Source Attribution: Uses relevant source text as evidence\n",
        "        - Reasonableness: Answer is properly contextualized\n",
        "\n",
        "        Provide your evaluation in this exact JSON format:\n",
        "        {{\n",
        "            \"model\": \"{result[\"model\"]}\",\n",
        "            \"chunk_strategy\": \"{result[\"chunk_strategy\"]}\",\n",
        "            \"threshold\": {result[\"threshold\"] if result[\"chunk_strategy\"] == \"semantic\" else \"null\"},\n",
        "            \"question\": \"{result[\"question\"]}\",\n",
        "            \"scores\": {{\n",
        "                \"accuracy\": <score>,\n",
        "                \"conciseness\": <score>,\n",
        "                \"source_attribution\": <score>,\n",
        "                \"reasonableness\": <score>\n",
        "            }},\n",
        "            \"composite_score\": <average of scores>,\n",
        "            \"explanation\": \"detailed explanation\"\n",
        "        }}\"\"\"\n",
        "\n",
        "    def _create_default_evaluation(self, experiment_results: Dict) -> Dict:\n",
        "        \"\"\"Create a default evaluation structure when parsing fails\"\"\"\n",
        "        print(\"\\n--- Creating Default Evaluation Due to Failure ---\")\n",
        "        default_eval = {\n",
        "            \"metadata\": {\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"model_used\": \"gpt-4o\",\n",
        "                \"num_permutations_evaluated\": len(experiment_results[\"results\"]),\n",
        "                \"num_questions_evaluated\": len(set(r[\"question\"] for r in experiment_results[\"results\"])),\n",
        "                \"evaluation_status\": \"failed\"\n",
        "            },\n",
        "            \"evaluations\": [],\n",
        "            \"summary\": {\n",
        "                \"overall_performance\": \"Evaluation failed - using default structure\",\n",
        "                \"optimal_permutation\": \"Not available\",\n",
        "                \"performance_analysis\": \"Evaluation process encountered errors\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for result in experiment_results[\"results\"]:\n",
        "            default_eval[\"evaluations\"].append({\n",
        "                \"model\": result[\"model\"],\n",
        "                \"threshold\": result[\"threshold\"],\n",
        "                \"question\": result[\"question\"],\n",
        "                \"scores\": {\n",
        "                    \"accuracy\": 0,\n",
        "                    \"conciseness\": 0,\n",
        "                    \"source_attribution\": 0,\n",
        "                    \"reasonableness\": 0\n",
        "                },\n",
        "                \"composite_score\": 0,\n",
        "                \"explanation\": \"Evaluation failed - default scores assigned\"\n",
        "            })\n",
        "\n",
        "        print(\"Created default evaluation with\", len(default_eval[\"evaluations\"]), \"empty evaluations\")\n",
        "        return default_eval\n",
        "\n",
        "    def _create_default_single_evaluation(self, result: Dict) -> Dict:\n",
        "        \"\"\"Create a default evaluation for a single response when evaluation fails\"\"\"\n",
        "        return {\n",
        "            \"model\": result[\"model\"],\n",
        "            \"chunk_strategy\": result[\"chunk_strategy\"],\n",
        "            \"threshold\": result[\"threshold\"] if result[\"chunk_strategy\"] == \"semantic\" else None,\n",
        "            \"question\": result[\"question\"],\n",
        "            \"scores\": {\n",
        "                \"accuracy\": 0,\n",
        "                \"conciseness\": 0,\n",
        "                \"source_attribution\": 0,\n",
        "                \"reasonableness\": 0\n",
        "            },\n",
        "            \"composite_score\": 0,\n",
        "            \"explanation\": \"Evaluation failed - default scores assigned\"\n",
        "        }\n",
        "\n",
        "\n",
        "    def _generate_summary(self, evaluations: List[Dict]) -> Dict:\n",
        "        \"\"\"Generate summary statistics from evaluations\"\"\"\n",
        "        if not evaluations:\n",
        "            return {\n",
        "                \"overall_performance\": \"No evaluations available\",\n",
        "                \"optimal_permutation\": \"Not available\",\n",
        "                \"performance_analysis\": \"Evaluation process failed\"\n",
        "            }\n",
        "\n",
        "        # Calculate average scores by model/strategy combination\n",
        "        strategy_scores = {}\n",
        "        for eval in evaluations:\n",
        "            key = (eval[\"model\"], eval[\"chunk_strategy\"])\n",
        "            if \"threshold\" in eval and eval[\"chunk_strategy\"] == \"semantic\":\n",
        "                key = (eval[\"model\"], eval[\"chunk_strategy\"], eval[\"threshold\"])\n",
        "\n",
        "            if key not in strategy_scores:\n",
        "                strategy_scores[key] = {\n",
        "                    \"count\": 0,\n",
        "                    \"total_composite\": 0\n",
        "                }\n",
        "\n",
        "            scores = strategy_scores[key]\n",
        "            scores[\"count\"] += 1\n",
        "            scores[\"total_composite\"] += eval[\"composite_score\"]\n",
        "\n",
        "        # Find best performing configuration\n",
        "        best_score = 0\n",
        "        best_config = None\n",
        "        strategy_analysis = {}\n",
        "\n",
        "        for key, scores in strategy_scores.items():\n",
        "            avg_composite = scores[\"total_composite\"] / scores[\"count\"]\n",
        "\n",
        "            if len(key) == 3:  # Semantic chunking with threshold\n",
        "                model, strategy, threshold = key\n",
        "                config_str = f\"{model} with {strategy} chunking (threshold: {threshold})\"\n",
        "            else:  # Other chunking strategies\n",
        "                model, strategy = key\n",
        "                config_str = f\"{model} with {strategy} chunking\"\n",
        "\n",
        "            strategy_analysis[config_str] = avg_composite\n",
        "\n",
        "            if avg_composite > best_score:\n",
        "                best_score = avg_composite\n",
        "                best_config = config_str\n",
        "\n",
        "        return {\n",
        "            \"overall_performance\": f\"Average composite score across all evaluations: {sum(e['composite_score'] for e in evaluations)/len(evaluations):.2f}/100\",\n",
        "            \"optimal_permutation\": f\"Best performance: {best_config} (score: {best_score:.2f}/100)\",\n",
        "            \"performance_analysis\": strategy_analysis\n",
        "        }"
      ],
      "metadata": {
        "id": "1rAK93yw4qCx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results Manager Class"
      ],
      "metadata": {
        "id": "lJmy7VbumFk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResultsManager:\n",
        "    \"\"\"Handles formatting, saving, and displaying results\"\"\"\n",
        "    def __init__(self, save_directory: str):\n",
        "        self.save_directory = save_directory\n",
        "        os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "    def _format_experiment_results(self, experiment_results: Dict) -> Dict:\n",
        "        \"\"\"Format raw experiment results\"\"\"\n",
        "        formatted = {\n",
        "            \"metadata\": experiment_results.get(\"metadata\", {}),\n",
        "            \"results\": []\n",
        "        }\n",
        "\n",
        "        for result in experiment_results[\"results\"]:\n",
        "            formatted_result = {\n",
        "                \"model\": result[\"model\"],\n",
        "                \"chunk_strategy\": result[\"chunk_strategy\"],\n",
        "                \"threshold\": result[\"threshold\"],\n",
        "                \"question\": result[\"question\"],\n",
        "                \"response\": {\n",
        "                    \"answer\": result[\"response\"].get(\"response_text\", \"\"),\n",
        "                    \"sources\": result[\"response\"].get(\"sources\", [])\n",
        "                }\n",
        "            }\n",
        "            formatted[\"results\"].append(formatted_result)\n",
        "\n",
        "        return formatted\n",
        "\n",
        "    def format_results(self, experiment_results: Dict, evaluation_results: Dict) -> Tuple[Dict, Dict]:\n",
        "        \"\"\"Format both experiment and evaluation results\"\"\"\n",
        "        print(\"\\n=== Starting Results Formatting ===\")\n",
        "\n",
        "        formatted_experiment = self._format_experiment_results(experiment_results)\n",
        "        formatted_evaluation = self._format_evaluation_results(evaluation_results)\n",
        "\n",
        "        return formatted_experiment, formatted_evaluation\n",
        "\n",
        "    def _process_evaluations(self, formatted_results: Dict, evaluations: List[Dict]):\n",
        "        \"\"\"Process evaluations and add to formatted results structure\"\"\"\n",
        "        for eval in evaluations:\n",
        "            model_name = eval[\"model\"]\n",
        "            chunk_strategy = eval.get(\"chunk_strategy\", \"semantic\")\n",
        "            threshold = eval.get(\"threshold\")\n",
        "\n",
        "            # Initialize model if not exists\n",
        "            if model_name not in formatted_results[\"model_evaluations\"]:\n",
        "                formatted_results[\"model_evaluations\"][model_name] = {\n",
        "                    \"strategies\": {}\n",
        "                }\n",
        "\n",
        "            # Initialize strategy if not exists\n",
        "            strategy_key = f\"{chunk_strategy}\"\n",
        "            if strategy_key not in formatted_results[\"model_evaluations\"][model_name][\"strategies\"]:\n",
        "                formatted_results[\"model_evaluations\"][model_name][\"strategies\"][strategy_key] = {\n",
        "                    \"thresholds\": {} if chunk_strategy == \"semantic\" else {\"default\": {\"questions\": []}}\n",
        "                }\n",
        "\n",
        "            # Add evaluation based on chunking strategy\n",
        "            if chunk_strategy == \"semantic\":\n",
        "                threshold_key = str(threshold)\n",
        "                if threshold_key not in formatted_results[\"model_evaluations\"][model_name][\"strategies\"][strategy_key][\"thresholds\"]:\n",
        "                    formatted_results[\"model_evaluations\"][model_name][\"strategies\"][strategy_key][\"thresholds\"][threshold_key] = {\n",
        "                        \"questions\": [],\n",
        "                        \"average_scores\": {\n",
        "                            \"accuracy\": 0,\n",
        "                            \"conciseness\": 0,\n",
        "                            \"source_attribution\": 0,\n",
        "                            \"reasonableness\": 0,\n",
        "                            \"composite\": 0\n",
        "                        }\n",
        "                    }\n",
        "                target_dict = formatted_results[\"model_evaluations\"][model_name][\"strategies\"][strategy_key][\"thresholds\"][threshold_key]\n",
        "            else:\n",
        "                target_dict = formatted_results[\"model_evaluations\"][model_name][\"strategies\"][strategy_key][\"thresholds\"][\"default\"]\n",
        "\n",
        "            # Add question evaluation\n",
        "            target_dict[\"questions\"].append({\n",
        "                \"question\": eval[\"question\"],\n",
        "                \"scores\": eval[\"scores\"],\n",
        "                \"composite_score\": eval.get(\"composite_score\", 0),\n",
        "                \"explanation\": eval.get(\"explanation\", \"\")\n",
        "            })\n",
        "\n",
        "            # Update average scores\n",
        "            questions = target_dict[\"questions\"]\n",
        "            avg_scores = target_dict.setdefault(\"average_scores\", {\n",
        "                \"accuracy\": 0,\n",
        "                \"conciseness\": 0,\n",
        "                \"source_attribution\": 0,\n",
        "                \"reasonableness\": 0,\n",
        "                \"composite\": 0\n",
        "            })\n",
        "\n",
        "            avg_scores[\"accuracy\"] = sum(q[\"scores\"][\"accuracy\"] for q in questions) / len(questions)\n",
        "            avg_scores[\"conciseness\"] = sum(q[\"scores\"][\"conciseness\"] for q in questions) / len(questions)\n",
        "            avg_scores[\"source_attribution\"] = sum(q[\"scores\"][\"source_attribution\"] for q in questions) / len(questions)\n",
        "            avg_scores[\"reasonableness\"] = sum(q[\"scores\"][\"reasonableness\"] for q in questions) / len(questions)\n",
        "            avg_scores[\"composite\"] = sum(q[\"composite_score\"] for q in questions) / len(questions)\n",
        "\n",
        "    def display_results(self, evaluation_results: Dict):\n",
        "        \"\"\"Format and display evaluation results\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"MODEL EVALUATION RESULTS\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "        self._display_detailed_performance(evaluation_results)\n",
        "        self._display_summary(evaluation_results)\n",
        "        self._display_metadata(evaluation_results)\n",
        "\n",
        "\n",
        "    def save_results(self, formatted_experiment: Dict, formatted_evaluation: Dict) -> Tuple[str, str]:\n",
        "          \"\"\"Save formatted results to files\"\"\"\n",
        "          timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "          experiment_file = f\"{self.save_directory}/experiment_results_{timestamp}.json\"\n",
        "          evaluation_file = f\"{self.save_directory}/evaluation_results_{timestamp}.json\"\n",
        "\n",
        "          with open(experiment_file, 'w', encoding='utf-8') as f:\n",
        "              json.dump(formatted_experiment, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "          with open(evaluation_file, 'w', encoding='utf-8') as f:\n",
        "              json.dump(formatted_evaluation, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "          return experiment_file, evaluation_file\n",
        "\n",
        "    def _format_evaluation_results(self, evaluation_results: Dict) -> Dict:\n",
        "        \"\"\"Format evaluation results with aggregated scores\"\"\"\n",
        "        formatted = {\n",
        "            \"metadata\": evaluation_results[\"metadata\"],\n",
        "            \"model_evaluations\": {},\n",
        "            \"overall_summary\": evaluation_results.get(\"summary\", {})\n",
        "        }\n",
        "\n",
        "        if \"evaluations\" in evaluation_results:\n",
        "            self._process_evaluations(formatted, evaluation_results[\"evaluations\"])\n",
        "\n",
        "        return formatted\n",
        "\n",
        "    def _display_detailed_performance(self, evaluation_results: Dict):\n",
        "        \"\"\"Display detailed performance metrics\"\"\"\n",
        "        if \"evaluations\" not in evaluation_results:\n",
        "            return\n",
        "\n",
        "        print(\"DETAILED MODEL PERFORMANCE\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "        current_model = None\n",
        "        current_strategy = None\n",
        "        current_threshold = None\n",
        "\n",
        "        sorted_evaluations = sorted(\n",
        "            evaluation_results[\"evaluations\"],\n",
        "            key=lambda x: (\n",
        "                x[\"model\"],\n",
        "                x.get(\"chunk_strategy\", \"semantic\"),\n",
        "                str(x.get(\"threshold\", \"default\")),\n",
        "                x[\"question\"]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        for eval in sorted_evaluations:\n",
        "            self._display_single_evaluation(eval, current_model, current_strategy, current_threshold)\n",
        "            current_model = eval[\"model\"]\n",
        "            current_strategy = eval.get(\"chunk_strategy\", \"semantic\")\n",
        "            current_threshold = eval.get(\"threshold\")\n",
        "\n",
        "    def _display_single_evaluation(self, eval: Dict, current_model: str,\n",
        "                                 current_strategy: str, current_threshold: str):\n",
        "        \"\"\"Display a single evaluation entry\"\"\"\n",
        "        if eval[\"model\"] != current_model:\n",
        "            print(f\"\\nModel: {eval['model']}\")\n",
        "\n",
        "        strategy = eval.get(\"chunk_strategy\", \"semantic\")\n",
        "        if strategy != current_strategy:\n",
        "            print(f\"\\nChunking Strategy: {strategy}\")\n",
        "\n",
        "        if strategy == \"semantic\":\n",
        "            threshold = eval.get(\"threshold\")\n",
        "            if threshold != current_threshold:\n",
        "                print(f\"\\nThreshold: {threshold}\")\n",
        "\n",
        "        print(\"─\"*40)\n",
        "        print(f\"\\nQuestion: {eval['question']}\")\n",
        "        print(f\"Accuracy Score:          {eval['scores']['accuracy']:>3}/100\")\n",
        "        print(f\"Conciseness Score:       {eval['scores']['conciseness']:>3}/100\")\n",
        "        print(f\"Source Attribution:      {eval['scores']['source_attribution']:>3}/100\")\n",
        "        print(f\"Reasonableness Score:    {eval['scores']['reasonableness']:>3}/100\")\n",
        "        print(f\"Final Composite Score:   {eval['composite_score']:>3}/100\")\n",
        "        print(\"\\nExplanation:\")\n",
        "        print(textwrap.fill(eval['explanation'], width=80))\n",
        "\n",
        "    def _display_summary(self, evaluation_results: Dict):\n",
        "            \"\"\"Display summary section of evaluation results\"\"\"\n",
        "            if \"summary\" not in evaluation_results:\n",
        "                return\n",
        "\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"OVERALL ANALYSIS\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            summary = evaluation_results[\"summary\"]\n",
        "\n",
        "            print(\"\\nPerformance Summary:\")\n",
        "            print(\"-\"*80)\n",
        "            print(textwrap.fill(summary[\"overall_performance\"], width=80))\n",
        "\n",
        "            print(\"\\nOptimal Configuration:\")\n",
        "            print(\"-\"*80)\n",
        "            print(textwrap.fill(summary[\"optimal_permutation\"], width=80))\n",
        "\n",
        "            print(\"\\nPerformance Analysis:\")\n",
        "            print(\"-\"*80)\n",
        "            if isinstance(summary[\"performance_analysis\"], dict):\n",
        "                for config, score in summary[\"performance_analysis\"].items():\n",
        "                    print(f\"{config}: {score:.2f}\")\n",
        "            else:\n",
        "                print(textwrap.fill(str(summary[\"performance_analysis\"]), width=80))\n",
        "\n",
        "    def _display_metadata(self, evaluation_results: Dict):\n",
        "            \"\"\"Display metadata section of evaluation results\"\"\"\n",
        "            if \"metadata\" not in evaluation_results:\n",
        "                return\n",
        "\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"METADATA\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            metadata = evaluation_results[\"metadata\"]\n",
        "            print(f\"Timestamp:           {metadata.get('timestamp', 'Not available')}\")\n",
        "            print(f\"Model Used:          {metadata.get('model_used', 'Not available')}\")\n",
        "            print(f\"Combinations:        {metadata.get('num_combinations_evaluated', 'Not available')}\")\n",
        "            print(f\"Questions:           {metadata.get('num_questions_evaluated', 'Not available')}\")\n",
        "            print(f\"Evaluation Status:   {metadata.get('evaluation_status', 'Not available')}\")\n",
        "\n",
        "# Example usage:\n",
        "def process_results(experiment_results: Dict, evaluation_results: Dict, save_dir: str):\n",
        "    # Initialize results manager\n",
        "    results_manager = ResultsManager(save_dir)\n",
        "\n",
        "    # Format results\n",
        "    formatted_experiment, formatted_evaluation = results_manager.format_results(\n",
        "        experiment_results,\n",
        "        evaluation_results\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    experiment_file, evaluation_file = results_manager.save_results(\n",
        "        formatted_experiment,\n",
        "        formatted_evaluation\n",
        "    )\n",
        "\n",
        "    # Display results\n",
        "    results_manager.display_results(formatted_evaluation)\n",
        "\n",
        "    return formatted_experiment, formatted_evaluation"
      ],
      "metadata": {
        "id": "vnMb5d8cmKQU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "koQ5ZObJC2ek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qdI5iaXYsun",
        "outputId": "7faafb3f-1d35-4eb4-ed11-875cea77733f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing new RAG pipeline\n",
            "Starting experiment with configurations:\n",
            "Models: ['open-mistral-nemo']\n",
            "Thresholds: [85, 95]\n",
            "Chunk strategies: ['semantic', 'paragraph', 'header']\n",
            "Number of questions: 7\n",
            "\n",
            "Processing strategy: semantic\n",
            "Testing semantic thresholds: [85, 95]\n",
            "\n",
            "=== CHUNK CREATION DEBUG ===\n",
            "Strategy: semantic\n",
            "Cache key: semantic_85\n",
            "Using semantic threshold: 85\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Initialize configurations\n",
        "    model_config = ModelConfig(\n",
        "        models=MODEL_CONFIGS[\"models\"],\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    # Initialize experiment runner\n",
        "    experiment_runner = ExperimentRunner(\n",
        "        model_config=model_config,\n",
        "        thresholds=MODEL_CONFIGS[\"thresholds\"],\n",
        "        questions=QUESTION_CONFIGS[\"questions\"],\n",
        "        chunk_strategies=MODEL_CONFIGS[\"chunk_strategies\"]\n",
        "    )\n",
        "\n",
        "    print(\"Starting experiment with configurations:\")\n",
        "    print(f\"Models: {[model['name'] for model in model_config.models]}\")\n",
        "    print(f\"Thresholds: {MODEL_CONFIGS['thresholds']}\")\n",
        "    print(f\"Chunk strategies: {MODEL_CONFIGS['chunk_strategies']}\")\n",
        "    print(f\"Number of questions: {len(QUESTION_CONFIGS['questions'])}\")\n",
        "\n",
        "    # Run experiments\n",
        "    experiment_results = experiment_runner.run_experiments()\n",
        "\n",
        "    # Get source document text\n",
        "    source_doc = documents[0].text\n",
        "\n",
        "    # Initialize evaluator\n",
        "    print(\"\\nInitializing GPT-4 evaluation...\")\n",
        "    evaluator = ExperimentEvaluator(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "    # Run evaluation\n",
        "    evaluation_results = evaluator.evaluate_experiments(\n",
        "        experiment_results=experiment_results,\n",
        "        source_doc=source_doc\n",
        "    )\n",
        "\n",
        "    # Initialize results manager\n",
        "    results_manager = ResultsManager(save_directory=FILE_CONFIGS['save_directory'])\n",
        "\n",
        "    # Format results\n",
        "    formatted_experiment, formatted_evaluation = results_manager.format_results(\n",
        "        experiment_results=experiment_results,\n",
        "        evaluation_results=evaluation_results\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    experiment_file, evaluation_file = results_manager.save_results(\n",
        "        formatted_experiment=formatted_experiment,\n",
        "        formatted_evaluation=formatted_evaluation\n",
        "    )\n",
        "\n",
        "    # Display results\n",
        "    results_manager.display_results(evaluation_results=formatted_evaluation)\n",
        "\n",
        "    print(\"\\nExperiment complete!\")\n",
        "    print(f\"Results saved to:\")\n",
        "    print(f\"  Experiment results: {experiment_file}\")\n",
        "    print(f\"  Evaluation results: {evaluation_file}\")\n",
        "\n",
        "    return formatted_experiment, formatted_evaluation\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results, evaluation = main()"
      ]
    }
  ]
}